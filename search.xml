<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[A brief survey of papers on GNN for Aspect-Based Sentiment Analysis]]></title>
    <url>%2F2020%2F05%2F13%2Fdeep-learning%2FA%20brief%20survey%20of%20papers%20on%20GNN%20for%20ABSA%2F</url>
    <content type="text"><![CDATA[A brief survey of papers on GNN for Aspect-Based Sentiment AnalysisAbstract​ In previous years, there are lots of rearch about Graph Neural Network(GNN), a modified neural network for graph data. And some rearchers forcus on using GNN for the NLP tasks, such as text classification(TextGCN, Yao et al.) and comment generation(graph2seq, Li et al.) . Sun et al. had purposed a new model called CDT, which is an application of Aspect-Based Sentiment Classification, the sub-task of Aspect-Based Sentiment Analysis(ABSA). Inspired by Sun et al., some rearchers considered to using the GNN for ABSA. We forcus on reviewing their rearches, and discussing about the future work in those aspects. ​ 最近关于图神经网络（Graph Neural Network）的研究非常火热，这是一种适应于图数据的神经网络，并且最近在NLP界也开始大展拳脚，比如文本分类任务（TextGCN，Yao et al.）和评论生成任务（graph2seq，Li et al.）。Sun et al. 提出了一个新的模型——CDT, 应用于Aspect-Based Sentiment Classification 任务，属于ABSA下的一个分支。受Sun et al.的研究的启发，一些研究人员开始致力于将GNN应用到ABSA任务上。我们希望回顾这些研究，并讨论一下这些研究未来的发展。 Introduction​ 这里简要介绍一下图神经网络，ABSA任务，以及GNN在ABSA方面的前沿研究。 Graph Nerual Network​ 图神经网络，是一种能适应于图数据的神经网络，其基本的思想是，定义图上结点与他的邻居之间的信息传递方式，并通过矩阵运算实现。 ​ 而图这种信息的聚合，一般有两种方式： Spatial：在原始的图空间上进行信息的传递，代表模型有：GAT Spectral：在经过一定变换后的谱空间上进行信息的传递，代表模型有：GCN GNN还有一种常见的分类，即直推式（Transductive）和归纳式（Inductive）: Transductive：面对新的数据（新结点）时，需要重新构建图，代表模型有：GCN Inductive：可以对新结点进行归纳，不需要对图进行重构，代表模型有：GAT GCN​ Graph Convolutional Network（图卷积神经网络）可以说是最流行的图神经网络了。GCN发展也经过了一定的阶段，其基本思想是通过拉普拉斯矩阵的运算，实现图上的卷积。 L^{sym} = D^{-1/2}LD^{-1/2} = I-D^{-1/2}AD^{-1/2}​ 目前最流行的版本由Kipf在2016年提出： H^{(l+1)}= \sigma(\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}H^{(l)}W^{(l)})其中： $H$表示Hidden layer，即每一层的值 $W$表示Weights，即当前层的参数权重 $\sigma$表示激活函数 $\tilde{A}=A+I_N$，实际上就是一个邻接矩阵A，再加上自身的连接，其中$I_N$表示单位矩阵。 $\tilde{D}_{ii}=\sum_j\tilde{A}_{ij}$ 如此看来，GCN在形式上跟普通的NN是非常相似的，只是在前面乘上了一个$\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}$ 便可实现在图上的卷积。这一版本的GCN，优点是不需要对矩阵进行分解，极大地加快了运算，但缺点也很明显，必须要把整个图放进GCN中训练，当然这也是大部分GNN所存在的问题。并且GCN是Transductive的，这也就意味着训练集和测试集必须合并在一起构建图。 GAT​ Graph Attention Network由Bengio团队提出，顾名思义，GAT使用了Attention机制。 ​ 其核心思想还是论文（Attention is all you need）中的思想，只不过Transformer计算的是各个词对当前词的贡献度，而GAT计算的是邻居结点对当前结点$h_i$的贡献度$\alpha_{ij}$。当然，也用到了多头，做多次Attention的计算，做拼接或是取平均，得到该结点的下一个状态$h’_{1}$。 ​ 其中最核心的公式如下： \vec{h'}_i = \sigma(\sum_{j\in N_i}\alpha_{ij}W\vec{h}_j)当然，对于多头而言，最后还需要对$\vec{h’}$做拼接或者取平均。 $h$：就是隐藏层的状态，特别地，$h_0=x$。 $\alpha_{ij}$：即结点$h_j$对结点$h_i$的权重，计算方式与一般的Self-Attention无异。 GAT相对于GCN，最大的好处在于它可以Inductive Learning，即面对新的结点时，可以动态地计算$\alpha$，而不必重新构建图。且从实验数据来看，GAT即使是在Transductive任务上也要优于GCN。 Methods Cora Citeseer Pubmed GCN 81.5% 70.3% 79.0% GAT 83.0% 72.5% 79.0% Aspect-Based Sentiment Analysis​ Aspect-Based Sentiment Analysis（ABSA），是应用十分广泛的一类任务，比如在特定领域内（如电影、商品、新闻），需要对文本进行情感分析，那么ABSA就可以有很大的发挥空间。基本上分为两类任务：分类和抽取。Classification：每个样本给定一个Aspect（可以理解为类别/方面/主题），基于这个Aspect对该样本进行情感分类；Extraction：给定一个样本，将该样本的Aspect抽取出来。 ​ ABSA的核心是Aspect，那么我们必须要明确两个名词： Aspect Term：指文本中出现的目标实体，比如：”The Avengers 4 is good, but the sound effect in this cinema is terrible. “ 这个Avengers 4就是Aspect Term。一般也称为Target。 Aspect Category：指文本中的aspect类别，不一定是具体的实体，比如上面那句话中，sound effect 可以归类为服务，即归类为Aspect Category “service”。 因此，ABSA又可以细分为四类任务： Aspect Based Sentiment Classification Aspect Term Sentiment Classification Aspect Category Sentiment Classification Aspect Extraction Aspect Term Extraction Aspect Category Recognition 此外，ABSA近两年还有一些新兴的任务变种，这里我们主要关注于上面两类任务，因此此处不再赘述。 GNN for ABSA​ 最近两年，图神经网络在ABSA，尤其是ABSC任务上有着一定的热度（主要还是ABSC）。基本的思路是根据现有的Aspect，以及文本自身存在的关系（经常是构造一棵Dependency Tree），构建出图，再通过图神经网络对图进行分类。 ​ 其中，基本上以GCN和GAT两种模型作为基础模型，在此之上做一些改进。下面给出一些有代表性的论文。 Aspect-Level Sentiment Analysis Via Convolution over Dependency Tree, Sun et al. Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks, Zhang et al. Selective Attention Based Graph Convolutional Networks forAspect-Level Sentiment Classification, Hou et al. Modeling sentiment dependencies with graph convolutional networks for aspect-level sentiment classification, Zhao et al. Syntax-Aware Aspect Level Sentiment Classification with Graph Attention Networks, Huang et al. Exploiting Typed Syntactic Dependencies for Targeted Sentiment Classification Using Graph Attention Neural Network, Bai et al. Analysis of model “CDT”​ 从时间上来说，Sun et al. 提出的CDT应该算是最早地将图神经网络应用于ABSA任务了，且其中的方法很具有代表性，为后来的同类型的研究提供了很多思路，因此我们在这里简单地对CDT进行分析。(paper), (code). ​ 论文中的原图很清晰地描绘出整个模型的架构，其中有几点需要注意： Word Embedding 部分采用的是Glove Embedding，这部分比较简单。 接下来输入到一个BiLSTM中，这也是一个比较常规的操作。 再接下来输入到GCN中，这里就是整个模型的核心了。这里接下来详细地讲。 最后需要注意的是，GCN的输出并不完全地进入到下一层，而是只使用Aspect所对应的隐藏层表示，做了一个Average Pooling之后就是用Softmax进行多分类了。 所有的任务，想要用图神经网络，都离不开一个问题，那就是图的构建。而在CDT中，研究者们很巧妙地使用了Dependency Tree这一结构，根据论文中的描述，他们使用的是Stanford的语义分析工具，具体的例子如下图所示（论文原图）： ​ 接下来，问题就变成了如何将Dependency Tree转化为图（图矩阵），CDT所使用的方法也很简单，构建一个邻接矩阵，其中的结点就是一句话里的各个单词，边就是他们之间的Dependency连接。（自身也算） A_{ij} = \textit{1 IF linked OR itself, else 0.}​ 然后就是仿照GCN中的定义： H^{(l+1)}= \sigma (AH^{(l)}W^{(l)})（原论文中对$A$进行了标准化，这里为了方便对比省略了） 这里就是与原GCN不同的地方了，原版用的是$\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}$，而这里则是更加简化，仅仅使用邻接矩阵$A$. 以下是源代码中构建图的函数： 1234567891011121314151617181920212223242526# tree.py 构建图def tree_to_adj(sent_len, tree, directed=False, self_loop=True): """ Convert a tree object to an (numpy) adjacency matrix. """ ret = np.zeros((sent_len, sent_len), dtype=np.float32) queue = [tree] idx = [] while len(queue) &gt; 0: t, queue = queue[0], queue[1:] idx += [t.idx] for c in t.children: ret[t.idx, c.idx] = 1 queue += t.children if not directed: ret = ret + ret.T if self_loop: for i in idx: ret[i, i] = 1 return ret GCN层： 123456789# gcn.py# gcn layerdenom = adj.sum(2).unsqueeze(2) + 1 # normfor l in range(self.layers): Ax = adj.bmm(gcn_inputs) AxW = self.W[l](Ax) AxW = AxW / denom gAxW = F.relu(AxW) gcn_inputs = self.gcn_drop(gAxW) if l &lt; self.layers - 1 else gAxW ​ 可以看到，源码中的实现与论文中的表述是一致的，使用的确实就是标准化了的邻接矩阵。而事实上，当我尝试地将其改为一般的GCN的形式时（即$H^{(l+1)}= \sigma(\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}H^{(l)}W^{(l)})$），除了速度更慢了以外，效果并没有什么变化。这也是此论文有趣的地方。 ExperimentsDatasets Rest14 是一个餐厅评论的数据集，收录在 SemEval 2014 (Pontiki et al., 2014)中。 Latop 是一个笔记本电脑评论的数据集，同样收录在 SemEval 2014 中。 Twitter 推特的数据集（Dong et al., 2014） Rest16 同样是餐厅评论的数据集，收录在 SemEval 2016中。 Positive Neutral Negative Dataset train test train test train test Rest14 2164 727 637 196 807 196 Latop14 976 337 455 167 851 128 Rest16 1657 611 101 44 748 204 Twitter 1507 172 3016 336 1528 169 注：观察发现，不同论文中的数据集划分有细微的差异，但并不会对结果的对比产生大的影响。这里记录的是CDT中的划分。 Methods CDT[Sun et al., 2019]，将Dependency Tree构造为图，应用GCN，学习aspect的表示。 ASGCN[Zhang et al., 2019]，采用两个通道，一个通道计算Attention，一个通道与CDT采用相似的结构，输出Aspect对应的隐藏层状态，再用另一个通道计算好的Attention值加权。 SDGCN[Zhao et al., 2019] 与ASGCN类似，但两个通道用的都是BiLSTM作为Encoder，GCN放在最后面（Softmax之前）。此外，还用到了较多的残差连接，模型整体较复杂。 SA-GCN[Hout et al, 2020]，为了缓解Dependency Tree解析错误带来的连锁反应，不再直接使用Aspect对应的隐藏层状态，而是将Attention与GCN相结合，将Top K个值作为最后输入分类器的值。 TD-GAT [Huang and Carley, 2019], 与CDT类似，使用Graph Attention Network去捕捉语义结构。 R-GAT [Bai et al., 2020]，与CDT类似，使用了Relational Graph Attention Network代替GCN。 Results Method Rest14 Latop Twitter Rest16 ACC F1 ACC F1 ACC F1 ACC F1 GCN CDT(Sun et al., 2019) 82.3 74.02 77.19 72.99 74.66 73.66 85.58 69.93 ASGCN(Zhang et al., 2019) 80.86 72.19 75.55 71.05 72.15 70.40 88.99 67.48 SDGCN*(Zhao et al., 2019) 83.57 76.47 81.35 78.34 - - - - SAGCN* (Hou et al., 2020) 86.9 80.1 82.3 80.0 - - - - GAT TD-GAT*(Huang et al., 2019) 83.0 - 80.1 - - - - - R-GAT*(Bai et al., 2020) 86.59 80.51 81.25 78.55 75.84 74.65 - - 默认使用Glove Embedding，*表示使用BERT Embedding。以上数据均记录自所引用论文中的最好结果。 Potential Improvement​ 目前基本上所有的图神经网络模型在各个领域的应用，都离不开的两个问题： 图的构建 图神经网络结构 ​ 回到ABSC任务，先说说第一个问题，目前基本上所有的模型所使用的图都是基于Dependency Tree构建的邻接矩阵。Dependency Tree属于句法分析领域，这一领域并不热门，但还是有一些先进的研究。再者，所构建的邻接矩阵是一个只有0和1的，往往还是稀疏的矩阵，这样显然会浪费掉很多信息。所以，一个改进的思路是，能否构建出一个信息更丰富、更稠密的图矩阵，让语料的先验知识得到更好的抽取。 ​ 再来看看第二个问题，目前的研究都局限于使用GCN和GAT这两种模型，以及在它们之上做的各种“魔改”，比如加BERT、加Attention、做各种拼接和残差连接等等。那我们能否尝试一些别的GNN呢？比如Recurrent型的Graph Recurrent Neural Netwok 以及带记忆力机制的 Graph Memory Neural Network。 ​ 再者，目前的研究主要集中在分类任务，能否在Aspect Extract任务等任务上应用GNN，也是一个研究的方向。 References[1] Zhou et al., 2018. Graph Neural Networks: A Review of Methods and Applications. arXiv:1812.08434. [2] Liang Yao, Chengsheng Mao, and Yuan Luo, ‘Graph convolutional net-works for text classification’, inProceedings of the AAAI Conference onArtificial Intelligence, volume 33, pp. 7370–7377, (2019). [3] Nazir et al., 2020. Issues and Challenges of Aspect-based Sentiment Analysis: A Comprehensive Survey. IEEE-TAC-20. [4] Sun et al., 2019. Aspect-Level Sentiment Analysis Via Convolution over Dependency Tree. EMNLP-19. [5] Zhang et al., 2019. Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks. EMNLP-19. [6] Hou et al., 2020. Selective Attention Based Graph Convolutional Networks forAspect-Level Sentiment Classification. arXiv-20. [7] Zhao et al., 2019. Modeling sentiment dependencies with graph convolutional networks for aspect-level sentiment classification. arXiv-19. [8] Huang et al., 2019. Syntax-Aware Aspect Level Sentiment Classification with Graph Attention Networks. EMNLP-19. [9] Bai et al., 2020. Exploiting Typed Syntactic Dependencies for Targeted Sentiment Classification Using Graph Attention Neural Network. arXiv-20. [10] Ye et al., 2020. Document and Word Representations Generated by Graph Convolutional Network and BERT for Short Text Classification. ECAI-20. [11] Li et al, 2019. Coherent Comment Generation for Chinese Articles with a Graph-to-Sequence Model. ACL-19.]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>deep learning</tag>
        <tag>GNN</tag>
        <tag>ABSA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于求平方根——C语言实现]]></title>
    <url>%2F2020%2F05%2F09%2Fds-algorithm%2Fsqrt%2F</url>
    <content type="text"><![CDATA[关于求平方根——C语言实现​ 求平方根一直是应用广泛、使用频繁的一个数学函数，比如很多距离的计算就需要用到平方根的计算，如何高效地求解平方根，一直以来都是一个有趣的问题。这里我们假设一个问题，要求不使用自带的求平方根函数，求解给定整数的平方根，返回一个整数。一般的思路有如下几种： 通过其它的数学函数代替平方根函数得到精确结果，取整数部分作为答案； 通过数学方法得到近似结果，直接作为答案。 袖珍计算器算法​ 袖珍计算器算法是一种用指数函数 $\exp$ 和对数函数 $\ln$ 代替平方根函数的方法。我们通过有限的可以使用的数学函数，得到我们想要计算的结果。基本原理如下： \sqrt{x} = x^{1/2} = (e^{lnx})^{1/2} = e^{0.5\ln(x)}1234567#include &lt;math.h&gt;int mySqrt(int x)&#123; if(x==0) return 0; int ans = exp(0.5 * log(x)); return ((long)(ans + 1) * (ans + 1) &lt;= x ? ans + 1 : ans);&#125; 注意： 由于计算机无法存储浮点数的精确值，而指数函数和对数函数的参数和返回值均为浮点数，因此运算过程中会存在误差。例如当 x = 2147395600时，$e^{0.5\ln(x)}$的计算结果与正确值 4634046340 相差 $10^{-11}$ ，这样在对结果取整数部分时，会得到 46339这个错误的结果。 因此在得到结果的整数部分ans后，我们应当找出ans与ans+1中哪一个是真正的答案。 二分法​ 基本的思路就是二分查找，找到一个数ans，满足它的平方等于x，由于题目要求返回整数部分，因此只需要满足$ans^2&lt;= x$且$(ans+1)^2&gt;x$即可。 1234567891011121314int mySqrt(int x)&#123; int l=0, r=x, ans=-1; while(l&lt;=r)&#123; int mid = (l+r)/2 ; if((long )mid*mid &lt;= x)&#123; ans = mid; l = mid+1; &#125; else&#123; r = mid-1; &#125; &#125; return ans;&#125; 牛顿法​ 牛顿法是一种快速求解函数零点的方法，为了叙述方便，假设$C$为需要求解平方根的值，那么有 y = x^2 - C我们任取一个$x_0$作为初始值，在每一步的迭代中，在这一个点上作一条斜率该点导数$f^{-1}(x_i)$的直线，与x轴的交点即为下一次迭代的x值。经过多次迭代之后，我们就可以得到一个距离零点非常接近的交点。如下图所示。 算法： 选择$x_0=C$作为初始值。 在每一步的迭代中，通过当前的点$(x_i,f(x_i))$，作一条斜率为$f^{-1}(x_i)=2x_i$的直线，直线的方程为： \begin{equation} \begin{split} y_i &= 2x_i(x-x_i)+x^2_i -C \\ &= 2x_ix-(x^2_i+C) \end{split} \end{equation}与横轴的交点为方程$2x_ix-(x^2_i+C)=0$的解，即为新的迭代结果$x_{i+1}$： x_{x+1} = \frac{1}{2}(x_i+\frac{C}{x_i})在经过$k$次迭代之后，$x_k$的值与真实的零点$\sqrt{C}$足够接近，即可作为答案。 12345678910111213#include &lt;math.h&gt;int mySqrt(int x)&#123; if (x == 0) return 0; double x0 = x; while (true) &#123; double xi = 0.5 * (x0 + x / x0); if (fabs(x0 - xi) &lt; 1e-7) &#123; break; &#125; x0 = xi; &#125; return (int)x0;&#125; 平方根倒数——Carmack算法(平方根倒数速算法) 浮点数的平方根倒数常用于计算正规化向量[文 1]。3D图形程序需要使用正规化向量来实现光照和投影效果，因此每秒都需做上百万次平方根倒数运算，而在处理坐标转换与光源的专用硬件设备出现前，这些计算都由软件完成，计算速度亦相当之慢；在1990年代这段代码开发出来之时，多数浮点数操作的速度更是远远滞后于整数操作[1]，因而针对正规化向量算法的优化就显得尤为重要。——Wikipedia 相较于求平方根，求平方根的倒数的应用可能会更加广泛，尤其是在对向量进行正规化时，就需要求解平方根倒数。一般而言，上面的几个方法都是可以用来求平方根倒数的，但还有一种更神奇的方法——Carmack算法。 注：真实的起源已经无从考究，但最广为人知的是工程师约翰·卡马克在《雷神之锤III竞技场》的源代码中的应用。不过后来据说他本人予以否认，但源代码和这个叫法已经广为流传。 源代码也很简单，如下: 123456789101112131415161718192021float Q_rsqrt( float number )&#123; long i; float x2, y; const float threehalfs = 1.5F; x2 = number * 0.5F; y = number; i = * ( long * ) &amp;y; // evil floating point bit level hacking i = 0x5f3759df - ( i &gt;&gt; 1 ); // what the fuck? y = * ( float * ) &amp;i; y = y * ( threehalfs - ( x2 * y * y ) ); // 1st iteration// y = y * ( threehalfs - ( x2 * y * y ) ); // 2nd iteration, this can be removed#ifndef Q3_VM#ifdef __linux__ assert( !isnan(y) ); // bk010122 - FPE?#endif#endif return y;&#125; 博主菊长的菊花田给出的简化版如下： 123456789float InvSqrt (float x)&#123; float xhalf = 0.5f*x; int i = *(int*)&amp;x; i = 0x5f3759df - (i &gt;&gt; 1); // 计算第一个近似根 x = *(float*)&amp;i; x = x*(1.5f - xhalf*x*x); // 牛顿迭代法 return x;&#125; ​ Carmack算法的本质其实是还是上面提到的牛顿迭代法，但最核心的部分在那行0x5f3759df。众所周知，牛顿迭代法需要设置一个初始值，然后不断的迭代接近真实值，而神奇的地方在于这行代码在第一次迭代就已经非常接近于这个值了。 引用【1】求平方根——LeetCode 官方题解 【2】平方根倒数速算法，维基百科) 【3】卡马克快速平方根,菊长的菊花田]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>算法</tag>
        <tag>C语言</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于链表——C语言实现]]></title>
    <url>%2F2020%2F05%2F03%2Fds-algorithm%2FLikedList%2F</url>
    <content type="text"><![CDATA[关于链表——C语言实现​ 链表（LinkedList），是一种链式存储的数据结构，类似于数组，但在内存中非连续的存放，通过链表指针连接。好处是插入删除等操作时，相对于数组复杂度更低，缺点是查找起来不如数组方便。 链表基础链表的建立1234567891011121314151617181920// 定义链表结构体typedef struct Node&#123; int data; struct Node *next;&#125;ListNode;// 建立链表ListNode *buildList(int n, int num[])&#123; ListNode *head,*end; head = (ListNode*)malloc(sizeof(ListNode)); end = head; for (int i = 0; i &lt; n; i++) &#123; ListNode node = (ListNode*)malloc(sizeof(ListNode)); node-&gt;data = num[i]; end-&gt;next = node; end = node; &#125; end-&gt;next = NULL; return head;&#125; 遍历链表​ 链表的遍历非常简单，不断地访问下一个结点即可，但它也是绝大部分链表算法的基础。 1234567891011//遍历链表int ListIter(ListNode* p)&#123; ListNode* h; h = p-&gt;next; while(h != NULL)&#123; printf("%d-&gt;", h-&gt;data); h = h-&gt;next; &#125; printf("\n"); return 0;&#125; 插入1234567891011121314151617181920ListNode *insertList(ListNode* p, int data, int index)&#123; int i = 0; ListNode *h, *t; t-&gt;data = data; t-&gt;next = NULL; if(p == NULL)&#123; return t; &#125; h = p; while(h)&#123; if (i == index)&#123; t-&gt;next = h-&gt;next; h-&gt;next = t; break; &#125; h = h-&gt;next; i+=1; &#125; return p;&#125; 删除​ 按值删除（删除第一个对应的值的结点，适当修改可以删除全部） 1234567891011121314struct ListNode* deleteNode(struct ListNode* head, int val)&#123; if(head==NULL)return NULL; if(head-&gt;val==val)return head-&gt;next; struct ListNode *p=head-&gt;next, *pr=head; while(p)&#123; if(p-&gt;val==val)&#123; pr-&gt;next=p-&gt;next; return head; &#125; pr = p; p=p-&gt;next; &#125; return head;&#125; ​ 按下标删除 123456789101112131415struct ListNode* deleteNode(struct ListNode* head, int index)&#123; if(head==NULL)return NULL; if(head-&gt;val==val)return head-&gt;next; struct ListNode *p=head-&gt;next, *pr=head; int i = 0; while(p)&#123; if(i == index)&#123; pr-&gt;next=p-&gt;next; return head; &#125; pr = p; p=p-&gt;next; &#125; return head;&#125; 经典题目移除链表中给定值的所有结点​ 跟普通的移除结点类似，但不能马上返回，且要注意删除的一个或多个节点位于链表的头部时的情况。 1234567891011121314151617struct ListNode* removeElements(struct ListNode* head, int val)&#123; struct ListNode *h = head; while (head &amp;&amp; head-&gt;val == val) &#123; head = head-&gt;next; &#125; while(h &amp;&amp; h-&gt;next)&#123; if (h-&gt;next-&gt;val == val)&#123; struct ListNode *t = h-&gt;next; h-&gt;next = h-&gt;next-&gt;next; free(t); &#125; else&#123; h = h-&gt;next; &#125; &#125; return head;&#125; 反转链表​ 反转链表的基本思路就是，用两个指针分别指向当前结点和上一个结点，再通过改变指向来使链表反转，同时需要多一个指针来保存状态。 123456789101112struct ListNode* reverseList(struct ListNode* head)&#123; struct ListNode *p, *tmp, *pr; p = head; pr = NULL; while(p != NULL)&#123; tmp = p-&gt;next; p-&gt;next = pr; pr = p; p = tmp; &#125; return pr;&#125; 回文链表​ 这道题的核心是：反转链表、快慢指针。基本思路是，用一个快指针每次走两步，慢指针每次走一步，那么快指针到末尾时，慢指针正好到一半。这个时候，只需要将慢指针的那一半反转，再和前面的一半做对比，即可知道改链表是否是回文链表。 1234567891011121314151617181920212223242526struct ListNode *isPalindrome(struct ListNode* head)&#123; struct ListNode *slow=head, *fast=head; //链表调用next之前一定确保其不为null while(fast &amp;&amp; fast-&gt;next)&#123; slow=slow-&gt;next; fast=fast-&gt;next-&gt;next; &#125; //slow就是指向链表的中间 struct ListNode *pre=NULL; //反转链表 while(slow)&#123; struct ListNode *next=slow-&gt;next; slow-&gt;next=pre; pre=slow; slow=next; &#125; //结束之后，pre指向翻转链表的第一个节点。 struct ListNode *forward=head; while(pre)&#123; if(pre-&gt;val!=forward-&gt;val) return false; pre=pre-&gt;next; forward=forward-&gt;next; &#125; return true;&#125; 链表倒数第K个结点​ 同样是使用快慢指针，只要先让快指针走K步，再让快慢指针同时启动，那么快指针到尾端时，慢指针就是倒数第K个结点。 1234567891011struct ListNode* getKthFromEnd(struct ListNode* head, int k)&#123; struct ListNode *slow=head; for(int i=0;i&lt;k;i++)&#123; head = head-&gt;next; &#125; while(head)&#123; slow = slow-&gt;next; head = head-&gt;next; &#125; return slow;&#125; 判断链表是否有环​ 快慢指针，快指针走2步，慢指针走1步，只要有环，那么他们一定会相遇。 1234567891011bool hasCycle(struct ListNode *head) &#123; struct ListNode *p1=head, *p2=head; while(p1 &amp;&amp; p2 &amp;&amp; p2-&gt;next)&#123; p1 = p1-&gt;next; p2 = p2-&gt;next-&gt;next; if (p1==p2)&#123; return true; &#125; &#125; return false;&#125; 判断两个链表是否相交​ 使用两个指针，分别走完各自的链表以后，再从对方的头结点开始走，如果相交，那么他们必定会相遇。简单的证明如下，假设两个链表ACBC相交于点D，那么有AD+DC+BD=BD+DC+AD，即他们走过的路程相等，又因为步长相等，所以他们一定会在D点相遇。 123456789101112131415struct ListNode *getIntersectionNode(struct ListNode *headA, struct ListNode *headB) &#123; if(headA == NULL|| headB == NULL) return NULL; struct ListNode *t1=headA, *t2=headB; while(t1 != t2)&#123; if(!t1) t1 = headB; else t1 = t1-&gt;next; if(!t2) t2 = headA; else t2 = t2-&gt;next; &#125; return t1;&#125; 合并两个有序链表​ 实际上就是归并排序，跟数组类似，我们只需要让两个指针，根据结点值的大小，交替地往后运动。使用一个伪头部记录结果，在链表移动的时候将该结点记录下来。如果两个链表长度不等，会有一个多出来尾端，最后再把多出来的尾部拼到结果后面即可。 123456789101112131415161718struct ListNode* mergeTwoLists(struct ListNode* l1, struct ListNode* l2)&#123; struct ListNode *dum, *h; dum = (struct ListNode*)malloc(sizeof(struct ListNode)); h = dum; while(l1 &amp;&amp; l2)&#123; if(l1-&gt;val &lt; l2-&gt;val)&#123; h-&gt;next = l1; l1 = l1-&gt;next; &#125; else&#123; h-&gt;next = l2; l2 = l2-&gt;next; &#125; h = h-&gt;next; &#125; h-&gt;next = l1?l1:l2; return dum-&gt;next;&#125; 二进制链表转整数​ 实际上是一道进制转换题，思路也非常简单，运用反向运算操作，我们在获得二进制的时候是除于2取余数，要计算被除数则是要商乘于2加余数。 12345678int getDecimalValue(struct ListNode* head)&#123; int ret = 0; while(head)&#123; ret = ret*2 + head-&gt;val; head = head-&gt;next; &#125; return ret;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>链表</tag>
        <tag>数据结构与算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Latex 笔记]]></title>
    <url>%2F2020%2F04%2F24%2FLatex-Note%2F</url>
    <content type="text"><![CDATA[Latex Note​ 一些简单的latex笔记，发现记录一下还是很有必要的，以免写论文的时候又忘了，到处去找，费时费神。 Font字体大小全局字体大小 1\documentclass&#123;article&#125;[12pt] 局部字体大小 \tiny\scriptsize\footnotesize\small\normalsize\large\Large\LARGE\huge\Huge 加粗、斜体、下划线 显示直立文本： \textup{文本} 意大利斜体： \textit{文本} slanted斜体： \textsl{文本} 显示小体大写文本： \textsc{文本} 中等权重： \textmd{文本} 加粗命令： \textbf{文本} 默认值： \textnormal{文本} 斜体字：\textit{italic}，或者 \emph{italic} 细体字：\textlf{light font} 使用等宽字体：\texttt{code} 使用无衬线字体：\textsf{sans-serif} 所有字母大写：\uppercase{CAPITALS} 所有字母大写，但小写字母比较小：\textsc{Small Capitals} 下划线：\underline{text} Color导入包 1\usepackage[table, dvipsnames]&#123;xcolor&#125; 文本颜色 1\textcolor&#123;color&#125;&#123;text&#125; 表格填充 1\rowcolor&#123;color&#125; 自定义颜色 1\definecolor&#123;rgb&#125;&#123;r, g, b&#125; http://latexcolor.com/ Page导入包 12\usepackage&#123;fancyhdr&#125;\usepackage&#123;lastpage&#125; 页面格式 12345% 页眉页脚\pagestyle&#123;fancy&#125;% 无页眉页脚等\pagestyle&#123;empty&#125; 页眉、页脚 12345678% 页眉左上角\lhead&#123;left head note&#125;% 页眉右上角\rhead&#123;Page \thepage\ of\ \pageref&#123;LastPage&#125;&#125;% 页脚中部\cnote&#123;&#125; Table Of Content12345\setcounter&#123;tocdepth&#125;&#123;3&#125; % the depth of toc% 标题居中\begin&#123;center&#125; \tableofcontents\end&#123;center&#125; Table导入包 1234\usepackage&#123;textcomp,booktabs&#125;\usepackage[usenames,dvipsnames]&#123;color&#125;\usepackage&#123;colortbl&#125;\definecolor&#123;mygray&#125;&#123;gray&#125;&#123;.9&#125; 三线表 12345678910111213141516171819% 位置参数h:here,t:top,b:bottom,p:\begin&#123;table*&#125;[htp]\centering \begin&#123;tabular&#125;&#123;cc&#125; \toprule[1.1pt] %表头直线 \bf&#123;Notation&#125; &amp; \bf&#123;Specification&#125;\\ \midrule[1.1pt] ... &amp; ...\\ \rowcolor&#123;grey&#125;&#123;0.9&#125; % 填充颜色 ... &amp; ...\\ ... &amp; ...\\ \bottomrule[1.1pt] %表底直线 \end&#123;tabular&#125;\caption&#123;notations&#125; % 表格标题\label&#123;tab:my_label&#125; % 表格标签（引用需要）\end&#123;table*&#125; Graph12345678\usepackage[pdftex]&#123;graphicx&#125;\begin&#123;figure&#125;[htbp]\centering\includegraphics[height=6.0cm,width=9.5cm]&#123;fig/universe.jpg&#125;\caption&#123;Campus environment detection system&#125; % 图片标题\label&#123;fig:my_fig&#125; % 图片标签（引用需要）\end&#123;figure&#125; Enumerate and Itemsize1234\begin&#123;enumerate&#125; \item . \item .\end&#123;enumerate&#125; 1234\begin&#123;itemize&#125; \item . \item .\end&#123;itemize&#125; References导入包 123\usepackage&#123;cite&#125;% references format\bibliographystyle&#123;acm&#125; 引用 123456789% 文献引用\cite&#123;&#125;% 表格、图片等引用\ref&#123;tab:my_label&#125;\ref&#123;fig:my_label&#125;% 脚注\footnote&#123;&#125; 章节位置 1234% References\bibliography&#123;references&#125;% add to toc\addcontentsline&#123;toc&#125;&#123;section&#125;&#123;References&#125; BiTex类型 https://wenku.baidu.com/view/0f2096643968011ca300916d.htmlMath 导入包12\usepackage&#123;amsmath,amssymb,amsthm&#125;\usepackage&#123;newtxmath&#125; % must come after amsXXX 数学公式分段函数 1234567\begin&#123;equation&#125;f(n) =\begin&#123;cases&#125;n/2, &amp; \text&#123;if $n$ is even&#125; \\3n+1, &amp; \text&#123;if $n$ is odd&#125;\end&#123;cases&#125;\end&#123;equation&#125; \begin{equation} f(n) = \begin{cases} n/2, & \text{if $n$ is even} \\ 3n+1, & \text{if $n$ is odd} \end{cases} \end{equation}方程组 123456789\begin&#123;equation*&#125;\left\&#123; \begin&#123;array&#125;&#123;c&#125;a_1x+b_1y+c_1z=d_1 \\ a_2x+b_2y+c_2z=d_2 \\ a_3x+b_3y+c_3z=d_3\end&#123;array&#125;\right. \end&#123;equation*&#125; \begin{equation*} \left\{ \begin{array}{c} a_1x+b_1y+c_1z=d_1 \\ a_2x+b_2y+c_2z=d_2 \\ a_3x+b_3y+c_3z=d_3 \end{array} \right. \end{equation*}公式推导 123456\begin&#123;equation&#125;\begin&#123;split&#125;\cos 2x &amp;= \cos^2 x - \sin^2 x\\&amp;= 2\cos^2 x - 1\end&#123;split&#125;\end&#123;equation&#125; \begin{equation} \begin{split} \cos 2x &= \cos^2 x - \sin^2 x\\ &= 2\cos^2 x - 1 \end{split} \end{equation}]]></content>
      <categories>
        <category>latex</category>
      </categories>
      <tags>
        <tag>latex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python collections 简单上手]]></title>
    <url>%2F2020%2F04%2F24%2Fpython%2Fcollections%2F</url>
    <content type="text"><![CDATA[collections 库简单上手Counter Counter 是 dictionary 对象的子类。collections 模块中的 Counter() 函数会接收一个诸如 list 或 tuple 的迭代器，然后返回一个 Counter dictionary。这个 dictionary 的键是该迭代器中的唯一元素，每个键的值是迭代器元素的计数。 1from collections import Counter 123lst = [1, 2, 3, 3, 2, 1, 1, 1, 2, 2, 3, 1, 2, 1, 1]counter = Counter(lst)print(counter) Counter({1: 7, 2: 5, 3: 3}) most_common()函数会以元组列表的形式，返回出现频率最高的n个元素 1counter.most_common(2) [(1, 7), (2, 5)] defaultdictdefaultdict 的工作方式和平常的 python dictionary 完全相同，只是当你试图访问一个不存在的键时，它不会报错，而是会使用默认值初始化这个键。默认值是根据在创建 defaultdict 对象时作为参数输入的数据类型自动设置的。 1from collections import defaultdict 如上所示，在访问不存在的键”Sara”时，程序并不会像一般字典一样报错，而是为其设置一个int型的默认值。同理，当我们设置默认值类型为list时，看看会发生什么。 12345names_dict = defaultdict(int)names_dict["Bob"] = 1names_dict["Katie"] = 2sara_number = names_dict["Sara"]print(names_dict) defaultdict(&lt;class &#39;int&#39;&gt;, {&#39;Bob&#39;: 1, &#39;Katie&#39;: 2, &#39;Sara&#39;: 0}) 12345names_dict1 = defaultdict(list)names_dict1["Bob"] = 1names_dict1["Katie"] = 2sara_number = names_dict1["Sara"]print(names_dict1) defaultdict(&lt;class &#39;list&#39;&gt;, {&#39;Bob&#39;: 1, &#39;Katie&#39;: 2, &#39;Sara&#39;: []}) 可以看到，默认值变成了一个空列表 OrderedDict返回一个字典的子类，具有可以将字典顺序重新排列的方法 1from collections import OrderedDict 123d = OrderedDict.fromkeys('abcde')d.move_to_end('b')''.join(d.keys()) &#39;acdeb&#39; last 参数默认为True，即将元素挪到最右边，False则将元素挪到最左边 123d.move_to_end('b', last=False) ''.join(d.keys())'bacde' &#39;bacde&#39; popitem()方法将最后的元素弹出，参数last默认为True，即弹出最右边的元素，False则为最左边。 12print("poped item: &#123;&#125;".format(d.popitem()))print(d) poped item: (&#39;e&#39;, None) OrderedDict([(&#39;b&#39;, None), (&#39;a&#39;, None), (&#39;c&#39;, None), (&#39;d&#39;, None)]) deque collections.deque是python中使用队列的一种非常好的方法，这个方法的一个关键特性是保持队列长度一直不变，也就是说，如果你将 queue 的最大大小设置为 10，那么 deque 将根据 FIFO 原则添加和删除元素，以保持 queue 的最大大小为 10。 1from collections import deque 12345my_queue = deque(maxlen=10)for i in range(10): my_queue.append(i+1)print(my_queue) deque([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], maxlen=10) 接下来，我们尝试继续往里面添加元素 1234for i in range(11,16): my_queue.append(i)print(my_queue) deque([6, 7, 8, 9, 10, 11, 12, 13, 14, 15], maxlen=10) 可以看到，最前面的5个元素被弹出，而新元素被追加到队列的后面 nametuplenametuple是tuple的子类，意味着它可以像tuple一样通过索引下标访问，同时，它还具备了通过字段访问元素的功能。 1from collections import namedtuple 123456Point = namedtuple('Point', ['x', 'y'])p = Point(11, y=22) # 像普通元组一样访问元素print(p[0] + p[1]) # 通过字段访问元素print(p.x + p.y) 33 33 为了防止与字段名冲突，nametuple的类方法和属性前都加了下划线_。 123# _make方法将列表转化为nametuplet = [11, 22]Point._make(t) Point(x=11, y=22) 123# _asdict()将nametuple转为字典p = Point(x=11, y=22)p._asdict() OrderedDict([(&#39;x&#39;, 11), (&#39;y&#39;, 22)]) 需要注意，由于nametuple本质上还是一个tuple，因此直接更改其中的元素是会报错的。正确的做法是使用_replace函数。 12# p.x = 12 报错p._replace(x=12) Point(x=12, y=22) 可以通过_fields属性访问nametuple的字段。一个很常见的用法是从现有的nametuple创建一个新的nametuple。 1p._fields (&#39;x&#39;, &#39;y&#39;) 123Color = namedtuple('Color', 'red green blue')Pixel = namedtuple('Pixel', Point._fields + Color._fields)Pixel(11, 22, 128, 255, 0) Pixel(x=11, y=22, red=128, green=255, blue=0) 如果希望将字典转换为nametuple，可以用如下方式 12d = &#123;'x': 11, 'y': 22&#125;Point(**d) Point(x=11, y=22) ChainMap可以快速链接许多映射，因此可以将它们视为一个单元，通常比创建新字典并运行多个update()要快得多。 1from collections import ChainMap 注意，ChainMap的迭代顺序是通过扫描从前往后的映射决定的（与官方文档说的从后往前相反，研究了甚久，有可能是版本问题） 1234baseline = &#123;'music': 'bach', 'art': 'rembrandt'&#125;adjustments = &#123;'art': 'van gogh', 'opera': 'carmen'&#125;print(list(ChainMap(adjustments, baseline)))print(dict(ChainMap(adjustments, baseline))) [&#39;art&#39;, &#39;opera&#39;, &#39;music&#39;] {&#39;art&#39;: &#39;van gogh&#39;, &#39;opera&#39;: &#39;carmen&#39;, &#39;music&#39;: &#39;bach&#39;} 这与从最后一个映射开始的一系列dict.update()调用的顺序相反: 1234combined = baseline.copy()combined.update(adjustments)print(list(combined))print(dict(combined)) [&#39;music&#39;, &#39;art&#39;, &#39;opera&#39;] {&#39;music&#39;: &#39;bach&#39;, &#39;art&#39;: &#39;van gogh&#39;, &#39;opera&#39;: &#39;carmen&#39;} ChainMap最常用的例子是在程序传入参数的优先级设置上，例如：参数可以通过命令行传入，可以通过环境变量传入，还可以有默认参数，我们可以设置优先级：命令行&gt;环境变量&gt;默认值 123456789101112131415161718192021222324# jupyter notebook中无法使用argparsefrom collections import ChainMapimport os, argparse# 构造缺省参数:defaults = &#123; 'color': 'red', 'user': 'guest'&#125;# 构造命令行参数:parser = argparse.ArgumentParser()parser.add_argument('-u', '--user')parser.add_argument('-c', '--color')namespace = parser.parse_args()command_line_args = &#123; k: v for k, v in vars(namespace).items() if v &#125;# 组合成ChainMap:combined = ChainMap(command_line_args, os.environ, defaults)# 打印参数:print('color=%s' % combined['color'])print('user=%s' % combined['user'])]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>collections</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Byte Pair Encoding]]></title>
    <url>%2F2020%2F04%2F23%2Fnatural-language-processing%2FByte-Pair-Encoding%2F</url>
    <content type="text"><![CDATA[Byte Pair Encoding​ Byte Pari Encoding（BPE））是一种简单的数据压缩技巧，最初在1994年被提出，而如今广泛应用于各种现代的NLP模型（如BERT、GPT-2、XLM…）。 背景​ 如今NLP的发展，由刚开始的基于频率的稀疏词向量（如词袋、N-gram），到前几年通过预训练（word2vec，glove）生成语义表示的稠密词向量，再到划时代意义的BERT，NLP界越来越重视通过预训练学习语义，从而得到好的词向量表示。 ​ 然而这其中有个问题需要解决，基本上所有的NLP任务都需要构造一个词表，往往是选取出现频率最高的N个词加入词表。这其中有什么问题呢，举个例子，BERT的预训练语料是维基百科，如此庞大的语料库，词量肯定也是惊人的高。如果我们构造的词表太小，很多频率较低的词语就会被表示为一个掩码（又或是被去除），在模型中就根本“看不到”，这不仅会让这些低频词无法被学习，还会影响上下文中的高频词的学习。但如果词表过大，就会带来效率问题。 ​ BPE就是一种很好的方法，在不扩大词表的情况下，能够让更多的词语“被模型看到”。 原始BPE​ BPE最早在1994年由Philip Gage提出（A New Algorithm for Data Compression），是一种数据压缩的技术，通过使用数据中未出现的字节代替常见的连续字节对。 子词标记（Subword Tokenization）的BPE​ 方法最早在Neural Machine Translation of Rare Words with Subword Units提出，最初是为了解决神经网络翻译（NMT）中的OOV（Out of vocabulary）问题，即处理的词表是定长的，而翻译却往往会在词表之外。同样的，这个问题各种NLP任务也会遇到。 ​ 为了进行子词标记，BPE简单地做了一些调整，经常出现的子字对合并在一起，而不是被另一个字节代替。这样一来，低频的词就会被分割成几个高频的子词，例如athazagoraphobia可能就会被分割成[‘▁ath’, ‘az’, ‘agor’, ‘aphobia’]。 第0步，初始化词表； 第1步，把每个词表示为字符的组合，为了防止与其他词混淆，在词的后面再加入一个特殊的标识”&lt;\w&gt;”。例如hello，就会被分割成：”h e l l o “ 。 第2步，遍历词表，统计所有的字符对（其实就是2-gram），如：(h, e), (e, l), (l, l), (l, o), (o, )。 第3步，合并频率最高的字符对，将合成的新子词加入词表（或词频+1）。例如，(h, e)出现频率最高，那么就将他们合并，变成新子词he，再将这个新词加入词表，同时原来的单词就变成“he l l o ”。 第4步，重复2、3步，直到达到迭代次数或达到所需的词汇量。（超参数） 编码与解码​ 得到子词的词表后，我们先需要对子词表进行长度从大到小的排序。编码时，我们对每个单词，遍历子词表，寻找是否有子词是该单词的子串，然后将该单词分割成若干子词。若某一子串没有找到对应的子词，那么将它替换为一个特殊的标记如“”。 12345678910# 给定单词序列[“the&lt;/w&gt;”, “biggest&lt;/w&gt;”, “stone&lt;/w&gt;”]# 假设已有排好序的subword词表[“er&lt;/w&gt;”, “tain&lt;/w&gt;”, “tone”, “est&lt;/w&gt;”, “big”, “the&lt;/w&gt;”, “one&lt;/w&gt;”, &quot;s&quot;, &quot;g&quot;]# 迭代结果&quot;the&lt;/w&gt;&quot; -&gt; [&quot;the&lt;/w&gt;&quot;]&quot;biggest&lt;/w&gt;&quot; -&gt; [&quot;big&quot;, &quot;g&quot;,&quot;est&lt;/w&gt;&quot;]&quot;stone&lt;/w&gt;&quot; -&gt; [&quot;st&quot;, &quot;tone&lt;/w&gt;&quot;] ​ 这种方式的好处是，可以很好地适应新语料，缺点是，如果语料很大，这一编码方式效率较慢，因此一般是在训练模型之前，预编码好语料。还有一种方式是，在构建子词表的时候，其实就可以对构建子词表的语料进行编码了。 1234567891011121314151617181920# 给定单词序列[..., “the&lt;/w&gt;”, “biggest&lt;/w&gt;”, “stone&lt;/w&gt;”, ...] # 假设语料库中还有其他词语# 第一步# [..., &quot;t h e &lt;/w&gt;&quot;, &quot;b i g g e s t &lt;/w&gt;&quot;, &quot;s t o n e &lt;/w&gt;&quot;, ...]# 第二步# ..., (t, h), (h, e), (e, &lt;/w&gt;), (b, i), ...# 第三步，假设he频率最高# [..., &quot;t he &lt;/w&gt;&quot;, &quot;b i g g e s t &lt;/w&gt;&quot;, &quot;s t o n e &lt;/w&gt;&quot;, ...]# 继续第二步# ..., (t , he), (he, &lt;/w&gt;), (e, &lt;/w&gt;), (b, i), ...# ... 第三步# ... 迭代，直到次数满足，或词表大小满足# 最终结果# [..., &quot;the&lt;/w&gt;&quot;, &quot;big g est&lt;/w&gt;&quot;, &quot;s tone&lt;/w&gt;&quot;] ​ 可以看到，这个时候，我们直接将改变后的每个单词，按照空格符分割即可。 ​ 解码的过程就简单多了，以”“作为分割符，将子词合并为单词即可 12345# 假设模型预测后得到[“the&lt;/w&gt;”, &quot;big&quot;, &quot;g&quot;, &quot;est&lt;/w&gt;&quot;, &quot;s&quot;, &quot;tone&lt;/w&gt;&quot;]# 解码the biggest stone 代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import refrom collections import Counter, defaultdictdef build_vocab(corpus: str) -&gt; dict: """Step 1. 建立词表""" # 把每个字符分割出来，并在词语最后加上&lt;/w&gt;，如:"hello"-&gt;"h e l l o &lt;/w&gt;" tokens = [" ".join(word) + " &lt;/w&gt;" for word in corpus.split()] # 统计语料中的词频 vocab = Counter(tokens) return vocabdef get_stats(vocab: dict) -&gt; dict: """Step 2. 分割字符串，并统计字符对的频率""" pairs = defaultdict(int) for word, frequency in vocab.items(): symbols = word.split() # 以空格分割 # 统计2-gram频率 for i in range(len(symbols) - 1): pairs[symbols[i], symbols[i + 1]] += frequency return pairsdef merge_vocab(pair: tuple, v_in: dict) -&gt; dict: """合并频率最高的对""" v_out = &#123;&#125; bigram = re.escape(' '.join(pair)) p = re.compile(r'(?&lt;!\S)' + bigram + r'(?!\S)') for word in v_in: # 将词表中所有的最高词频对替换，如："h e l l o" -&gt; "he l l o" w_out = p.sub(''.join(pair), word) v_out[w_out] = v_in[word] return v_outcorpus = "I had seen the biggest stone."vocab = build_vocab(corpus) # Step 1num_merges = 50 # 迭代次数，超参数for i in range(num_merges): pairs = get_stats(vocab) # Step 2 if not pairs: break # step 3 best = max(pairs, key=pairs.get) vocab = merge_vocab(best, vocab) References​ [1] Sennrich, Rico, Barry Haddow, and Alexandra Birch. “Neural machine translation of rare words with subword units.”arXiv preprint arXiv:1508.07909(2015). ​ [2] Akashdeep Singh Jaswal, Byte Pair Encoding — The Dark Horse of Modern NLP, 2019. ​ [3] PLM, subword-units, 2017. ​ [4] Luke, 深入理解NLP Subword算法：BPE、WordPiece、ULM, 2020. ​ [5] Byte pair encoding - Wikipedia - https://en.wikipedia.org/wiki/B.]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>BERT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ELU and GELU]]></title>
    <url>%2F2019%2F01%2F11%2Fdeep-learning%2FELU%26GELU%2F</url>
    <content type="text"><![CDATA[ELU&emsp; ELU(Exponential Linear Units)，是对ReLU负半区为0的改进，使得激活单元均值更趋向于0. 图像和公式 其中，LReLU($\alpha=0.1$)，ELU($\alpha=1$) 公式如下 实验结果 在MINST数据集上的实验结果，左边是激活值的平均值，右边是交叉熵损失，可以看到ELU的激活值均值要更接近0，且交叉熵损失下降得更快. GELU&emsp; GELU(Gaussian Error Linear Unit)，是ReLU的一种改进，改进了ReLU缺乏概率解释的缺陷，在一些任务上表现得要更好. 图像和公式&emsp;GELU的曲线如下 其中，GELU($\mu =0,\sigma =1$)，ELU($\alpha = 1$) 公式如下： GELU(x) = xP(X]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>激活函数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BERT]]></title>
    <url>%2F2019%2F01%2F11%2Fnatural-language-processing%2FBERT%2F</url>
    <content type="text"><![CDATA[BERT&emsp; BERT全称是Bidirectional Encoder Representations from Transformers，从字面意思可以知道，这是一个基于Transformer的双向的编码器表征模型. 上面是BERT和GPT、ELMo的结构对比. Embedding &emsp; BERT的Embedding是3个Embedding的相加，即Token Embeddings、Segment Embeddings、Position Embeddings. Token Embeddings：词向量，其中第一个单词是分类任务的标记，可用于后面的分类任务 Segment Embeddings：用来区别上句和下句 Position Embeddings：位置信息向量，和Transformer中使用三角函数不一样，这里的是通过学习得到的 预训练任务&emsp; BERT有两个无监督预训练任务，第一步是随机遮盖一些词，然后依赖上下文来预测这些词，得到初步的预训练模型；第二步在第一步的基础上，是随机改变一半的句子对中的第二句，预测一个句子对的第二句是否是第一句的下一句。 Task 1&emsp; 第一个任务称为Mask-LM，常规的LanguageModel是这样的 P(X_i|X_{i-1},...X_1)或是这样的 P(X_i|X_{i+1},...,X_{n})而Mask-LM充分利用上下文信息，具体地，随机Mask掉15%的词，然后只去预测那些被Mask的词，即 P(masked|X_{else})&emsp; 实际上，BERT并不总是Mask那15%的词，而是 80%的概率Mask 10%的概率用一个随机的词代替 10%的概率不变 需注意的是模型本身并不知道它将要被要求预测哪些单词，或哪些单词被替换. Task 2&emsp; 第二个任务则是预测句子的下一句，即给定句子对$$，预测$S_2$是否是$S_1$的下一句。具体地，BERT随机替换50%的句子对中的$S_2$，并自动给定标签，然后去对所有句子对做预测. 例如： Input = [CLS] the man went to [MASK] store [SEP] he bought a gallon [MASK] milk [SEP] Label = IsNext Input = [CLS] the man [MASK] to the store [SEP] penguin [MASK] are flight ##less birds [SEP] Label = NotNext Fine-tuning&emsp; 预训练好的BERT模型，再去做一些简单调整，再在新的数据集上做训练，可以在许多自然语言处理的任务上有很好的表现. 例如，文本分类、问答、命名实体识别、上下文预测、对话等. &lt;/br&gt; &lt;/br&gt; 参考资料 https://arxiv.org/abs/1810.04805 https://zhuanlan.zhihu.com/p/46652512---]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>BERT</tag>
        <tag>Pre-training</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Attention]]></title>
    <url>%2F2019%2F01%2F11%2Fdeep-learning%2Fattention%2F</url>
    <content type="text"><![CDATA[注意力模型&emsp; 注意力模型(Attention Model)被广泛使用在自然语言处理、图像识别及语音识别等各种不同类型的深度学习任务中，它的基本思想是仿照人类在做阅读文本、观察物体和听声音等工作时，会对特定的词语、物体、声音给予更多的关注，因而能更好地完成对语言、图像和声音的识别以及理解. Encoder-Decoder模型&emsp; 注意力模型的一个广泛的应用是在机器翻译和对话生成上，而这两个应用最广泛使用的即是Encoder-Decoder模型，AM可以对Encoder-Decoder模型进行改进. &emsp; 如图所示，这是Encoder-Decoder模型的基本架构. 具体地，Encoder负责将输入的序列$(x_1,x_2,…,x_m)$，转化为语义编码$C$，而Decoder则负责将$C$生成$(y_1,y_2,…,y_n)$. \begin{align} C &= F(x_1,x_2,...,x_m) \\ y_i &= G(C,y_1,y_2,...,y_{i-1}) \end{align}一般在文本处理领域，Encoder和Decoder都是RNN模型. &emsp; 但普通的Encoder-Decoder模型存在一个问题，即Encoder生成的语义编码$C$都是一样的，这样一来，无论是生成哪个单词，输入的每个单词对它的影响力都是一样的. \begin{align} y_1 &= f(C) \\ y_2 &= f(C,y_1) \\ y_3 &= f(C,y_1,y_2) \end{align}这样一来，无法体现输入的不同单词对于目标单词的影响，于是需要引入注意力机制. 引入注意力&emsp; 引入注意力机制的Encoder-Decoder模型的结构如下， 相比于普通的Encoder-Decoder模型，引入注意力机制的模型在生成语义编码上面更加多样化，对于每个生成的词语$y_i$，都有语义编码$C_i$与之对应. 于是目标单词的生成变成了如下的形式： \begin{align} y_1 &= f(C_1) \\ y_2 &= f(C_2,y_1) \\ y_3 &= f(C_3,y_1,y_2) \end{align}那么，如何计算不同的目标单词对应的语义编码$C_i$呢？ &emsp; 为了体现不同输入单词对目标单词的影响，而又尽可能不要损失信息，一般采用加权求和的方式计算$C_i$. C_i = \sum_{j=1}^{L_x}a_{ij}h_j其中，$L_x$代表输入句子的长度，$a_{ij}$代表输出第$i$个单词的注意力分配系数，而$h_j$是输入句子中第$j$个单词的语义编码. &emsp; 于是问题就转变成如何求得注意力分配系数$a_{ij}$. 我们把引入注意力机制的Encoder-Decode模型展开，如下所示. (图中有误，$H_n$应该为$H_{n-1}$)那么我们可以采用如下方式计算注意力分配系数. 如图所示(图中有误，$H_{i-1}$应该是$H_{i-2}$，$H_{i}$应该是$H_{i-1 }$)，使用函数F，使得$h_j$与Decoder中的$y_i$之前的隐藏状态$h_i$对齐，最后以Softmax函数输出概率，即可得到目标单词$y_i$所对应的输入单词$x_j$的注意力分配系数$a_{ij}$. 注意力机制​ 我们可以总结出一个通用的注意力机制模式，即如下所示. 将Source中的元素看成是的键值对，然后计算Query和各个key之间的相似度或相关度，就能得到每个key对应value的权重系数，然后加权求和就能得到最终的AttentionValue. Attention(Query,Source)=\sum_{i=1}^{L_x}Similarity(Query,Key_i)*Value_i&emsp; 详细一点，我们可以将注意力机制分为3个阶段，第1个阶段，计算Query和Key的相似度或相关度；第2个阶段，进行归一化处理；第3个阶段，根据权重对Value进行加权求和得到AttentionValue. &lt;/br&gt; &lt;/br&gt; 参考资料： [1]. 张俊林, 深度学习中的注意力机制, https://blog.csdn.net/tg229dvt5i93mxaq5a6u/article/details/78422216. 2017-11-02.]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Transformer]]></title>
    <url>%2F2019%2F01%2F10%2Fdeep-learning%2FTransformer%2F</url>
    <content type="text"><![CDATA[Transformer&emsp; 在Google的“Attention is all you need”论文中，在Sequence-to-Sequence的任务下，打破了以往Encoder-Decoder模型以RNN、CNN为代表的神经网络架构，对序列的建模摒弃了RNN的时序观点和CNN的结构观点，而是直接使用self-attention机制，提出了一种新的神经网络结构——Transformer. 结构&emsp; Transformer的结构如下 Self-Attention&emsp; Self-Attention的好处是可以建立句子内部的联系，可以直接跨越多个词捕获长期依赖知识，而无需像RNN那样按照时序建模. ​ RNN对序列建模的结构如下： &emsp; Self-Attention对序列的建模如下： &emsp;在Attention机制中，我们对Source和Target进行Attention建模，如果把Source看作是存储器存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attetion值. 即把计算Attention值看作是一种软寻址，即根据Query和Key的相似性来决定取出对应Value的可能性. Attention(Q,K,V) = softmax(QK^T)V&emsp; 而Self-Attention则可看作是词自身为Query，整句都是的软寻址. Self-Attention的变种&emsp; Transformer里面的Self-Attention机制是一种新的变种，具体来说有两个方面，一个是加入了缩放因子(Scaling Factor)，另一方面，引入了多头注意力机制(muti-head attention). &emsp; 具体地，Attention的计算公式中的分母多了一个向量的维度的平方根，为的是防止维度过大导致点乘结果过大，而进入SoftMax的饱和区，引起梯度饱和. 公式如下: Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V&emsp; 而多头机制指的是，引入多组的参数矩阵来分别做Self-Attention，最后将所有结果拼接起来，如下图所示 左边是单层的Attention，而右边是多头机制的Attention. 公式如下： \begin{align} MultiHead(Q,K,V) &= Concat(head_1,...,head_h)W^O \\ Where \ head_i &= Attention(QW_i^Q,KW_i^Q,VW_i^Q) \end{align}这样一来，模型有多套比较独立的Attention参数，有一定集成的效果，理论上可以增强模型的能力. 位置编码&emsp; 由于Self-Attetion机制是一种扁平化的词袋的结构，导致不论距离多远的词，他们在Self-Attention中的距离都是1，这样的话，会丢失词之间的相对距离关系。导致像“牛 吃 草”、“草 吃 牛”在Selft-Attention下可能是一个意思. 因此，词向量中应尽可能地包含词的位置信息. &emsp; Transformer的做法是，将词在句子中所处的位置映射为向量，补充到其Embedding当中. 具体来说，Transformer使用了一种非常新颖的时序建模方式——利用三角函数的周期性，来构建词之间的相对位置关系. \begin{align} PE(pos,2i) = sin(pos/10000^{2i/d_{model}}) \\ PE(pos, 2i+1) = cos(pos/10000^{2i/d_{model}}) \end{align}其中$pos$是词的绝对位置，而$i$是词的维度下标，$d_{model}$是Embedding的维度，由上述公式可以看出，Transformer将绝对位置，做为三角函数中的变量，利用三角函数的周期性去表示词的相对位置. Position-wise Feed-Forward Networks&emsp; 除了在Embedding时引入词的位置信息外，Transformer还采用一种”位置智能的前馈神经网络“，实际上是进行两次线性变换，一次ReLU激活，公式如下： FFN(x) = max(0,xW_1+b_1)W_2+b_2这样一来，如果在不同位置的词具有相同的线性变换，再经过ReLU非线性激活和再一次的线性变换后，在更新参数时能得到不同的参数. &lt;/br&gt; &lt;/br&gt; 参考资料 [1] 腾讯云+社区, “变形金刚”为何强大：从模型到代码全面解析Google Tensor2Tensor系统, https://segmentfault.com/a/1190000015575985#articleHeader6, 2018-07-09. [2] Ashish Vaswani,Noam Shazeer,Niki Parmar,Jakob Uszkorei,Llion Jones,Aidan N. Gomez,Łukasz Kaise and Illia Polosukhin. Attention Is All You need. arXiv preprint arXiv:1706.03762 , 2017.]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>自然语言处理</tag>
        <tag>注意力机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LSTM]]></title>
    <url>%2F2018%2F11%2F13%2Fdeep-learning%2FLSTM%2F</url>
    <content type="text"><![CDATA[LSTM&emsp; LSTM(Long Short Term Memory)，是RNN的一种，通过门控机制，有选择地遗忘先前的状态和输出当前状态，可以很好地解决长期依赖(Long-Term Dependencies)的问题. 长期依赖&emsp; RNN的一个重要特性，就是可以对具有时序关系的数据进行很好地学习，但普通的RRN存在一个问题，在输入序列较长的情况下，处在靠后的位置往往很难得到靠前的输入的信息，如下图所示. 结构&emsp; 首先先看普通的RNN，我们把从输入到输出中间的结构看作是一个“细胞”，那么普通的RNN的细胞中只有一个tanh函数，用来处理旧信息$c_t$和当前输入$X_t$. 那么LSTM的结构是这样的，相比普通RNN，多了几个门的结构. 下面分块来解释各个门及其应用. 遗忘门&emsp; 首先我们需要遗忘门来控制我们需要丢弃哪些先前的信息，具体来说，遗忘门是一个Sigmoid函数，输出一个0~1之间的数$f_t$，“1”表示完全保留，“0”表示完全舍弃. 输入门&emsp; 接着我们用输入门控制哪些信息被更新，并且作为新信息输入到下一个时间点. 具体地，和遗忘门一样，用一个Sigmoid函数输出保留信息的比率$i_t$. 另外，和普通的RRN一样，使用一个tanh函数计算生成的状态信息$\hat{C_t}$ . 这里我们把新信息和旧信息结合，成为最终的状态输出到下个时间点. 这里就用到了前面计算得出的两个保留信息的比率$f_t$和$i_t$，让它们分别乘上新旧信息$C_{t-1}$和$\hat{C_t}$，最后相加得到最终的状态信息$C_t$，并输出到下一时间点$t+1$. 输出门&emsp; 最后我们需要输出门来控制最终输出的值，跟前面的思路基本一样，用一个Sigmoid函数输出0~1的值，然后用这个值乘上tanh函数包裹的状态$C_t$，便得到了此时间点$t$的输出$h_t$，并输出到下一时间点$t+1$. Peephole Connection&emsp; LSTM的一种比较流行的变体是增加了一种叫“Peephole Connection”的连接，即将细胞状态也输入到门中. GRU&emsp;GRU(Gated Recurrent Unit)是比较常见的LSTM的变体，其最大的改动是将遗忘门和输入门合成了单一的更新门，同时将原来的细胞状态信息$c_t$与$h_t$混合，成为单一的$h_t$，并且做了其他改动. &lt;/br&gt; &lt;/br&gt; 参考资料： [1]. wangduo, [译] 理解 LSTM(Long Short-Term Memory, LSTM) 网络 , 2017-04-27. [2]. colah, Understanding LSTM Networks, http://colah.github.io/posts/2015-08-Understanding-LSTMs/ , 2015-08-27.]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dropout]]></title>
    <url>%2F2018%2F11%2F13%2Fdeep-learning%2Fdropout%2F</url>
    <content type="text"><![CDATA[Dropout&emsp; Dropout是指在深度学习网络的训练过程中，按照一定概率暂时将神经元从网络中丢弃，这样一来，每一批的训练数据都在训练不同的网络. 实验证明，Dropout具有提高神经网络训练速度和防止过拟合的效果. 流程&emsp; 假设我们要训练如下的一个神经网络 使用Dropout时，我们临时随机删除一些神经元，得到如下的子网络： 使用反向传播算法，训练此子网络的参数，然后再重新从完整的神经网络，随机删除一些神经元，不断重复这一过程. 算法首先我们要设置一个参数p，代表的是神经网络中被随机删除的神经元比例. 训练阶段 Standard network \begin{align} z_i^{(l+1)} &= w_i^{(l+1)}y^l+b_i^{(l+1)} \\ y_i^{(l+1)} &= f(z_i^{l+1}) \end{align} Drop network \begin{align} r_j^{(l)} &\sim Bernoulli(p) \\ \hat{y} &= r^{(l)}*y^{(l)} \\ z_i^{(l+1)} &= w_i^{(l+1)} \hat{y}^l + b_i^{(l+1)} \\ y_i^{(l+1)} &= f(z_i^{(l+1)}) \end{align} 测试阶段&emsp; 测试时，每一个单元的参数都要乘以p 即 w_{test}^{(l)} = pW^{(l)}缓解过拟合的原因&emsp; Dropout的原理很简单直接，但为什么Dropout能够缓解神经网络中过拟合的问题呢？ 集成(Ensemble)的思想 &emsp; 如果把Dropout后的每个子网络看成是一个学习器，那么就可以把Dropout看作是Bagging的一种近似，且消耗更少的内存和运算力. 神经元共享的集成 &emsp; Dropout不仅仅是训练一个Bagging的集成模型，而且是共享神经元的集成模型. 这意味着，无论其他神经元是否在模型中，每个神经元都必须表现良好. 这样一来，每一个神经元在很多情况下都能表现良好，也就提高了神经网络的泛化能力. 引入噪声 &emsp; 我们可以认为Dropout实际上是对神经元施加了噪声扰动，增强了神经网络的学习能力和鲁棒性. 传统地，我们为了在神经网络中引入噪声，会对输入的原始值进行破坏，而Dropout直接作用于神经元，相当于是屏蔽掉一部分的输入，这比原始的方法更加智能和高效. &lt;/br&gt; &lt;/br&gt; 参考资料： [1] Micorstrong0305. 深度学习中Dropout原理解析. https://blog.csdn.net/program_developer/article/details/80737724 [2] Srivastava N, Hinton G, Krizhevsky A, et al. Dropout: A simple way to prevent neural networks from overfitting[J]. The Journal of Machine Learning Research, 2014, 15(1): 1929-1958. [3] Dropout as data augmentation. http://arxiv.org/abs/1506.08700]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[批标准化]]></title>
    <url>%2F2018%2F11%2F13%2Fdeep-learning%2Fbatch-normalization%2F</url>
    <content type="text"><![CDATA[批标准化(Batch Normalization)&emsp; 批标准化(Batch Normalization)，是解决随着网络深度加深，训练困难的问题的一种方法，一般用在激活函数之前，其基本思想是把每层的输入值的分布拉回到标准正态分布当中，使得每一层中，每一次的输入独立同分布，使得神经网络可以很好地学习数据；针对于非线性激活函数，还能将处于饱和区的值，拉回到梯度较大的区域，一定程度上解决梯度消失的问题. 独立同分布&emsp; 机器学习有一个重要的假设：独立同分布(Independently and Identically Distributed, IID)。即假设训练数据和测试数据是满足相同分布的，它是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障. &emsp; 如果将神经网络的每一层都看作是一次的输入和输出，由于隐藏层之间存在非线性映射的关系，那么显然难以保证每一层中每一次的输入独立同分布，并且随着网络的加深，这种情况会越来越糟糕. &emsp; 而批标准化可以很好地解决这一问题，具体来说，批标准化将输入变换到均值为0、方差为1的正态分布中，这样一来就实现了输入的同分布. 梯度消失&emsp; 深度神经网络中，非线性激活函数往往会遇到一个问题，梯度消失. 以Logistic函数为例， 如图，当输入值太大或太小的时候，函数值容易进入饱和区，这个时候斜率几乎为0，这样在反向传播的时候，梯度显然是无法继续传递的，也就导致了前面几层的参数无法更新. &emsp; 批标准化将输入标准化为均值为0、方差为1的正态分布，如下图所示. 可以看到，在此正态分布中，数据落在[-1,1]的概率为68%，落在[-2,2]的概率为95%，结合Logisitc函数的图像来看，批标准化后的数据大部分都会落在斜率较大的区间内，这样一来，梯度消失的问题就得到了很好的解决. 算法&emsp; 在神经网络中，往往是在激活函数之前加入对数据的批标准化操作，如下图所示. BN算法的核心即将数据进行正态分布的变换，即 \hat{x}^{(k)} = \frac{x^{(k)-E[x^{(k)}]}}{\sqrt{Var[x^{(k)}]}}但这也会导致网络的表达能力变差，于是引入两个调节参数，可以通过训练来学习，对变换后的值进行修正. y^{(k)} = \gamma^{(k)}\hat{x}^{(k)}+\beta^{(k)}具体算法如下所示 总结深度神经网络使用批标准化有如下好处： 使得隐藏层的每一次输入同分布，模型能更好地从数据中学习； 提升了训练速度，收敛过程加快； 一定程度上缓解了过拟合问题； 使得神经网络对初始化的要求降低，可以使用较大的学习率等； &lt;/br&gt; &lt;/br&gt; 参考资料 [1] 郭耀华 深入理解Batch Normalization批标准化 , 2018-04-05.]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[循环神经网络]]></title>
    <url>%2F2018%2F10%2F08%2Fdeep-learning%2Frecurrent-neural-network%2F</url>
    <content type="text"><![CDATA[循环神经网络结构算法]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
        <tag>自然语言处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[卷积神经网络]]></title>
    <url>%2F2018%2F10%2F08%2Fdeep-learning%2Fconvolutional-neural-network%2F</url>
    <content type="text"><![CDATA[卷积神经网络卷积池化]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
        <tag>计算机视觉</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[神经网络]]></title>
    <url>%2F2018%2F10%2F08%2Fdeep-learning%2Fneural-network%2F</url>
    <content type="text"><![CDATA[神经网络&emsp; 神经网络(neural network)，是深度学习(deep learning)领域的基础和核心工具，是一种模仿生物神经网络的结构和功能的机器学习模型。神经网络由大量的人工神经元(neron)组成，基本思想是神经元超过一个阈值(threshold)后被激活，输出值传到下一神经元，一层层传播，最终输出结果。 基本结构&emsp; 神经网络的基本结构如下图所示，由输入层、隐藏层和输出层组成。 隐藏单元&emsp; 在神经网络中，我们把隐藏层的神经元称为隐藏单元，其作用是对输入的特征进行非线性变换，将原有的特征空间拓展到更高维度，使原来线性不可分的样本点线性可分。 &emsp; 由于隐藏单元要对输入值做非线性运算，因此隐藏单元中的激活函数必须是非线性的。常见的激活函数有：Sigmoid、Tanh和ReLU。 Logistic Sigmoid函数式如下： f(z) = \frac{1}{1+exp{(-z)}}使用Logistic Sigmoid函数作为神经网络的激活函数，会有如下几个问题： 计算量大，反向传播求误差梯度时，求导涉及除法； 函数值易饱和，反向传播时，容易出现梯度消失的情况； Tanh函数式如下： f(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}}与Logistic Sigmoid函数相比，Tanh函数有如下特点： 同样存在计算量大的问题； 同样容易出现梯度消失的情况，但延迟了函数值的饱和期； 在0附近与单位函数类似，在激活保持地很小的情况下，类似于训练一个线性模型，使得训练神经网络更容易； ReLUReLU的函数式很简单： f(z) = max(0,z) 没有函数值饱和导致的梯度消失的问题； 只需要设置阈值就可以得到激活值，运算量小； 可以较好地模拟生物神经网络中，部分神经元未处于激活状态的情景； 可能出现神经元”大面积死亡”的情况； 现如今的深度神经网络多使用ReLU作为隐藏单元。 反向传播算法&emsp; 跟其他机器学习算法一样，神经网络同样涉及到模型训练的问题。训练一个神经网络，最常用的算法是反向传播算法(back propagation algorithm)，经常简称为backprop或BP算法。 &emsp; 其基本原理是复合函数求导的链式法则，具体地，我们首先对神经网络进行前向传播，即从输入值传播到输出值，然后计算误差，再将误差梯度后向传播，更新参数。 正则化&emsp; 在机器学习中，我们使用许多策略来减少模型在测试集上的误差，即使这些策略可能会增大训练误差，这些策略统称为正则化(normalization)。实际上，正则化就是希望显著地减少方差(varience)而不过度增加偏差(bias)的策略。 参数范数惩罚L2参数正则化&emsp; L2正则化策略，通过向目标函数添加一个正则项 $\Omega(\theta) = \frac{1}{2}||w||_2^2$ ，使权值更加接近原点。与线性回归中添加L2正则项一样，神经网络使用L2正则化，目的是通过允许一定的训练误差，来防止过拟合。 L1参数正则化&emsp; 与L2正则化类似，L1正则化也是通过向目标函数添加一个正则项 $\Omega(\theta)=||w||_1=\sum_i |w_i|$ ，来使得权重进行一定程度的衰减。不同的是，L1正则化回产生更稀疏(sparse)的解，即最终会得到一些权重为0的神经网络，因而对特征选择(feature selection)有一定的作用。 数据集增强&emsp; 与其他的机器学习算法一样，提高神经网络的泛化能力的最好办法是使用更多的数据进行训练，然而，在实际情况中，我们所拥有的数据量是有限的。一种解决方案是创造一些数据添加到训练集中。 &emsp; 在计算机视觉中，图像是高维的并且包含着各种巨大的变化因素，我们可以模拟其中的一些，像是通过放大缩小，移动像素，来模拟现实中同一物体在不同位置的情景。 &emsp; 同时，为了改善神经网络对噪声的健壮性，可以将随机噪声添加到输入中进行训练，使得训练出来的模型在具有一定噪声的情况下也能很好地完成任务。 &emsp; 对于某些模型而言，向输入添加方差极小的噪声等价于对权重施加范数惩罚，而一般情况下，注入噪声远比简单地收缩参数要更强大。于是将噪声添加到权重中也是一种正则化的手段，这项技术主要用于循环神经网络(RNN)。实际上，这种做法是对权重添加了随机扰动，可以反映贝叶斯学习过程的权重不确定性。 &emsp; 在现实的应用场景中，大多数的数据集的标签都存在一定错误，错误的标签是不利于最大化似然的，于是，我们可以通过显式地对标签上的噪声进行建模。例如假设一个小常数 $\epsilon $ ，训练集标签正确的概率是 $1-\epsilon$ ，其他的可能标签也可能是正确的，这样就不用显式地将噪声样本抽取出来。 提前终止&emsp; 当我们训练一个有足够表示能力的模型时，往往可以观察到，训练误差会随着时间的推移逐渐降低，但验证集的误差会再次上升。这意味着，我们应当在验证集误差有所改善时，存储训练好的模型，而当验证集误差在一定循环次数内没有改善时，我们应当终止训练。这种策略被称为提前终止(early stopping)。 &lt;/br&gt; 参考资料： [1] 深度学习[M], 北京: 人民邮电出版社, 2017: 105-156.]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Word2Vec]]></title>
    <url>%2F2018%2F09%2F09%2Fnatural-language-processing%2FWord2Vec%2F</url>
    <content type="text"><![CDATA[Word2Vec&emsp; Word2Vec是一种以无监督的方式来得到词向量的模型，其核心思想是通过上下文信息表达一个词语，将词语从原先的空间映射到新的空间中(本质上是一种降维)。这样得到的词向量相对于独热编码具有维度较低、词语之间相似度易衡量等优点. &emsp; 具体地，Word2Vec首先将词语进行独热编码，然后将编码后的词向量输入神经网络，通过最小化误差来更新权值矩阵，最后将训练好的权值矩阵作为处理后的词向量矩阵. &emsp; Word2Vec主要有CBOW、Skip-gram两种模型. CBOW是根据上下文词语预测当前词；而Skip-Gram正好相反，根据当前词预测上下文. 如下图所示: 对于CBOW，我们的目标函数为： L = \sum_{w\in C} \log p(w|context(w))而对于Skip-gram，目标函数为： L = \sum_{w\in C}\log p(context(w)|w)&emsp; Word2Vec有两个trick，分别是Hierarchical Softmax 和 Nagative Sampling，他们是Word2Vec提高训练效率和效果的两种技巧(trick). 基本流程Hierarchical SoftmaxNagative Sampling]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>自然语言处理</tag>
        <tag>word2vec</tag>
        <tag>词向量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[特征选择]]></title>
    <url>%2F2018%2F08%2F10%2Ffeature-selection%2F</url>
    <content type="text"><![CDATA[特征选择&emsp; 特征选择是数据预处理中非常重要的技术，具体来说，在一般的场景下，数据的特征往往很多，会出现数据样本稀疏、距离计算困难等问题，称为维数灾难(curse of dimensionality). 另外，去除那些不那么重要的特征，能使得重要的特征对结果的预测有更好的作用. 因此，特征的选择就变得非常重要. 具体的，在特征选择中，我们的目标是去除那些与预测结果相关性小的特征，而保留那些相关性大的特征. 过滤式包裹式嵌入式]]></content>
      <categories>
        <category>特征工程</category>
      </categories>
      <tags>
        <tag>特征选择</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性判别分析]]></title>
    <url>%2F2018%2F08%2F10%2Fdescend-dimension%2FLDA%2F</url>
    <content type="text"><![CDATA[线性判别分析(LDA)&emsp; 线性判别分析(Linear Discriminant Analysis，简称LDA)，是一种有监督的降维方法. 其思想是希望投影到超平面上的点具有如下性质： 同类的样本点尽可能地接近 不同类的样本点尽可能远离 如下图所示，右图的降维效果显然要好于左图. (图片来源于博客园的刘建平大神，文末分享链接) 原理二分类情况&emsp; 首先，我们从二分类的情况来解释原理。给定 $n$ 维数据集 $D= \{(x_i,y_i)\}^m_{i=1}, y_i \in \{0,1\}$, 令 $X_i、\mu_i、\Sigma_i$ 分别表示第 $i\in \{0,1\}$ 类的样本的集合、均值向量、协方差矩阵. &emsp; 在二分类的情况下，我们将 $n$ 维数据压缩到 $1$ 维，即数据投影到直线 $w$ 上，则两类样本的中心在直线上的投影分别为：$w^T\mu_0 $ 和 $w^T\mu_1$ ; 两类样本的协方差分别为：$w^T\Sigma_0 w$ 和 $w^T \Sigma_1 w$ . 显然，在一维空间中，它们都是一个实数. &emsp; 回到开头我们讲LDA的两个目标： 同类的样本点尽可能地接近，则同类样本的投影点的协方差尽可能小，即 $w^T\Sigma_0w+w^T\Sigma_1w$ 尽可能小； 不同类样本点尽可能远离，则不同类的中心之间的距离尽可能大，即 $||w^T\mu_0-w^T\mu_1||_2^2$ 尽可能大. 同时考虑二者，我们就可以得到要最大化的目标： \begin{align} J &= \frac{||w^T\mu_0-w^T\mu_1||_2^2}{w^T\Sigma_0w+w^T\Sigma_1w} \\ &= \frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T(\Sigma_0+\Sigma_1)w} \end{align} \tag{1}我们给出 类内散度矩阵(within-class scatter matrix) 的定义 \begin{align} S_w &= \Sigma_0 + \Sigma_1 \\ &= \sum_{x\in X_0}(x-\mu_0)(x-u_0)^T+ \sum_{x\in X_1}(x-\mu_1)(x-\mu_1)^T \end{align} \tag{2}以及 类间散度矩阵(between-class scatter matrix) 的定义 S_b =(\mu_0 -\mu_1)(\mu_0-\mu_1)^T \tag{3}于是 式$(1)$ 可以重写为 J = \frac{w^TS_bw}{w^TS_w w} \tag{4}这个就是 LDA 要最大化的目标函数，即 $S_b$ 与 $S_w$ 的 广义瑞利商(generalized Rayleigh quotient) . &emsp; 注意到式$(4)$的分子分母都是关于 $w $ 的二次项，因此式$(4)$的解与 $w$ 的长度无关，只与其方向有关. 于是，我们可以令 $w^TS_ww=1$ ，可得 \begin{align} \min_w& \quad -w^TS_bw \\ s.t.& \quad w^TS_ww =1 \end{align} \tag{5}应用拉格朗日乘子法，上式等价于 S_bw = \lambda S_ww \tag{6}即 S_w^{-1}S_b w = \lambda w \tag{7}这正好是特征值分解的形式，因此我们可以对 $S_w^{-1}S_b$ 特征值分解，得到的最大的特征值对应的特征向量即为降维后的投影方向 $w$ . 同理，如果是降到 $d$ 维的情况，只需选取最大的 $d$ 个特征值对应的特征向量张成投影矩阵即可. 另外，对于二类的情况，注意到 $S_bw$ 的方向恒为 $\mu_0 -\mu_1$,不妨令 S_bw = \lambda(\mu_0-\mu_1) \tag{8}代入式$(6)$可得 w = S^{-1}(\mu_0-\mu_1) \tag{9}即为最佳的投影方向. 多分类情况&emsp; 设$W=(w_1,w_2,…,w_d)$，即我们将数据压缩到 $d$ 维，$X_j$ 是第 $i$ 类样本的集合，$N_j$ 是第 $ j$ 类样本的个数，将式$(4)$推广到多分类情形时，有 J = \frac{W^TS_bW}{W^TS_w W} \tag{10}其中，$S_b = \sum^k_{j=1}N_j(\mu_j-\mu)(\mu_j-\mu)^T$，$\mu$ 为所有样本的均值向量，$S_w = \sum^k_{j=1}S_{wj}=\sum^k_{j=1}\sum_{x\in X_j}(x-\mu_j)(x-\mu_j)^T$ . &emsp; 问题在于，式$(10)$的分子和分母都是矩阵，而不是标量，没有办法作为一个标量函数来优化. 而针对这个问题，常见的一种方法是采用优化目标 \arg\max_W\frac{tr(W^TS_bW)}{tr(W^TS_w W)} \tag{11}其中 $tr(·)$ 表示矩阵的迹(trace)，即主对角线元素的乘积. 于是，我们可以通过如下问题求解 S_bW = \lambda S_w W \tag{12}正好和二类中的式(6)形式一样，因此我们也是同样采用对 $S_W^{-1}S_b$ 求特征值的方法，选取最大的 $d$ 个特征值对应的特征向量，把它们张成的矩阵作为降维的投影矩阵 $W$. 算法流程 输入：数据集 $D=\{(x_1,y_1),(x_2,y_2),…,(x_m,y_m)\} $，其中任意样本 $x_i$ 为 $n$ 维向量，$y_i \in \{C_1,C_2,…,C_k\}$，降维后的维度 $d$. 计算类内散度矩阵 S_w = \sum^k_{j=1}S_{wj}=\sum^k_{j=1}\sum_{x\in X_j}(x-\mu_j)(x-\mu_j)^T 计算类间散度矩阵 S_b = \sum^k_{j=1}N_j(\mu_j-\mu)(\mu_j-\mu)^T 计算矩阵 S_w^{-1}S_b 对矩阵 $S^{-1}_wS_b$ 进行特征值分解，选取特征值最大的 $d$ 个特征值对应的特征向量，张成投影矩阵 $W =(w_1,w_2,…,w_d)$ for i =1,2,…m do z_i = W^Tx_iend for 得到输出样本集 $D^{‘} = \{(z_1,y_1),(z_2,y_2),…,(z_m,y_m)\}$ . 输出：降维后的样本集 $D^{‘}$ . &lt;/br&gt; 参考资料： [1] 刘建平Pinard. 博客园: 线性判别分析LDA原理总结, https://www.cnblogs.com/pinard/p/6244265.html , 2017-01-03/2018-08-10 . [2] 周志华. 机器学习[M], 北京: 清华大学出版社, 2016: 60-63.]]></content>
      <categories>
        <category>特征工程</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>数据降维</tag>
        <tag>线性判别分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[梯度提升树(GBDT)]]></title>
    <url>%2F2018%2F08%2F08%2Fensemble-learning%2FGBDT%2F</url>
    <content type="text"><![CDATA[梯度提升树(GBDT)&emsp; 提升树(Boosting Tree)是以分类树或回归树为基学习器的 Boosting 算法. 而因为面对不同的学习任务，会有不同的损失函数，而对于一般的损失函数，往往每一步的优化不容易，于是提出了梯度提升(gradient boosting)算法，即梯度提升树(Gradient Boosting Decision Tree, 简称GBDT). 总的来说，GBDT是一种学习性能非常好的算法. 模型&emsp; 我们知道，提升树是由分类树或回归树为基学习器的Boosting方法，因此提升树模型可以表示为决策树的加法模型： f_M(x) = \sum^M_{m=1}T(x;\theta_m) \tag{1}其中，$T(x;\theta_m)$ 表示决策树；$\theta_m$ 为决策树的参数；$M$ 为树的个数. 前向分步算法算法原理&emsp; 首先确定初始提升树 $f_0(x)=0$，第 $m$ 步的模型是 f_m(x) =f_{m-1}(X) + T(x;\theta_m) \tag{2}其中，$f_{m-1}(x)$ 为当前模型，通过经验风险极小化确定下一棵决策树的参数 $\theta$， \theta_m = \arg\min_{\theta_m}\sum^N_{i=1}L(y_i,f_{m-1}(x_i)+T(x_i;\theta_m)) \tag{3}由于树的线性组合可以很好地拟合训练数据，即使数据中的输入与输出之间的关系很复杂，所以提升树是一个高功能的学习算法. &emsp; 给定一个训练集 $D = \{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\}$ ，我们将输入的属性集 $X$ 划分为 $J$ 个互不相交的子集 $R_1,R_2,…,R_J$，并且在每个子集上确定输出的常量 $c_j$，那么树可以表示为： T(x;\theta) = \sum^J_{j=1}c_jI(x\in R_j) \tag{4}其中 $\theta = \{(R_1,c_1),(R_2,c_2),…,(R_J,c_J)\}$ 表示树的划分和所对应划分的输出常量；$J$ 是叶结点的个数. &emsp; 我们使用如下的前向分步算法： \begin{align} f_0(x) &= 0 \\ f_m(x) &= f_{m-1}(x) + T(x;\theta_m) , \quad m=1,2,...,M \\ f_M(x) &= \sum^M_{m=1} T(x;\theta_m) \end{align}于是，在算法的第 $m$ 步，我们需要求解： \theta_m = \arg\min_{\theta_m} \sum^N_{i=1}L(y_i;f_{m-1}(x_i)+T(x;\theta_m))得到 $\theta_m$ ，即第 $m$ 棵树的参数. &emsp; 如果我们使用平方误差函数， L(y,f(x) ) = (y-f(x))^2其损失变为 \begin{align} L&(y,f_{m-1}(x)+T(x;\theta_m)) \\ &= [y-f_{m-1}(x)-T(x;\theta_m)]^2 \\ &= [r-T(x;\theta_m)]^2 \end{align}这里， r = y-f_{m-1}(x) \tag{5}是当前模型拟合数据的残差(residual) . 所以，提升树算法实际上是不断地训练出能拟合上一个树产生的残差的当前树. 算法流程 输入：训练数据集 $D = \{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\}$ 初始化： f_0(x) = 0for $m = 1, 2, …, M$ do &emsp; 计算残差: r_{mi} = y_i - f_{m-1}(x_i) , \ i =1,2,...,N&emsp; 拟合残差 $r_{mi}$ 学习一个回归树，得到 $T(x;\theta_m)$ &emsp; 更新 $f_m(x) = f_{m-1}(x) + T(x;\theta_m)$ end for 得到提升树 f_M(x) = \sum^M_{m=1}T(x;\theta_m)输出：提升树 $f_M(x)$ . &emsp; 需注意的是，无论是面对回归任务还是分类任务，提升树的子树都是 CART 回归树. 因为这样，残差的计算才有意义. 但由于分类问题的损失函数不是平方损失，因此不能由上述的算法简单地求解. 梯度提升算法&emsp; 在上述的的前向分步算法中，当损失函数是平方损失函数时，每一步的优化是很简单的. 但是面对不同的任务时，会有不同的损失函数，对于一般的损失函数而言，每一步的优化往往不那么简单. 于是，Friedman 提出了梯度提升(gradient boosting)算法. 关键是利用损失函数的负梯度在当前模型的值，作为残差的估计，即 r_{mi}=-[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)} 输入：训练数据集 $D =\{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\}$，损失函数 $L(y,f(x))$ 初始化： f_0(x)= \arg\min_c \sum^N_{i=1}L(y_i,c)for $m =1,2,…,M$ do &emsp; for $i=1,2,…,N$ do r_{mi}=-[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}&emsp; end for &emsp; 对 $r_{mi}$ 拟合一个回归树，得到第 $m$ 棵树的叶结点的属性子集 $R_{mj}, j=1,2,…,J$ &emsp; for $ j =1,2,…,J $ do c_{mj} =\arg\min_c \sum_{x\in R_{mj}}L(y_i,f_{m-1}(x_i)+c)&emsp; end for &emsp; update f_m(x) = f_{m-1}(x) + \sum^J_{j=1}c_{mj}I(x\in R_{mj})end for 得到梯度提升树 f_M(x) = \sum^M_{m=1}\sum^J_{j=1}c_{mj}I(x\in R_{mj})输出：梯度提升树 $f_M(x)$ . &gt; 参考资料：[1] 李航. 统计学习方法[M], 北京: 清华大学出版社, 2012: 146-152.]]></content>
      <categories>
        <category>集成学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>集成学习</tag>
        <tag>Boosting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[主成分分析]]></title>
    <url>%2F2018%2F08%2F07%2Fdescend-dimension%2FPCA%2F</url>
    <content type="text"><![CDATA[主成分分析(PCA)&emsp;主成分分析(Principal Component Analysis , 简称 PCA ) 是最常用的一种降维方法. 其求解主要有对样本的协方差矩阵的特征值分解和对原样本矩阵的奇异值分解两种方法. 特征值分解原理&emsp;对于样本空间是高维的情况，我们希望用一个超平面对进行所有样本进行恰当的表达，因此我们希望这个超平面满足如下两点性质： 最近重构性：样本点到这个超平面的距离都足够近 ; 最大可分性：样本点在这个超平面上的投影能尽可能分开 . &emsp;我们基于上述两点性质对PCA中特征值分解的方法进行推导，首先是基于最近重构性的推导： &emsp;我们假设样本点进行了中心化，即 $\sum_i x_i =0$; 再假定投影变换后得到的新坐标系为 $\{w_1,w_2,…,w_d\}$ ，其中 $w_i$ 是标准正交基向量，即 $||w_i||_2 =1 , w_i^Tw_j =0$ . 如果丢弃新坐标系中的部分坐标，即将维度降到 $d^{‘}&lt;d$，则样本点在新坐标系下的投影是 $z_i = (z_{i1};z_{i2};…;z_{id^{‘}})$，其中 $z_{ij}= w^T_jx_i$ 是 $x_i$ 在低维坐标系下第 $j$ 维的坐标. 若基于 $z_i$ 来重构 $x_i$，则会得到 $\hat{x_i} = \sum_{j=1}^{d^{‘}}z_{ij}w_j$ . &emsp; 考虑整个训练集，那么原样本点和基于投影重构的样本点 $\hat{x_i}$ 之间的距离为： \sum^m_{i=1}||\sum^{d^{'}}_{j=1}z_{ij}w_j-x_i||_2^2 = \sum_{i=1}^mz_i^Tz_i-2\sum^m_{i=1}z^T_iW^Tx_i + C \\ \alpha - (W^T(\sum^m_{i=1}x_ix_i^T)W) \tag{1}即PCA的目标为： \begin{align} &\min_W \ - tr(W^T(\sum^m_{i=1}x_ix_i^T)W) \\ &s.t. \ W^TW = I \end{align} \tag{2}&emsp; 我们从最大可分性出发，希望所有样本点的投影尽可能分开，即希望投影后的样本点方程最大化. &emsp; 投影后样本点的方差是 $\sum_i W^Tx_ix_i^TW$ ，于是优化目标可写为： \begin{align} &\max_W \ tr(W^TXX^TW) \\ &s.t. \ W^TW = I \end{align} \tag{3}显然，式 (2) 与 式 (3) 等价 . 于是，使用拉格朗日乘子法可得： XX^Tw_i = \lambda_iw_i \tag{4}算法流程 输入：样本集 $D = \{x_1,x_2,…,x_m\}$; 低维空间维数 $d^{‘}$. 对所有样本进行中心化：$x_i := x_i - \frac{1}{m}\sum^m_{i=1}x_i$ ; 计算样本的协方差矩阵 $XX^T$ ; 对协方差矩阵 $XX^T$ 进行特征值分解 ; 取最大的 $d^{‘}$ 个特征值所对应的特征向量 $w_1,w_2,…,w_{d^{‘}}$. 输出：投影矩阵 $W^\ast =(w_1,w_2,…,w_{d^{‘}})$ . 奇异值分解&emsp;奇异值分解(Sigular Value Decomposition, 简称SVD)，是机器学习广泛应用的算法，这里我们简单介绍奇异值分解的原理，及其在降维中的应用. 定义&emsp; 假设矩阵 $A$ 是一个 $m\times n$ 矩阵，那么我们定义 $A$ 的 $SVD$ 为： A = U\Sigma V^T其中 $U$ 是一个 $m \times m $ 矩阵，$\Sigma$ 是一个 $m\times n $ 矩阵，除了对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，$V $ 是一个 $n\times n$ 矩阵，如下图所示 其中，我们对 $AA^T$ 进行特征分解，有： (AA^T)u_i =\lambda_i u_i于是，将 $AA^T$ 的所有特征向量张成一个 $m\times m$ 矩阵，就是 $U$ 矩阵. 同理，将 $A^TA$ 的所有特征向量张成一个 $n\times n $ 矩阵，就是 $V$ 矩阵. $\Sigma$ 对角线上的每个元素(奇异值) $\sigma_i$ 和特征值 $\lambda_i$ 有如下关系 \sigma_i = \sqrt{\lambda_i}性质&emsp; 奇异值与特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值减少得特别快，很多情况下，前10%甚至1%的奇异值的和就占了全部奇异值之和的99%以上. 所以，我们可以用最大的 k 个奇异值和对应的左右奇异向量来近似描述矩阵. A_{m\times n} = U_{m\times m} \Sigma_{m\times n } V^T_{n\times n} \approx U_{m\times k}\Sigma_{k\times k} V^T_{k\times n}即我们可以用一小部分的$U、V、\Sigma$ 来近似描述 $A$ ，如下图所示 应用于PCA&emsp; 在 PCA 中，我们进行如下处理，右乘矩阵 $V$，即 X^{'}_{m\times d} = X_{m\times n} V_{n\times k}即可得到降维后的数据. 另外，如果我们对原特征矩阵左乘 $U^T$, 即 X^{'}_{d\times n} = U^T_{d\times m} X_{m\times n}则可以对行数进行压缩. &emsp; 有一些算法可以不用先求出协方差矩阵 $X^TX$，也能求出右奇异矩阵 $V$，因此，SVD 要比特征值分解高效. 参考资料：[1] 周志华. 机器学习[M], 北京: 清华大学出版社, 2016: 229-232. [2] 刘建平Pinard. 奇异值分解(SVD)原理与在降维中的应用[EB/OL], http://www.cnblogs.com/pinard/p/6251584.html, 2017-01-05/2018-08-07 .]]></content>
      <categories>
        <category>特征工程</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>数据降维</tag>
        <tag>主成分分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Random Forest]]></title>
    <url>%2F2018%2F08%2F07%2Fensemble-learning%2FRandom-Forest%2F</url>
    <content type="text"><![CDATA[Random Forest介绍&emsp; 我们知道，Bagging 算法在样本选择的时候采用自助采样法，这样的好处在于引入了随机性，使得最终学习器不容易过拟合，而在随机森林(Random Forest) 中，我们进一步引入随机性：在决策树的训练过程中引入随机属性选择. 最终由多个决策树共同组成随机森林. &emsp; 令人惊讶的是，这样一种原理简单的算法，在许多任务中都有非常优秀的性能. 算法流程 输入：训练样本 $D$，决策树数目 $T$，决策树深度 $H$ for $t = 1, 2, …, T$ do &emsp; 自助采样法对样本进行采样，得到采样集 $D_{t}$; &emsp; 随机从 $D_t$ 中选取包含 $k$ 个属性的子集 $D_{t,sub}$; &emsp; 基于 $D_{t,sub}$ 训练决策树 $tree_t$ . end for $forest = (tree_1, tree_2, …, tree_T)$ . 输出：随机森林 forest . 优缺点优点 准确率高. 随机性的引入很好地解决过拟合的问题. 随机性的引入，使随机森林具有很好的抗噪声能力. 可以在不做特征选择的情况下，处理高维度的数据. 能处理离散型数据，也能处理连续型数据，无需规范化数据. 训练速度快，容易实现并行化. 缺点 当决策树的个数很多时，需要的空间和时间比较大. 算法的可解释性不强. 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243import numpy as npimport random as rdfrom math import logfrom cart_tree import build_classify_tree, predict import timeimport pickledef load_data(dir): """ 导入数据 :param dir:数据的路径 :return: 放入学习器的数据 """ tmp = np.loadtxt(dir, dtype=np.str, delimiter=",") data = tmp[1:500, 1:-2].astype(np.float) label = tmp[1:500, -1:].astype(np.float) # 将数据进行标准化 x_std = standard_transform(data) return np.hstack((x_std, label))def standard_transform(data): """ 将数据进行标准化处理 :param data: 原始数据 :return: 处理后的数据 """ return (data - np.mean(data, axis=0))/np.std(data, axis=0)def random_forest_training(data_train, trees_num): """ 训练随机森科 :param data_train: 训练集 :param trees_num: 树的个数 :return: trees_result: 训练好的决策树集 trees_feature: 决策树集对应的特征集 """ trees_result = [] trees_feature = [] n = np.shape(data_train)[1] if n &gt; 2: k = int(log(n, 2)) else: k = 1 for i in range(trees_num): data_samples, feature = choose_samples(data_train, k) # print(data_samples) tree = build_classify_tree(data_samples, min_sample=0, min_gain=0) trees_result.append(tree) trees_feature.append(feature) return trees_result, trees_featuredef choose_samples(data, k): """ 随机选择样本和特征集 :param data: 训练集 :param k: 选取的特征数 :return: data_samples: 样本集 feature: 特征集 """ m, n = np.shape(data) feature = [] for j in range(k): feature.append(rd.randint(0, n - 2)) index = [] for i in range(m): index.append(rd.randint(0, m-1)) data_samples = [] for i in range(m): data_tmp = [] for fea in feature: data_tmp.append(data[index[i]][fea]) data_tmp.append(data[index[i]][-1]) data_samples.append(data_tmp) print(data_samples) return data_samples, featuredef split_data(data_test, feature): """ 根据树中的特征集切分样本 :param data_test: 样本集 :param feature: 特征集 :return: data: 切分后的样本 """ m = np.shape(data_test)[0] data = [] for i in range(m): data_x_tmp = [] for x in feature: data_x_tmp.append(data_test[i][x]) data_x_tmp.append(data_test[i][-1]) data.append(data_x_tmp) # print(data) return datadef get_predict_classify(trees_result, trees_feature, data_test): """ 对训练好的模型进行分类预测 :param trees_result: 树的集合 :param trees_feature: 特征集合 :param data_test: 测试集或验证集 :return: final_predict: 预测结果集 """ m_tree = len(trees_result) m = np.shape(data_test)[0] result = [] for i in range(m_tree): clf = trees_result[i] feature = trees_feature[i] data = split_data(data_test, feature) result_i = [] # print(data) for j in range(m): # print(predict(data[i][0:-1], clf)) pre = list(predict(data[j][0:-1], clf).values()) # print(pre) # print("--------------") result_i.append(pre) result.append(result_i) final_predict = np.sum(result, axis=0) # print("==============") # (np.shape(final_predict)) return final_predictdef cal_correct_rate(data_test, final_predict): """ 计算正确率 :param data_test: 验证集 :param final_predict: 预测结果集 :return: corr/m: 正确率 """ m = np.shape(final_predict)[0] corr = 0.0 # print(final_predict) pre = [] for i in range(m): # print(final_predict[i][0]) # print("--------") # print(final_predict[i][1]) if final_predict[i][0] &gt; final_predict[i][1]: if data_test[i][-1] == 0: corr += 1 pre.append(0) else: pre.append(1) if data_test[i][-1] == 1: corr += 1 # print(corr) # print(m) return corr / mdef save_model(trees_result, trees_feature, result_flie, feature_file): """ 保存模型 :param trees_result: 训练好的随机森林模型 :param trees_feature: 对应的特征集 :param result_flie: 保存的模型路径 :param feature_file: 保存的特征集路径 :return: """ m = len(trees_feature) f_fea = open(feature_file, "w") for i in range(m): fea_tmp = [] for x in trees_feature[i]: fea_tmp.append(str(x)) f_fea.writelines("\t".join(fea_tmp) + "\n") f_fea.close() with open(result_flie, "wb+") as f: pickle.dump(trees_result, f)def load_model(result_file, feature_file): """ 加载模型 :param result_file: 保存的随机森林的模型 :param feature_file: 保存的对应的特征集 :return: trees_result: 随机森林的模型 trees_feature: 随机森林对应的特征 """ trees_feature = [] f_fea = open(feature_file) for line in f_fea.readlines(): lines = line.strip().split("\t") tmp = [] for x in lines: tmp.append(int(x)) trees_feature.append(tmp) f_fea.close() with open(result_file, "rb+") as f : trees_result = pickle.load(f) return trees_result, trees_featuredef train(): """ 模型训练 :return: """ t = time.time() print("--------load data-------") data_train = load_data("train_data.csv") trees_result, trees_feature = random_forest_training(data_train, 140) result = get_predict_classify(trees_result, trees_feature, data_train) corr_rate = cal_correct_rate(data_train, result) print("&#123;:.2%&#125;".format(corr_rate)) print("--------save model-------") save_model(trees_result, trees_feature, "result_file", "feature_file") print("cost time: " + str(time.time() - t))def test(): """ 模型测试 :return: """ t = time.time() print("--------load ver-------") data_test = load_data("verification_data.csv") print("--------load model-------") trees_result, trees_feature = load_model("result_file", "feature_file") print("--------predict-------") result = get_predict_classify(trees_result, trees_feature, data_test) corr_rate = cal_correct_rate(data_test, result) print("&#123;:.2%&#125;".format(corr_rate)) print("cost time: " + str(time.time() - t))if __name__ == '__main__': train() test() 引用的CART算法在CART中有详细代码示例. 注意到此算法随机性较强，每次运行的验证正确率都会有所浮动. 参考资料： [1] 周志华. 机器学习[M], 北京: 清华大学出版社, 2016:179-180. [2] shjyoudp. CSDN: 随机森林算法学习(RandomForest), https://blog.csdn.net/qq547276542/article/details/78304454, 2017-10-21/2018-08-08.]]></content>
      <categories>
        <category>集成学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>集成学习</tag>
        <tag>Bagging</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AdaBoost]]></title>
    <url>%2F2018%2F08%2F06%2Fensemble-learning%2FAdaBoost%2F</url>
    <content type="text"><![CDATA[AdaBoost介绍&emsp; 我们知道 AdaBoost 是一种 Boosting 算法，而 Boosting 的核心问题在于：如何在每一轮改变训练数据的权值或概率分布；如何将弱分类器组合成一个强分类器. 而 AdaBoost 的做法是，提高那些被前一轮弱分类器错误分类的样本权值，而降低那些正确分类样本的权值；采用加权多数表决的方法，加大分类误差率小的弱分类器的权值，减小分类误差率大的弱分类器的权值. 算法流程 输入：训练数据集 $T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中 $x_i \in \chi \subseteq R^n$，$y_i \in \gamma = \{-1,+1\}$；弱学习算法； 初始化训练数据的权值分布： D_1=(w_{11},...,w_{1i},...,w_{1N}), \quad w_{1i} = \frac{1}{N}, \quad i=1,2...,Nfor m = 1, 2, …, M do &emsp;使用具有权值分布 $D_m$ 的训练数据集学习，得到基本分类器 G_m(x) :\chi \rightarrow \{-1,+1\}&emsp;计算 $G_m(x)$ 在训练数据集上的分类误差率 e_m = \sum_{i=1}^NP(G_m(x_i)\neq y_i)=\sum^N_{i=1}w_{mi}I(G_m(x_i)\neq y_i)&emsp;计算 $G_m(x)$ 的系数 \alpha_m = \frac{1}{2} \log \frac{1-e_m}{e_m}&emsp;更新训练数据集的权值分布 w_{m+1,i} = \frac{w_{mi}}{Z_m}exp (-\alpha_my_iG_m(x_i)), \quad i=1,2,...,N \\ D_{m+1} = (w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N})这里，$Z_m$ 是规范化因子 Z_m = \sum^N_{i=1}w_{mi}exp(-\alpha_my_iG_m(x_i))它使 $D_{m+1}$ 成为一个概率分布. end for 构建基本分类器的线性组合 f(x) = \sum^M_{m=1}\alpha_mG_m(x)得到最终分类器 G(x) = sign(f(x)) = sign(\sum^M_{m=1}\alpha_mG_m(x))输出：最终分类器 $G(x)$. 优缺点优点 很好地训练弱分类器，并且将它们线性结合. 可以将不同的分类算法作为弱分类器. 具有很高的精度. 相对于 Bagging，充分考虑了每个分类器的权重. 缺点 迭代次数，即弱分类器数目不好设定，可以通过交叉验证来确定. 数据不平衡导致分类精度下降. 训练耗时. 参考资料： [1] 李航. 统计学习方法[M], 北京: 清华大学出版社, 2012: 138-146.]]></content>
      <categories>
        <category>集成学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>集成学习</tag>
        <tag>Boosting</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集成学习]]></title>
    <url>%2F2018%2F08%2F06%2Fensemble-learning%2Fensemble-learning%2F</url>
    <content type="text"><![CDATA[集成学习(ensemble learning)&emsp;集成学习(ensemble learning)通过构建并结合多个学习器来完成学习任务，有时也被称为多分类器系统(multi-classifier system)、基于委员会的学习(committee-based learning)等. &emsp;集成学习根据学习方法可分为两大类：基学习器间存在强依赖关系、必须串行生成的序列化方法；基学习器不存在强依赖关系、可同时生成的并行化方法. &emsp;集成学习比较常见的有 Boosting、Bagging、Stacking 三种类型，其中它们的主要效果分别是减小偏差(bias)、减小方差(variance)、改进预测. Boosting &emsp; Boosting 是一族可以将弱学习器提升为强学习器的算法. 这族算法的工作机制大致相同： 输入：训练样本 $D$，基学习器数目 $T$，基学习算法 for $t = 1, 2, …, T$ &emsp; 从训练样本 $X$ 训练一个基学习器 ; &emsp; 基于学习表现调整训练样本 $D=D^{‘}$. end for &emsp; 将 $t$ 个基学习器进行加权结合; 输出：基学习器结合后的强学习器. 常见的 Boosting 算法有：AdaBoost、GBDT 等. Q: 为什么说 boosting 算法是减少 bias 而不是 variance? A: 从优化的角度看，boosting 是使用前向分步算法去最小化损失函数，即连续有顺序地最小化损失函数，因此 bias 自然会逐步下降. 但由于各基学习器之间是强相关的，它们的和并不能显著降低 variance. Bagging &emsp; Bagging 全称是 Boostrap aggregating，从名字我们就可以看出，此算法基于自助采样法(boostrap sampling). 具体地，Bagging 算法流程大致如下： 输入：训练样本 $D$，基学习器数目 $T$，基学习算法 for $t = 1, 2, …, T$ &emsp; 自助采样法对样本进行采样，得到采样集 $D_{t}$; &emsp; 基于采样集 $D_t$ 训练一个基学习器. end for &emsp; 将 $t$ 个基学习器进行结合; 输出：基学习器结合后的强学习器. 常见的 Bagging 算法有 Random forest 等. Q: 为什么说 bagging 算法是减少 variance 而不是 bias? A: Bagging 对样本进行自助重采样，得到若干子样本集，由于子样本集的相似性且使用的基学习器模型相同，因此各个基学习器有近似的 bias 和 variance. 由于 $E[\frac{\sum X_i}{n}] = E[X_i]$ ，所以结合后的学习器和单个基学习器的 bias 接近. 另一方面，若各基学习器独立，则 Var(\frac{\sum X_i}{n}) = \frac{Var(X_i)}{n}此时可以显著降低 variance，而若各个基学习器完全相同 Var(\frac{\sum X_i}{n}) = Var(X_i)则不能降低 variance. 而 bagging 由于自助重采样，各个子样本集有部分交集，因此学习到的各个基学习器之间有一定相关性，但相关性不强，处于上述两个极端状态之间，可以一定程度上减小 variance. Stacking &emsp;Stacking 算法从初始数据集中训练出初级学习器，然后将初级学习器的输出作为次级学习器的输入，而样本标记继续使用初始样本的标记. 具体如下： 输入：训练样本 $D$，初级学习器 $h_1,h_2,…,h_T$ for $t = 1, 2, …, T $ &emsp;基于初始样本训练初级学习器 $h_t$ end for for $ i = 1, 2, …, m$ &emsp;for $t = 1, 2, …, T$ &emsp; &emsp;计算初级训练器的训练结果：$z_{it} = h_t(x_i)$ &emsp;end for &emsp;将初级输出结合为次级输入的样本集： &emsp; $D^{‘} = D^{‘} \cup ((z_{i1},z_{i2},…,z_{it},y_i))$ end for 基于样本集 $D^{‘}$ 训练次级学习器 $h^{‘}$. 输出：次级学习器 $h^{‘}$. 参考资料： [1] 周志华. 机器学习[M], 北京: 清华大学出版社, 2016: 171-191. [2] 过拟合. 知乎: 为什么说bagging是减少variance，而boosting是减少bias?[EB/OL] : https://www.zhihu.com/question/26760839, 2017-03-08/2018-08-06.]]></content>
      <categories>
        <category>集成学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[条件随机场]]></title>
    <url>%2F2018%2F08%2F02%2Fprobabilistic-graphical-model%2FCRF%2F</url>
    <content type="text"><![CDATA[条件随机场 首先，概率图模型(Probabilistic graphical model)的体系结构如下： 可见，条件随机场是无向图模型(Unidirected graphical models)中马尔可夫网络(Markov networks)的一种. 具体地，条件随机场(Conditional random field) 是给定随机变量 $X$ 的条件下，随机变量 $Y$ 的马尔可夫随机场. 这里主要介绍定义在线性链上的特殊的条件随机场，称为线性链条件随机场 (Linear chain conditional random field). 线性链条件随机场可以用于标注等问题. 定义 首先给出一般的条件随机场的定义: (条件随机场) 设 $X$ 与 $Y$ 是随机变量，$P(Y|X)$ 是在给定 $X$ 的条件下 $Y$ 的条件概率分布. 若随机变量 $Y$ 构成一个由无向图 $G=(V,E)$ 表示的马尔可夫随机场，即 P(Y_v|X,Y_w,w\neq v) = P(Y_v|X,Y_w,w\sim v) \tag{1}对任意结点 $v$ 成立，则称条件概率分布 $P(Y|X)$ 为条件随机场. 其中，$w\sim v$ 表示表示图 $G=(V,E)$ 中与与结点 $v$ 有边连接的所有结点 $w$, $w\neq v$ 表示结点 $v$ 以外的所有结点. 注意：定义中并没有要求 $X$ 和 $Y$ 具有相同的结构. 但在现实中，一般假设 $X$ 和 $Y$ 有相同的图结构. 即主要考虑线性链的情况如下： 给出线性链条件随机场的定义： (线性链条件随机场) 设 $X=(X_1,X_2,…,X_n)$ , $Y=(Y_1,Y_2,…,Y_n)$ 均为线性链表示的随机变量序列，若在给定随机变量序列 $X$ 的条件下，随机变量序列 $Y$ 的条件概率分布 $P(Y|X)$ 构成条件随机场，即满足马尔可夫性 P(Y_i|X,Y_1,...,Y_{i-1},Y_{i+1},...,Y_n) = P(Y_i|X,Y_{i-1},Y_{i+1}) \tag{2}则称 $P(Y|X)$ 为线性链条件随机场. 无向图 要理解条件随机场，首先我们要引入无向图(一般可指马尔可夫网络)的概念. 如果一个图太大，则我们可以用因子分解(factoriazation)将 $P=(Y)$ 写成若干个联合概率的乘积。 具体来说，即将一个图分为若干个最大团(任何两个结点均有边连接，且不能加入任何一个结点成为更大的 团). 那么我们有： P(Y) = \frac{1}{Z(x)}\prod_c \psi_c(Y_c) \tag{3}其中，$Z(x) = \sum_Y \prod_c \psi_c(Y_c)$，归一化是为了让结果算作概率. 于是，上图中： P(Y) = \frac{1}{Z(x)}(\psi_1(X_1,X_3,X_4)\cdot\psi_2(X_2,X_3,X_4)) \tag{4}其中，$\psi_c(Y_c)$ 是一个最大团 $C$ 上随机变量的联合概率，一般去指数函数： \psi_c(Y_c) = e^{-E(Y_c)}=e^{\sum_k\lambda_kf_k(c,y|c,x)}称$\psi_c(Y_c)$ 为 势函数(potential function). 那么概率无向图的联合概率分布可以因子分解为： \begin{align} P(Y) &= \frac{1}{Z(x)}\prod_c \psi_c(Y_c) \\ &= \frac{1}{Z(x)}\prod_cexp({\sum_k\lambda_kf_k(c,y|c,x)} \\ &= \frac{1}{Z(x)}exp({\sum_c\sum_k\lambda_kf_k(y_i,y_{i-1},x,i)}) \end{align} \tag{5}概率无向图模型的因子分解由 Hammersley-Clifford 定理保证. 此处不展开. 模型建模公式我们给定长度为$T$ 的观测序列 $O=(O_1,O_2,…,O_T)$ 和 状态序列 $I=(I_1,I_2,…,I_T)$. 由式(2)和式(5)我s们可以得到条件随机场的建模公式： \begin{align} P(I|O) &= \frac{1}{Z(O)}\prod_i \psi_i(I_i|O) \\ &= \frac{1}{Z(O)}\prod_i exp(\sum_k\lambda_kf_k(O,I_{i-1},I_i,i)) \\ &= \frac{1}{Z(O)}exp(\sum^T_{i=1}\sum^M_{k=1}\lambda_kf_k(O,I_{i-1},I_i,i)) \end{align} \tag{6}其中 下标 $i$ 表示当前所在结点($token$)的位置. 下标 $k$ 表示这是第几个特征函数，并且每个特征函数都赋予一个权重 $\lambda_k$. 在每个团里，我们为每个 $token_i$ 构造 $M$ 个特征，每个特征执行一定的限定作用，建模的时候再为每个特征函数加权求和. 其中，$Z(O)=\sum_I exp(\sum^T_{i=1}\sum^M_{k=1}\lambda_kf_k(O,I_{i-1},I_i,i))$，表示在所有可能的状态序列上求和. $f_k$ 表示 特征函数. $P(I|O)$ 表示给定了一条观测序列 $O=(o_1,…,o_i)$ 的条件下，所求出来的状态序列$I = (i_1,…,i_i)$ 的概率. 特征函数特征函数 $f_k$ ，实际上是两种特征函数的结合： 转移特征和状态特征. 对建模公式(6)展开，我们有： \begin{align} P(I|O) &= \frac{1}{Z(O)}exp(\sum^T_{i=1}\sum^M_{k=1}\lambda_kf_k(O,I_{i-1},I_i,i)) \\ &= \frac{1}{Z(O)}exp[\sum_{i=1}^T\sum_{j=1}^J\eta_kt_k(O,I_{i-1},I_i,i)+\sum^T_{i=1}\sum^L_{l=1}\mu_ls_l(O,I_i,i)] \end{align} \tag{7}其中： $t_j$ 为 $i$ 处的转移特征，对应权重 $\eta_j$，每个 $token_i$ 都有 $J$ 个特征，转移特征是对前后 $token$ 的限定. 比如： t_{j=1}(O,I_{i-1},I_i,i) = \left \{ \begin{align} &1 \qquad满足特点转移条件，比如前一个token 是“T”\\ &0 \qquad other \end{align} \right. $s_l$ 为 $i$ 处的状态特征，对应权重 $\mu_l$，每个 $token_i$ 都有 $L$ 个特征. 比如 s_{l=1}(O,I_i,i) = \left \{ \begin{align} &1 \qquad满足特点状态条件，比如当前的token 是“R”\\ &0 \qquad other \end{align} \right. 满足特征条件就取值为1，否则没贡献，甚至可以给它打负分. 将它们合在一起，即为特征函数 $f_k$. f_k(O,I_{i-1},I_i,i) = \left \{ \begin{align} &t_k(O,I_{i-1},I_i,i), \qquad k=1,2,...,J \\ &s_{l}(O,I_i,i), \qquad k= J+l; l=1,2,...,L \end{align} \tag{8} \right. 实际上，我们还可以进一步简化， 对转移特征与状态特征在各个位置 $i$ 求和，记作 f_k(O,I) = \sum_{i=1}^T f_k(O,I_{i-1},I_i,i) \tag{9}对应地，权值记作 \lambda_k = \left \{ \begin{align} &\eta_k, \qquad k=1,2,...,J \\ &\mu_l, \qquad k= J+l; l=1,2,...,L \end{align} \tag{10} \right.再进一步简化： 以 $\lambda$ 表示权值向量，即 \lambda = (\lambda_1,\lambda_2,...,\lambda_k)^T \tag{11}以 $F(O,I,i)$ 表示全局特征向量，即 F(O,I)=(f_1(O,I),f_2(O,I),...,f_k(O,I))^T \tag{12}则原来的乘积求和可以表示为向量内积的形式，条件随机场可以简化为： P(I|O) = \frac{exp(\lambda\cdot F(O,I))}{Z_\lambda(x)} \tag{13}其中， Z_\lambda = \sum_I exp(\lambda\cdot F(O,I)) \tag{14}运行过程 预定义特征函数 . 在给定的数据上，训练模型，确定参数 $\lambda_k$ . 用训练好的模型解决预测问题. 概率计算 我们像隐马尔可夫模型那样，引入前向-后向向量，递归地计算概率和期望值. 前向-后向算法 首先，引进特殊的起点和终点状态标记 $I_o=start, I_{T}=stop$. 对 $i=1,2,…,T$ ，定义前向向量 $\alpha_i(O)$ \alpha_0(I|O)= \left \{ \begin{align} &1, \qquad I=start \\ &0, \qquad 否则 \end{align} \tag{15} \right.递推公式为(注意，此处以及后面的上标T为转置的意思)： \alpha_i^T (I_i|O)= \alpha_{i-1}^T(I_{i-1}|O)[M_i(I_{i-1},I_{i}|O)], \quad i=1,2,...T \tag{16}又可表示为： \alpha_i^T(O) = \alpha_{i-1}^T(O)M_i(O) \tag{17}$\alpha_i(I_i|O)$ 表示在位置 $i$ 的标记是 $y_i$ 并且到位置 $i$ 的前部分标记序列的非规范化概率，$y_i$ 可取的值有 $m$ 个，所以 $\alpha_i(O)$ 是 $m $ 维列向量. 其中，我们对观测序列 $O$ 的每一个位置 $i=1,2,…,T$ ，定义一个 $m$ 阶矩阵( $m$ 是标记 $I_i$ 取值的个数) M_i(O) = [M_i(I_{i-1},I_i)|O] \tag{18} Mi(I_{i-1},I_i|O) = exp(W_i(I_{i-1},I_i|O)) \tag{19} W_i(I_{i-1},I_i|O)= \sum^M_{k=1}\lambda_kf_k(I_{i-1},I_i,O,i) \tag{20} 同样地，对 $i=0,1,..,T$ 定义后向向量 $\beta_i(O)$: \beta_T(I_T|O)= \left \{ \begin{align} &1, \qquad I=stop \\ &0, \qquad 否则 \end{align} \tag{21} \right. \beta_i(I_i|O) = [M_i(I_i,I_{i+1}|O)]\beta_{i+1}(I_{i+1}|O) \tag{22}又可表示为： \beta_i(O) = M_{i+1}(O)\beta_{i+1}(O) \tag{23}$\beta_i(I_i|O)$ 表示在位置 $i$ 的标记为 $I_i$ 并且从 $i+1$ 到 $T$ 的后部分标记序列的非规范化概率. 由前向-后向向量定义不难得到： Z(O) = \alpha_T^T(O) \cdot1= 1^T\beta_1(O) \tag{24}这里，$1$ 是元素均为 $1$ 的 $m$ 维列向量. 有前向-后向向量，可以比较方便的计算概率和期望值，这里不进行展开. 学习算法 条件随机场实际上是定义在时序数据上的对数线性模型，可以通过极大似然估计求解参数. 由式(6)和式(9)，可得到训练数据的对数似然函数为 L(\lambda) = \log \prod_{x,y}P_\lambda(I|O) = \sum^T_{j=1} \sum^M_{k=1}\lambda_kf_k(O_j,I_j)-\sum^T_{j=1}\log Z_\lambda(O_j) \tag{25}可以由改进的迭代尺度法IIS、梯度下降法、牛顿法等方法来求解参数，此处不进行展开. 预测算法 条件随机场的预测问题是给定条件随机场 $P(Y|X)$ 和输入序列(观测序列) $x$，求条件概率最大的输出序列(标记序列)的 $y^\ast$，即对观测序列进行标注. 与隐马尔可夫模型的预测方法一样，条件随机场的预测方法也是维特比算法. 输入：模型特征向量 $F(O|I)$(式12) 和权值向量 $\lambda$，观测序列 $O=(O_1,O_2,…,O_T)$. 初始化 \delta_1(j) = \lambda \cdot F_1(I_o=start,I_1=j,O), \quad j=1,2,...,m递推, 对 $i=2,3,…,T$ \delta_i(l) = \max_{1\le j \le m}\{\delta_{i-1}(j)+\lambda\cdot F_i(I_{i-1}=j,I_i=l,O)\},\quad l=1,2,...,m \\ \psi_i(l) = \arg\max_{1 \le j \le m}\{\delta_{i-1}(j)+\lambda\cdot F_i(I_{i-1}=j,I_i=l,O)\} , \quad l=1,2,...,m终止 \max_y (\lambda\cdot F(O,I)) = \max_{1 \le j \le m}\delta_T(j) \\ I_T^\ast= \arg \max_{1 \le j\le m} \delta_T(j)返回路径 I^\ast_i = \psi_{i+1}(I_{i+1}^\ast), \qquad i=T-1,T-2,...,1输出：最优路径 $I^\ast = (I_1^\ast,I^\ast_2,…,I_T^\ast)$. 参考资料： [1] 李航. 统计学习方法[M], 北京: 清华大学出版社, 2012: 191-210 [2] Scofield-知乎. 如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？[EB/OL], https://www.zhihu.com/question/35866596, : 2018-3-21/2018-8-3.]]></content>
      <categories>
        <category>概率图模型</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>判别模型</tag>
        <tag>条件随机场</tag>
        <tag>概率图模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[隐马尔可夫模型]]></title>
    <url>%2F2018%2F07%2F31%2Fprobabilistic-graphical-model%2FHMM%2F</url>
    <content type="text"><![CDATA[隐马尔可夫模型 隐马尔可夫模型(Hidden Markov Model，HMM)，是结构最简单的动态贝叶斯网(dynamic Bayesian network)，是一种著名的有向图模型，主要用于时序数据建模，在语音识别、自然语言处理等邻域有广泛应用. 模型模型变量隐马尔可夫模型中的变量可分为两组： 状态变量 $Q=\{q_1,q_2,…,q_N\}$，其中 $q_i$ 表示第 $i$ 时刻的系统状态，$N$是可能的状态数，通常假定状态变量是隐藏的、不可被观测的，因此状态变量也称为隐变量. 观测变量 $V = \{v_1,v_2,…,v_M\}$，其中 $x_i$ 表示第 $i$ 时刻的观测值，$M$是可能的观测数. 对应地，$I=(i_1,i_2,…,i_T)$ 是长度为 $T$ 的状态序列，$O=(o_1,o_2,…,o_T)$ 是对应的观测序列. 注意，隐马尔可夫模型有如下两个假设： (观测独立性假设) 观测变量的取值仅依赖与状态变量，即 $x_t$ 由 $y_t$ 确定，与其他状态变量及观测变量的取值无关. (齐次马尔可夫假设) $t$ 时刻的状态 $q_t$ 仅依赖于 $t-1$ 时刻的状态 $q_{t-1}$，与此前的 $t-2$ 个状态无关. 即”马尔可夫链(Markov chain)”. 模型参数除了上面的状态信息之外，想要确定一个隐马尔可夫模型，还需要以下3组参数： 状态转移概率矩阵: A = [a_{ij}]_{N\times N} \tag{1}其中， a_{ij} = P(i_{t+1}=q_j|i_t=q_i),\quad i=1,2,...,N; \ j=1,2,...N \tag{2}是在时刻 $t$ 处于状态 $q_i$ 的条件下，在 $t+1$ 时刻转移到状态 $q_j$ 的概率. 输出观测概率矩阵: B = [b_j(k)]_{N\times M} \tag{3}其中， b_j(k) = P(o_t = v_k|i_t=q_j),\quad k=1,2,...,M;\ j=1,2,...N \tag{4}是在时刻 $t$ 处于状态 $q_j$ 的条件下，生成观测 $v_k$ 的概率. 初始状态概率向量: \pi = (\pi_i) \tag{5}其中， \pi_i = P(i_1=q_i), \quad i=1,2,...N \tag{6}是时刻 $t=1$ 处于状态 $q_i$ 的概率. $A,B,\pi$ 称为隐马尔可夫模型的三要素. 因此，隐马尔可夫模型 $\lambda$ 可以用三元符号表示： \lambda = (A,B,\pi) \tag{7}举例假设你是一个医生，眼前有个病人，你的任务是确定他是否得了感冒。 首先，病人的状态($Q$)只有两种：{感冒，没有感冒}。 然后，病人的感觉（观测$V$）有三种：{正常，冷，头晕}。 手头有病人的病例，你可以从病例的第一天确定 $\pi$（初始状态概率向量）； 然后根据其他病例信息，确定 $A$（状态转移矩阵）也就是病人某天是否感冒和他第二天是否感冒的关系； 还可以确定 $B$（观测概率矩阵）也就是病人某天是什么感觉和他那天是否感冒的关系。 观测序列的生成 根据隐马尔可夫模型定义，可以将一个长度为 $T$ 的观测序列 $O=(o_1,o_2,…o_n)$ 的生成过程描述如下： 输入：隐马尔可夫模型 $\lambda =(A,B,\pi)$，观测序列长度 $T$ ; 按照初始状态分布 $\pi$ 产生状态 $i_1$ for t = 1, 2, …, T ​ 按照状态 $i_t$ 的观测概率分布 $b_{i_t}(k)$ 生成 $o_t$ ​ 按照状态 $i_t$ 的状态转移概率分布 $\{a_{i_ti_{t+1}}\}$ 产生状态 $i_{t+1},\quad i_{t+1}=1,2,…N$ end for 输出：观测序列 $O=(o_1,o_2,…o_T)$. 三个基本问题 概率计算问题. 给定模型 $\lambda=(A,B,\pi)$ 和观测序列 $O=(o_1,o_2,…o_T)$，计算在模型 $\lambda$ 下观测序列 $O$ 出现的概率 $P(O|\lambda)$ . 学习问题. 一直观测序列 $O =(o_1,o_2,…,o_T)$，估计模型 $\lambda=(A,B,\pi)$ 参数，使得在该模型下观测序列概率 $P(O|\lambda)$ 最大. 即用极大似然的方法估计参数. 预测问题. 也称为解码(decoding)问题. 已知模型 $\lambda= (A,B,\pi)$ 和观测序列 $O=(o_1,o_2,…,o_T)$，求对给定观测序列条件概率 $P(I|O)$ 最大的状态序列 $I=(i_1,i_2,…i_T)$. 即给定观测序列，求最有可能的对应的状态序列. 概率计算方法前向算法(forward algorithm)定义 给定隐马尔可夫模型 $\lambda$ ，定义到时刻 $t$ 部分观测序列为 $o_1,o_2,…,o_t$ 且状态为 $q_i$ 的概率为前向概率，记作： \alpha_t(i) = P(o_1,o_2,...,o_t,\quad i_t=q_i|\lambda) \tag{8}算法 输入：隐马尔可夫模型 $\lambda$，观测序列 $O$ 初始化 $\alpha_1(i)= \pi_ib_i(o_1),\quad i=1,2,…N \tag{9}$ for t =1, 2, …, T-1 \alpha_{t+1}(i) = [\sum^N_{j=1}\alpha_t(j)a_{ji}]b_i(o_{t+1}),\quad i=1,2,...N \tag{10}end for 计算 $P(O|\lambda)$ P(O|\lambda) = \sum^N_{i=1}\alpha_T(i) \tag{11}输出：观测序列概率 $P(O|\lambda)$ 后向算法(backward algorithm)定义 给定隐马尔可夫模型 $\lambda$，定义在时刻 $t$ 状态为 $q_i$ 的条件下，从 $t+1$ 到 $T$ 的部分观测序列为 $o_{t+1},o_{t+2},…,o_{T}$ 为后向概率，记作： \beta_t(i) = P(o_{t+1},o_{t+2},...,o_T|i_t = q_i,\lambda) \tag{12}算法 输入：隐马尔可夫模型 $\lambda$，观测序列 $O$ 初始化 $\beta_T(1) =1 ,\quad i=1,2,…N \tag{13}$ for t = T-1, T-2, …, 1 \beta_t(i) = \sum^N_{j=1}a_{ij}b_j(o_{t+1})\beta_{t+1}(j) \tag{14}end for 计算 $P(O|\lambda)$ P(O|\lambda) = \sum^N_{i=1}\pi_ib_i(o_1)\beta_1(i) \tag{15}输出：观测序列概率 $P(O|\lambda)$ 一些概率和期望值的计算 给定模型 $\lambda$ 和观测 $O$，在时刻 $t$ 处于状态 $q_i$ 的概率，记 \gamma_t(i) = P(i_t =q_i|O,\lambda) \tag{16}可以通过前向后向概率计算. 事实上， \gamma_t(i) = P(i_t =q_i|O,\lambda) = \frac{P(i_t=q_i,O|\lambda)}{P(O|\lambda)}由前向概率和后向概率的定义可知： \alpha_t(i) \beta_t(i)= P(i_t=q_i ,O|\lambda)于是有 \gamma_t(i) = \frac{\alpha_t(i)\beta_t(i)}{P(O|\lambda)}=\frac{\alpha_t(i)\beta_t(i)}{\sum^N_{j=1}\alpha_t(j)\beta_t(j)}\tag{17} 给定模型 $\lambda$ 和观测 $O$，在时刻 $t$ 处与状态 $q_i$ 且在时刻 $t+1$ 处于状态 $q_j$ 的概率. 记 \xi_t(i,j) = P(i_t=q_i,\ i_{t+1}=q_j|O,\lambda) \tag{18}可以通过前向后向概率计算 \xi_t(i,j) = \frac{P(i_t=q_i,\ i_{t+1}=q_j,O,|\lambda)}{P(O|\lambda)} = \frac{P(i_t=q_i,\ i_{t+1}=q_j,O,|\lambda)}{\sum^N_{i=1}\sum^N_{j=1}P(i_t=q_i,\ i_{t+1}=q_j,O|\lambda)}而 P(i_t=q_i,\ i_{t+1}=q_j,O,|\lambda) = \alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)所以 \xi_t(i,j) == \frac{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{\sum^N_{i=1}\sum^N_{j=1}P(i_t=q_i,\ i_{t+1}=q_j,O|\lambda)} \tag{19} 于是将 $\gamma_t(i)$ 和 $\xi_t(i,j) $ 对各个时刻求和，可以得到一些有用的期望值： 在观测 $O$ 下状态 $i$ 出现的期望值 \sum^T_{t+1}\gamma_t(i) \tag{20} 在观测 $O$ 下由状态 $i$ 转移的期望值 \sum^{T-1}_{t=1} \gamma_t(i) \tag{21} 在观测 $O$ 下有状态 $i$ 转移到 状态 $j$ 的期望值 \sum_{t=1}^{T-1}\xi_t(i,j) \tag{22} 学习方法 这里我们只关注非监督的学习算法，有监督的学习算法在有标注数据的情况下，可以用 极大似然估计 很方便地估计参数. 对于非监督的情况，我们可以将状态变量视作隐变量，那么隐马尔可夫模型实际上上一个含有隐变量的概率模型， P(O|\lambda) = \sum_IP(O|I,\lambda)P(I|\lambda) \tag{23}使用EM算法估计参数，这个算法由Baum和Weich发明，所以被称作Baum-Weich 算法. Baum-Weich 算法 确定完全数据的对数似然函数 所有观测数据写成 $O=(o_1,o_2,…,o_T)$，所有的隐数据写成 $I=(i_1,i_2,…i_T)$，完全数据是 $(O,I)=(o_1,o_2,…,o_T,i_1,i_2,…,i_T)$. 完全数据的对数似然函数是 $log P(O,I|\lambda)$. EM算法的E步：求 $Q$ 函数 $Q(\lambda,\bar{\lambda})$ Q(\lambda,\bar{\lambda})=\sum_Ilog P(O,I|\lambda)P(O,I|\bar{\lambda}) \tag{24}其中，$\bar{\lambda}$ 是隐马尔可夫模型参数的当前估计值，$\lambda$ 是要极大化的隐马尔可夫模型参数. P(O,I|\lambda) = \pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)...a_{i_{T-1}i_T}b_{i_T}(o_T) \tag{25}于是函数 $Q(\lambda,\bar{|\lambda})$ 可以写成 \begin{align} Q(\lambda,\bar{\lambda}) =& \sum_I\log\pi_{i_1}P(O,I|\bar{\lambda}) \\ &+\sum_I(\sum^{T-1}_{t=1}\log a_{i_ti_{t+1}})P(O,I|\bar{\lambda})+ \sum_I(\sum^T_{t=1}\log b_{i_t}(o_t))P(O,I|\bar{\lambda}) \end{align} \tag{26} EM算法的M步： 对式(26)中的每一项分别应用拉格朗日乘子法，可得 \pi_i = \frac{P(O,i_1=i|\bar{\lambda})}{P(O|\bar{\lambda})} \tag{27} a_{ij} = \frac{\sum^{T-1}_{t=1}P(O,i_t=i,i_{t+1}=j|\bar{\lambda})}{\sum^{T-1}_{t=1}P(O,i_t=i|\bar{\lambda})} \tag{28} b_j(k) = \frac{\sum^T_{t=1}P(O,i_t=i|\bar{\lambda})I(o_t=v_k)}{\sum^T_{t=1}P(O,i_t=j|\bar{\lambda})} \tag{29}对各概率分别用 $\gamma_t(i), \xi_t(i,j)$ 表示，可将相应的公式写成： a_{ij} =\frac{\sum^{T-1}_{t=1}\xi_t(i,j)}{\sum^{T-1}_{t=1}\gamma_t(i)} \tag{30} b_j(k) = \frac{\sum^T_{t=1,o_t=v_k}\gamma_t(j)}{\sum^T_{t=1}\gamma_t(j)} \tag{31} \pi_i = \gamma_1(i) \tag{32} 预测方法 隐马尔可夫模型预测主要有两种算法：近似算法与维比特算法(Viterbi algorithm). 近似算法 近似算法的思想是，在每个时刻 $t$ 选择在该时刻最有可能出现的状态 $i^\ast_t$，从而得到一个状态序列 $I^\ast=(i^\ast_1,i_2^\ast,…,i^\ast_{T})$. 算法流程如下： 输入：模型 $\lambda=(A,B,\pi)$ 和观测 $O=(o_1,o_2,…,o_T)$; 计算在时刻 $t$ 处于状态 $q_i$ 的概率 $\gamma_t(i)$ \gamma_t(i) = \frac{\alpha_t(i)\beta_t(i)}{P(O|\lambda)}=\frac{\alpha_t(i)\beta_t(i)}{\sum^N_{j=1}\alpha_t(j)\beta_t(j)} \tag{33}在每一时刻 $t$ 最有可能的状态 $i_t$ 是 i^\ast_t = \arg \max_{1\le i\le N}[\gamma_t(i)], \quad i=1,2,...,T \tag{34}输出：状态序列 $I^\ast=(i^\ast_1,i_2^\ast,…,i^\ast_{T})$. 优点：计算简单 缺点：不能保证预测的状态序列整体是最有可能的状态序列 维特比算法 维比特算法是用动态规划来解决隐马尔可夫模型预测问题，即用动态规划(dynamic programming) 求概率最大路径 (最优路径) . 这时一条路径对于着一个状态序列. 根据动态规划原理，最优路径具有如下性质：如果最优路径在时刻 $t$ 通过结点 $i_t^\ast$ ，那么这一路径从结点 $i_t^\ast$ 到终点 $i_T^\ast$ 的部分路径，对于从 $i_t^\ast$ 到 $i_T^\ast$ 所有可能的部分路径来说，必须是最优的. 依据这一原理，我们只需要从时刻 $t=1$ 开始，递推地计算在时刻 $t$ 状态为 $i$ 的各条部分路径的最大概率，直到得到时刻 $t=T$ 状态为 $i$ 的各条路径的最大概率. 首先导入两个变量 $\delta$ 和 $\psi$ . 定义在时刻 $t$ 状态为 $i$ 的所有单个路径 $(i_1,i_2,…,i_t)$ 中概率最大值为 \delta_t(i) = \max_{i_1,i_2,...,i_{t-1}}P(i_t=i,i_{t-1},...,i_1,\ o_t,...,o_1|\lambda), \quad i=1,2,...,N \tag{35}可得变量 $\delta$ 的递推公式： \begin{align} \delta_t(i) &= \max_{i_1,i_2,...,i_{t}}P(i_{t+1}=i,i_{t},...,i_1,\ o_{t+1},...,o_1|\lambda), \\ &= \max_{1\le j\le N}[\delta_{t-1}(j)a_{ji}]b_i(o_t), \quad i=1,2,...,N; t=1,2,...,T-1 \\ \end{align} \tag{36} 定义在时刻 $t$ 状态为 $i$ 的所有单个路径 $(i_1,i_2,…,i_t)$ 中概率最大的路径的第 $t-1$ 个结点为： \psi_t(i) = \arg \max_{1 \le j \le N} [\delta_{t-1}(j)a_{ji}], \quad i=1,2,...,N下面是算法流程： 输入：模型 $\lambda=(A,B,\pi)$ 和观测 $O=(o_1,o_2,…,o_T)$; 初始化 \delta_1(i) = \pi_ib_i(o_1), \quad i=1,2,...N \\ \psi_1(i) = 0 , \quad i=1,2,...,Nfor t = 2, 3, …, T \delta_t(i) = \max_{1\le j\le N}[\delta_{t-1}(j)a_{ji}]b_i(o_t), \quad i=1,2,...,N \\ \psi_t(i) = \arg \max_{1 \le j \le N} [\delta_{t-1}(j)a_{ji}], \quad i=1,2,...,Nend for 终止 P^* = \max_{1\le i \le N} \delta_T(i) \\ i_T^* = \arg\max_{1\le i\le N}[\delta_T(i)]最优路径回溯 for t = T-1, T-2, …, 1 i_t^* = \psi_{t+1}(i^*_{t+1})end for 输出：最优路径 $I^\ast=(i^\ast_1,i_2^\ast,…,i^\ast_{T})$. 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168#coding:UTF-8import numpy as npclass HMM: """ 隐马尔可夫模型 """ def __init__(self, A, B, pi): self.A = A # 状态转移概率矩阵(N*N) self.B = B # 观测概率矩阵(N*M) self.pi = pi # 初始概率向量(N*1)def cal_forward(hmm, obs): """ 前向算法 :param hmm: 隐马尔可夫模型 :param obs: 观测序列 :return: alpha: 前向概率矩阵 (N*T) """ N = hmm.A.shape[0] T = len(obs) alpha = np.zeros((N, T)) alpha[:, 0] = hmm.pi * hmm.B[:, 0] for t in range(1, T): for i in range(N): alpha[i, t] = np.dot(alpha[:, t-1], hmm.A[:, i]) * hmm.B[i, obs[t]] # print("alpha") # print(alpha) return alphadef cal_backward(hmm, obs): """ 后向算法 :param hmm: 隐马尔可夫模型 :param obs: 观测序列 :return: beta: 后向概率矩阵 (N*T) """ N = hmm.A.shape[0] T = len(obs) beta = np.zeros((N, T)) beta[:, -1:] = 1 for t in reversed(range(T-1)): for i in range(N): beta[i, t] = np.sum(beta[:, t+1] * hmm.A[i, :] * hmm.B[:, obs[t+1]]) return betadef hmm_training(hmm, obs): """ 训练HMM :param hmm: 隐马尔可夫模型 :param obs: 观测矩阵 :return: 0 """ N = hmm.A.shape[0] T = len(obs) alpha = cal_forward(hmm, obs) beta = cal_backward(hmm, obs) gamma = np.zeros((N, T)) for t in range(T): prob = np.sum(alpha[:, t]*beta[:, t]) for i in range(N): gamma[i, t] = alpha[i, t] * beta[i, t] / prob xi = np.zeros((N, N, T)) for t in range(T-1): denominator = np.dot(np.dot(alpha[:, t].T, hmm.A) * hmm.B[:, obs[t + 1]].T, beta[:, t + 1]) # print(denominator) for i in range(N): numerator = alpha[i, t] * hmm.A[i, :] * hmm.B[:, obs[t + 1]].T * beta[:, t + 1].T # print(numerator) xi[i, :, t] = numerator / denominator # print(xi[:, :, :-1]) hmm.pi = gamma[:, 0] hmm.A = np.sum(xi, axis=2) / np.sum(gamma[:, :-1], axis=1).reshape((-1, 1)) sum_gamma = np.sum(gamma, axis=1) M = hmm.B.shape[1] for k in range(M): mask = obs == k hmm.B[:, k] = np.sum(gamma[:, mask], axis=1) / sum_gamma return 0def viterbi(hmm, obs): """ 维特比算法求预测问题 :param hmm: 隐马尔可夫模型 :param obs: 观测序列 :return: p_star: 输出的状态序列的概率 i_star: 输出的状态序列 """ N = np.shape(hmm.B)[0] M = np.shape(hmm.B)[1] T = len(obs) delta = np.zeros((N, T)) psi = np.zeros((N, T)) delta[:, 0] = hmm.pi[:] * hmm.B[:, obs[0]] psi[:, 0] = 0 for t in range(1, T): for i in range(N): delta[i, t] = np.max(delta[:, t-1] * hmm.A[:, i]) * hmm.B[i, obs[t]] max =0 for j in range(N): tmp = delta[j, t-1] * hmm.A[j, i] if tmp &gt; max: max = tmp psi[i, t] = j # print(delta[j, t-1] * hmm.A[j, i]) # print(psi[i, t]) # print(delta) # print(psi) p_star = np.max(delta[:, T-1]) # print(p_star) i_star = np.zeros((T, 1)) i_star[T-1] = np.argmax(delta[:, T-1]) for t in reversed(range(T-1)): i_star[t] = psi[int(i_star[t+1]), t+1] return p_star, i_stardef main(hmm): obs = np.array([0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2]) hmm_training(hmm, obs) prob, i = viterbi(hmm, obs) print("--------A:") print(hmm.A) print("--------B:") print(hmm.B) print("--------PI:") print(hmm.pi) print("--------prob:") print(prob) print("--------I:") print(i) return 0states = ('Healthy', 'Fever')observations = ('Normal', 'Cold', 'Dizzy')# 初始状态概率向量start_probability = [0.6, 0.4]# 状态转移概率矩阵transition_probability = [[0.7, 0.3], [0.4, 0.6]]# 观测概率矩阵emission_probability = [[0.5, 0.4, 0.1], [0.1, 0.3, 0.6]]HMM.A = np.array(transition_probability)HMM.B = np.array(emission_probability)HMM.pi = np.array(start_probability)main(HMM) 参考资料： [1] 李航. 统计学习方法[M], 北京: 清华大学出版社, 2012: 171-189. [2] 周志华. 机器学习[M], 北京: 清华大学出版社, 2016: 319-322. [3] Kevin Chan. 隐马尔科夫模型（HMM）及其Python实现[EB/OL], https://applenob.github.io/hmm.html, 2016-12-15/2018-8-2.]]></content>
      <categories>
        <category>概率图模型</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>生成模型</tag>
        <tag>概率图模型</tag>
        <tag>隐马尔可分模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高斯混合聚类]]></title>
    <url>%2F2018%2F07%2F30%2Fclustering%2FGMM%2F</url>
    <content type="text"><![CDATA[高斯混合聚类&emsp;高斯混合模型(Gaussian Mixed Model，GMM)，是一种采用概率模型来表达聚类模型的模型. 它是多个高斯分布函数的线性组合，理论上可以拟合出任意类型的分布，通常用于解决同一集合下的数据包含多个不同的分布的情况. 示例&emsp;如图，图中的点，显然可以聚类为两个簇，而且两个簇中的点分别通过两个不同的正态分布随机生成而来. 但如果我们用一个二维的高斯分布来拟合图中的分布，得到的会是图中的正态分布椭圆. 这显然是不合理的，因为样本显然应该分为两类. &emsp;这个时候，GMM便可以发挥作用了，如下图，我们使用两个高斯分布来拟合数据，那么我们就可以得到两个不同的椭圆区域，可以将样本分为两个簇. 高斯混合模型&emsp;首先假设样本空间中的随机向量 $x$ 服从高斯分布，则其概率密度函数为 ： p(x) = \frac{1}{(2\pi)^{\frac{n}{2}}|\Sigma|^\frac{1}{2}}exp(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu))其中 $\mu$ 是 $n$ 维均值向量，$\Sigma$ 是 $n\times n$ 的 协方差矩阵. 为了明确显示高斯分布与相应参数的依赖关系，我们将概率密度函数记为 $p(x|\mu,\Sigma)$. 我们可以定义高斯混合分布： p(x) = \sum^k_{i=1} \alpha_ip(x|\mu_i,\Sigma_i) \tag{1}&emsp;此分布由 $k$ 个混合成分组成，每个混合成分对应一个高斯分布，其中 $\mu_i$ 和 $\Sigma_i$ 是第 $i$ 个高斯混合成分的参数，而 $\alpha_i &gt;0$ 为相应的 混合系数(mixture coefficient)，$\sum^k_{i=1}\alpha_i =1$，其表示的是第 $i$ 个混合成分的概率. &emsp;我们令随机变量 $z_j \in \{1,2,…k\}$ 表示生成样本 $x_j$ 的高斯混合成分，其取值未知，则 $P(z_j = i) = a_i$. 根据贝叶斯定理，$z_j$ 的后验分布为： \begin{align} p(z_j=i|x_j) &= \frac{P(z_j=i)\cdot p(x_j|z_j=i)}{p(x_j)} \\ &= \frac{\alpha_i\cdot p(x_j|\mu_i,\Sigma_i)}{\sum^k_{l=1}\alpha_l\cdot p(x_j|\mu_l,\Sigma_l)} \end{align} \tag{2}为方便描述，我们将其记为 $\gamma_{ji}(i=1,2,…k)$ . 则每个样本 $x_j$ 的簇标记 $\lambda_j$ 如下确定： \lambda_j = arg\max_{i \in \{1,2,...,k\}} \gamma_{ji} \tag{3}&emsp;接着，我们采用 极大似然估计，首先给出如下对数似然： \begin{align} LL(D) &= ln(\prod_{j=1}^m p(x_j)) \\ &= \sum^m_{j=1} ln(\sum^k_{i=1}\alpha_i \cdot p(x_j| \mu_i,\Sigma_i)) \end{align} \tag{4}我们使用EM算法来求解. EM算法&emsp;前面的文章已经介绍了EM算法，此处不再赘述. 我们直接使用EM算法迭代来得到高斯混合模型. &emsp;在高斯混合模型的EM算法中，我们令： \begin{align} Q_j(z_j)&=p(z_j=i|x_j) \\ & = \frac{\alpha_i\cdot p(x_j|\mu_i,\Sigma_i)}{\sum^k_{l=1}\alpha_l\cdot p(x_j|\mu_l,\Sigma_l)} \qquad(式(2)) \end{align}可将原对数似然化为: LL(D) = \sum_{j=1}^m log \sum_{i=1}^kQ_j(z_j)\frac{p(x_j,z_j;\theta)}{Q_i(z^{(i)})}由EM算法的结论我们可以知道，参数集合 $\theta$ 的极大似然估计为： \theta := arg\max_\theta \sum_{j=1}^m \sum_{i=1}^k Q_j(z_j)log\frac{p(x_j,z_j;\theta)}{Q_i(z^{(i)})}然后我们对各个参数求偏导为0，即可得到参数. 下面是EM算法实现高斯混合模型的流程. 初始化参数 $\{(\alpha_i,\mu_i,\Sigma_i)|1\le i \le k\}$ Repeat &emsp; (E-step) for j = 1, 2, …, m do \begin{align} \gamma_{ji} := \frac{\alpha_i\cdot p(x_j|\mu_i,\Sigma_i)}{\sum^k_{l=1}\alpha_l\cdot p(x_j|\mu_l,\Sigma_l)} \end{align}&emsp;end for &emsp;&emsp;(M-step) for i = 1, 2, …, k do \begin{align} \mu_i &:= \frac{\sum^m_{j=1}\gamma_{ji}x_j}{\sum_{j=1}^m\gamma_{ji}} \\ \Sigma_i &:= \frac{\sum^m_{j=1}\gamma_{ji}(x_j-\mu_i)(x_j-\mu_i)^T}{\sum_{j=1}^m\gamma_{ji}} \\ \alpha_i &:= \frac{1}{m}\sum^m_{j=1}\gamma_{ji} \end{align}&emsp;end for until convergence 得到高斯混合模型的各个参数后，根据式(3)就可以将样本点 $x_j$划入相应的簇. 参考资料： [1] 阿拉丁吃米粉-CSDN. 高斯混合模型(GMM)及其EM算法的理解[EB/OL], https://blog.csdn.net/jinping_shi/article/details/59613054, 2017-03-02/2018-7-30 [2] 周志华. 机器学习[M], 北京: 清华大学出版社, 2016:206-211.]]></content>
      <categories>
        <category>无监督学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>EM算法</tag>
        <tag>无监督学习</tag>
        <tag>聚类</tag>
        <tag>原型聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EM算法]]></title>
    <url>%2F2018%2F07%2F30%2Foptimization-theory%2FEM%2F</url>
    <content type="text"><![CDATA[EM算法 期望极大算法(expectation maximization algorithm)，简称EM算法，是一种迭代算法，可以用于含义隐变量(hidden variable) 的概率模型参数的极大似然估计，或极大后验概率估计. 延森不等式 在介绍EM算法之前，首先介绍一个重要的不等式——延森不等式(Jensen’s Inequality) 令 $x_1&lt; x_2$ , $f(x)$ 是一个凸函数，且如果它有二阶导数，其二阶导数恒大于等于0，那么有： tf(x_1) + (1-t)f(x) \ge f(tx_1+(1-t)x_2) \quad 0\le t\le1\tag{1} ​ 如果我们在此条件下，假设 $x$ 为随机变量，则有： E[f(x)] \ge f[E(x)] \tag{2} 同理，当 $f(x)$ 为凹函数时，有： E[f(x)] \le f[E(x)] \tag{3} 进一步，如果函数的二阶导数大于0，那么： \begin{align} E[f(x)] = f[E(x)] &\Leftrightarrow x为常量 \\ &\Leftrightarrow x = E(x) \end{align}\tag{4} EM算法问题定义 假设训练集 $\{x^{(1)},x^{(2)},…,x^{(m)}\}$ 是由$m$个独立的无标记样本构成。我们有这个训练集的概率分布模型 $p(x,z;θ)$ ，但是我们只能观察到 $x$ 。我们需要使参数 $θ$ 的对数似然性最大化，即： \begin{align} arg \max_\theta l(\theta) &= arg \max_\theta \sum_{i=1}^m \log P(x^{(i)};\theta) \\ &= arg \max_\theta \sum^m_{i=1} \log \sum_z P(x^{(i)},z^{(i)};\theta) \end{align} \tag{5}形式化过程 具体来说，我们需要每次为函数$\log P(x;\theta)$上的点$\theta$，找到一个凹函数 $g(\theta) \le \log P(x;\theta)$， 每次取 $g(\theta)$ 的最大值点为下一个 $\theta$ ，迭代直到目标函数 $logP(x;\theta)$ 达到局部最大值. 如下图所示： 推导我们假设每一个 $z^{(i)}$ 的分布函数为 $Q_i$，所以有 $\sum_ZQ_i(z)=1, Q_i(z) \ge0$，有： \begin{align} l(\theta) &= \sum_i\log\sum_{z^{(i)}}p(x^{(i)},z^{(i)};\theta) \\ &= \sum_i \log \sum_{z^{(i)}}Q_i(z^{(i)})\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} \\ &\ge \sum_i \sum_{z^{(i)}}Q_i(z^{(i)})\log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} \end{align} \tag{6} 我们知道 $\log$ 是一个凹函数，如果把 $Q_i(z^{(i)})$ 看作随机变量，$\sum_{z^{(i)}}Q_i(z^{(i)}) \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$看作随机变量的概率分布函数 $\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$ 的期望，则由延森不等式(3)可得到上述不等式. 这样以来，$\theta$ 的对数似然函数 $l(\theta)$ 便有了一个下界，但我们希望得到一个更加紧密的下界，也就是使等号成立的情况. 那么有： \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} = c \quad(c为常数) \tag{7}则： Q_i(z^{(i)}) = c*p(x^{(i)},z^{(i)};\theta) \tag{8}又 $\sum_ZQ_i(z)=1, Q_i(z) \ge0$，所以： \sum_ZQ_i(z^{(i)})= \sum_Z c*p(x^{(i)},z^{(i)};\theta)=1 \tag{9}所以： c = \frac{1}{\sum_Z p(x^{(i)},z^{(i)};\theta)} \tag{10}那么再由式(8)，有： \begin{align} Q_i(z^{(i)}) &= \frac{p(x^{(i)},z^{(i)};\theta)}{\sum_Z p(x^{(i)},z^{(i)};\theta)}\\ & = \frac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)};\theta)} \\ &= p(z^{(i)}| x^{(i)};\theta) \end{align} \tag{11}算法流程 repeat ​ (E step) for each i Q_i(z^{(i)}) := p(z^{(i)}| x^{(i)};\theta)​ end for ​ (M step) \theta := arg\max_\theta\sum_i \sum_{z^{(i)}}Q_i(z^{(i)})\log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}until 似然函数达到最大值 参考资料 [1] danerer-CSDN. Andrew Ng机器学习课程笔记（十三）之无监督学习之EM算法[EB/OL] , https://blog.csdn.net/danerer/article/details/80282612#expectation-maximization-algorithm, 2018-5-11/2018-7-30. [2] 维基百科. Jensen’s inequality[DB/OL], https://en.wikipedia.org/wiki/Jensen%27s_inequality, 2018-06-06/2018-7-30.]]></content>
      <categories>
        <category>优化理论</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>EM算法</tag>
        <tag>无监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k-means算法]]></title>
    <url>%2F2018%2F07%2F30%2Fclustering%2FK-Means%2F</url>
    <content type="text"><![CDATA[k均值算法 k均值(k-means)算法是机器学习中一种较为常见的聚类算法. 原理 给定样本集 $D=\{x_1, x_2, …, x_m\}$ , 我们针对 聚类(cluster)所得簇划分 $C=\{C_1,C_2,…,C_k\}$ , 最小化平方误差： E = \sum^k_{i=1}\sum_{x\in C_i}||x-\mu_i||_2^2 \tag{1}其中 $\mu_i = \frac{1}{|C_i|}\sum_{x\in C_i}x$ 是簇 $C_i$ 的均值向量. 最小化式(1)并不容易，找到它的最优解需要考察样本集 $D$ 所有可能的簇划分，这是一个NP难问题. 因此k-means 采用了贪心策略，通过迭代来近似求解式(1). 算法流程 输入：样本集 $D=\{x_1,x_2,…,x_m\}$; ​ 聚类簇数 k. 过程： 从 D 中随机选择 k 个样本作为初始均值向量 $\{\mu_1,\mu_2,…,\mu_k\}$ repeat &emsp; 令 $C_i = \phi (1 \le i \le k)$ &emsp;for j = 1, 2, …, m do &emsp; &emsp; 计算样本 $x_j$ 和各个均值向量之间的距离, 根据距离最近的均值向量, 将 $x_j$ 划入相应的簇. &emsp; end for &emsp; for i = 1, 2, …, k do &emsp; &emsp; 计算新的均值向量：$\mu^`_i = \frac{1}{|C_i|}\sum_{x\in C_i}x$ &emsp; if $\mu^`_i \neq \mu_i$ then &emsp; &emsp; 将当前均值向量更新为 $\mu^`_i$ &emsp; else &emsp; &emsp; 保持当前向量不变 &emsp; &emsp; end if &emsp; end for until 当前均值向量均未更新 输出：簇划分 $C = {C_1,C_2,…C_k}$ ​ 参考资料： [1] 周志华. 机器学习[M], 北京: 清华大学出版社, 2016: 202-204. [2] 维基百科. k-means clustering[DB/OL], https://en.wikipedia.org/wiki/K-means_clustering, 2018-7-14/2018-7-30.]]></content>
      <categories>
        <category>无监督学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>无监督学习</tag>
        <tag>聚类</tag>
        <tag>原型聚类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[约束优化方法]]></title>
    <url>%2F2018%2F07%2F24%2Foptimization-theory%2FCO%2F</url>
    <content type="text"><![CDATA[约束优化方法(Constrained Optimization ) 在优化问题中，往往会有约束条件，即我们希望在约束条件下求得优化问题的最优解。对于等式约束的优化问题，我们可以直接应用拉格朗日乘子法去求取最优值；对于含有不等式约束的优化问题，则可以转化为在满足 KKT 约束条件下应用拉格朗日乘子法求解。 约束优化无约束优化 我们首先来看一个无约束的优化问题： \min_x f(x) \tag{1}其中，$x\in R^N$，$f(x)$是凸函数，这种问题求解很简单，只需要找到令 $\nabla_xf(x) =0$ 的 $x$ 点即可. 等式约束下面，我们给优化问题加上一个等式约束： \begin{align} &\min_x f(x) \\ &s.t \quad h_i(x) = 0 , \quad i=1,2,...,m \end{align} \tag{2}其中，$x$ 为 $n$ 维向量. 为了直观地看这个问题，我们下面假设这是一个二维的等式约束优化问题，如下图所示： 图中虚线是 $f(x,y)$ 的等值线，而约束条件 $h(x,y)=0$ 是约束条件. 我们知道 $f(x,y)$ 的最优化问题需要在 $h(x,y) = 0 $ 的条件下求解，即图中虚线要与绿线相交或相切. 而显然我们可以看到，当虚线与绿线相切时，在约束条件 $h(x,y) =0$ 下， $f(x,y)$ 取得最小值. 这时，我们看到，两函数的法向量平行的，即有： \nabla_{x,y} f(x,y) + \alpha\nabla_{x,y}h(x,y) = 0 \\ \tag{3}拓展到 $n$ 维有： \nabla_{x} f(x) + \alpha\nabla_{x}h(x) = 0 \\ \tag{4}又有 $h_{i}(x) = 0$，联立即得此优化问题的最优解. 于是，我们可以令原函数： L(x,\alpha) = f(x) + \sum^m_{i=1}\alpha_i h_i(x) \tag{5}即问题转化为了求函数 $L(x)$ 的最优解. 即： \begin{align} \nabla_xL(x,\alpha) = 0 \\ \nabla_\alpha L(x,\alpha) = 0 \end{align} \tag{6}求得 $x$ 和 $\alpha$ 后，将 $x$ 代入 $f(x)$ 中即得到可行解，这便是拉格朗日乘子法(Lagrangian Multiplier Method)，其中 $\alpha=\alpha_1^T,\alpha_2^T,…,\alpha_m^T$ 称为拉格朗日乘子. 不等式约束那么，如果加入不等式约束： \begin{align} &\min_x f(x) \\ &s.t \quad g_i(x) \le 0 , \quad i=1,2,...,m \end{align} \tag{7}同样的，我们把问题简化为二维： 显然，可行解有可能落在 $g(x) \le 0$ 里面或者是边界上，即如下两种情况： 可见： 当可行解 $x$ 落在 $0&lt;g(x)&lt;0$ 的区域内，此时直接极小化 $f(x)$ 即可； 当可行解 $x$ 落在 $g(x)=0$ 即边界上，此时等价于等式约束优化问题. 于是，我们可以得到： \beta g(x) = 0 \tag{8}即当 $\beta =0$ 时，此时约束不起作用； $\beta \neq 0$ 时，等价于等式约束. 另外，在等式约束中，我们并没有对乘子加以约束，但在不等式约束中，有： 从图中可以看到，若要最小化 $f(x)$，约束域 $g(x) \le 0$ 的法向量应当与 $f(x)$ 的负梯度同向，即： \begin{align} -\nabla_x f(x) &= \beta \nabla_x g(x) \\ \beta &\ge 0 \end{align} \tag{9}于是，与等式约束优化问题类似，不等式约束的优化问题，在满足一定条件下，便可以用拉格朗日乘子法解决，此条件即为KKT条件(Karush-Kuhn-Tucker conditions). 我们给出形式化的约束优化问题： \begin{align} \min_x &f(x) \\ s.t. \ &h_i(x) = 0, \qquad i=1,2,...,m\\ &g_j(x) \le 0, \qquad j=1,2,...,k\\ \end{align} \tag{10}写出拉格朗日函数： L(x,\alpha,\beta) = f(x) + \sum^m_{i=1}\alpha_i h_i(x) + \sum^k_{j=1}\beta g_i(x) \tag{11}综合上面的分析，我们有以下的KKT条件： \begin{align} \nabla_x L(x,\alpha,\beta) = 0 \\ \beta_j g_j(x) =0 \\ h_i(x) = 0 \\ g_j(x) \le 0 \\ \beta_j \ge0 \end{align} \tag{12}满足 KKT 条件后极小化拉格朗日函数，即可得到在不等式约束条件下的可行解。 对偶问题 在优化理论中，对目标函数 $f(x)$ 的在约束条件下的最优化问题，可以转化为一个与之对应的对偶问题(dual problem)，而原来的问题称为原始问题(primal problem)，而对偶问题有如下几个良好的性质： 对偶问题的对偶是原始问题； 无论原始问题是否是凸的，对偶问题都是凸优化问题； 对偶问题可以给出原始问题一个下界； 当满足一定条件时，原始问题与对偶问题的解是完全等价的. 原始问题我们给定不等式约束优化问题： \begin{align} \min_x &f(x) \\ s.t. \ &h_i(x) = 0, \qquad i=1,2,...,m\\ &g_j(x) \le 0, \qquad j=1,2,...,k\\ \end{align}定义一个拉格朗日函数： L(x,\alpha,\beta) = f(x) + \sum^m_{i=1}\alpha_i h_i(x) + \sum^k_{j=1}\beta_j g_j(x)我们知道 $h_i(x) =0$，且 $\beta_j \ge0 ,g_j(x)\le0$ 即 $\beta_j g_i(x) \le0$，所以有： f(x) = \max_{\alpha,\beta;\beta\ge0} L(x,\alpha,\beta) > L(x,\alpha,\beta) \tag{13}于是我们的优化问题转化为： \min_x f(x) = \min_x \max_{\alpha,\beta;\beta\ge0} L(x,\alpha,\beta) \tag{14}这便是我们的原始问题. 对偶问题 我们将原始问题的解记为 $p^{\ast}$，这里定义一个对偶函数： D(\alpha,\beta) = \min_xL(x,\alpha,\beta) \tag{15}然后我们定义对偶问题： \max_{\alpha,\beta;\beta\ge0} \min_x L(x,\alpha,\beta) \tag{16}接着我们定义对偶问题的最优解为 d^{*} =\max_{\alpha,\beta;\beta\ge0}D(\alpha,\beta) \tag{17}易有： d^* \le p^* \tag{18} 我们把这个性质叫做弱对偶性(weak duality)，对所有优化问题都成立，即使原始问题非凸. 之前我们提到过，无论原始问题是什么形式，对偶问题总是凸的，所以由弱对偶性，我们通过求解对偶问题可以得到原始问题的一个下界. 与弱对偶性相对的有一个强对偶性(strong duality)，即 d^* = p^* \tag{19}于是，满足具有强对偶性的问题，我们可以通过求解对偶问题得到原始问题的解. 但我们需要一些条件，使得强对偶性成立，比如Slater条件和KKT条件. Slater条件：存在x，使得不等式约束 $g(x)\le0$ 严格成立，即 $g(x) &lt;0$. 如果原始问题是凸优化问题并且满足 Slater 条件的话，那么强对偶性成立。需要注意的是，这里只是指出了强对偶成立的一种情况，并不是唯一的情况。例如，对于某些非凸优化的问题，强对偶也成立. (SVM 中的原始问题 是一个凸优化问题(二次规划也属于凸优化问题), Slater 条件在 SVM 中指的是存在一个超平面可将数据分隔开 ). 但为了保证 $d^{_*}$ 是最优解, 还需要满足KKT条件： 我们假设 $x^{\ast}$ 与 $\alpha^{\ast},\beta^{\ast}$ 分别是原始问题（并不一定是凸的）和对偶问题的最优解，且满足强对偶性，则相应的极值的关系满足： \begin{aligned} f(x^*) &= d^* = p^* =D(\alpha^*,\beta^*) \\ &=\min_x f(x)+ \sum_{i = 1}^m \alpha_i^*h_i(x) + \sum_{j=1}^n\beta_j^*g_j(x) \\ & \le f(x^*)+ \sum_{i = 1}^m \alpha_i^*h_i(x^*) + \sum_{j=1}^n\beta_j^*g_j(x^*) \\ &\le f(x^*) \end{aligned} \tag{20}看到第一个不等式，我们知道 $h_i(x)=0$ , 可以得到： \sum_{j=1}^n\beta_j^*g_j(x^*) \ge 0 \tag{21}又有：$ \sum_{j=1}^n\beta_j^\ast g_j(x^\ast) \le 0 $ , 所以 \sum_{j=1}^n\beta_j^*g_j(x^*) = 0 \tag{22}由因为 $x^{\ast}$ 是 $f(x)$ 的极小值点，所以 \nabla_x L(x,\alpha,\beta) = 0 \tag{23}结合上述和一开始的约束条件，有： \begin{align} \nabla_x L(x,\alpha,\beta) = 0 \\ \beta_j g_j(x) =0 \\ h_i(x) = 0 \\ g_j(x) \le 0 \\ \beta_j \ge0 \end{align} \tag{24}即KKT条件. 参考资料： [1] ooon-博客园. 约束优化方法之拉格朗日乘子法与KKT条件[EB/OL]. http://www.cnblogs.com/ooon/p/5721119.html , 2016-07-30/2018-7-30. [2] ooon-博客园. 拉格朗日对偶[EB/OL]. http://www.cnblogs.com/ooon/p/5723725.html , 2016-07-31/2018-7-30. [3] feilong_csdn-CSDN. 支持向量机（SVM）必备知识(KKT、slater、对偶）[EB/OL]. https://blog.csdn.net/feilong_csdn/article/details/62427148, 2017-03-16/2018-7-30.]]></content>
      <categories>
        <category>优化理论</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>数学</tag>
        <tag>凸优化</tag>
        <tag>微积分</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机]]></title>
    <url>%2F2018%2F07%2F24%2Fdiscrimination-model%2FSVM%2FSVM%2F</url>
    <content type="text"><![CDATA[支持向量机(Support Vector Machine)介绍 在 感知机(Perceptron) 算法中，我们寻求一个超平面将两类数据分开，但显然这样的超平面可以有很多个，那么哪个超平面才是最好的呢？很显然，那个距离两边的点最远的超平面，能够更好地将样本分类. 这便是 支持向量机(Support Vector Machine) 的基本型. 如图，$H_2$ 和 $H_3$ 都可以将两类数据分离，但显然 $H_3$ 更好. 线性可分支持向量机介绍​ 我们给定线性可分训练数据集，希望得到一个分离超平面为 w^* \cdot x + b^ * = 0 \tag{1}以及相应的分类决策函数 f(x) = sign(w^*\cdot x + b^*) \tag{2}把它称为线性可分支持向量机. 其中 sign(t) = \left \{ \begin{align} &+1, \quad t \ge0 \\ &-1, \quad t 0 \rightarrow E_2 = min(E_i) \\ E_1 < 0 \rightarrow E_2 = max(E_i) \\ \alpha_2 = arg\max_{\alpha_i \in \alpha}(|E_1 - E_2|) \end{align} 变量求解 在SMO算法中，我们有： \alpha_2^{new,unc} = \alpha_2^{old} + \frac{y_2(E_1-E_2)}{\eta} \\ \eta = K_{11} + K_{22} - K_{12}接着，我们计算上下界 $L$ 和 $H$： 如果 $y_1 \neq y_2$: L=max(0,\alpha_2^{old}-\alpha_1^{old}) \\ H = min(C, C+\alpha_2^{old}-\alpha_1^{old}) \\如果 $y_1 = y_2 $: L=max(0,\alpha_2^{old}+\alpha_1{old}-C) \\ H= min(C, \alpha_2^{old}+\alpha_1^{old})那么有： \alpha_2^{new} = \left \{ \begin{matrix} H, \qquad \alpha_2^{new,unc} > H\\ \alpha_2^{new,unc}, \qquad L\le\alpha_2^{new,unc} \le H\\ L, \qquad \alpha_2^{new,unc}]]></content>
      <categories>
        <category>判别模型</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>判别模型</tag>
        <tag>支持向量机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[感知机]]></title>
    <url>%2F2018%2F07%2F20%2Fdiscrimination-model%2FSVM%2Fperceptron%2F</url>
    <content type="text"><![CDATA[感知机(perceptron)介绍 ​ 感知机是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1二值. 感知机对应于输入空间（特征空间）中将实例划分为正负两类的分离超平面，属于判别模型. —— 李航 《统计学习方法》 感知机模型定义 我们定义从输入空间$\{ x \in \chi \subseteq R^n \}$到输出空间$\{y\in \gamma =\{+1,-1\}\}$的如下函数： f(x) = sign(w\cdot x+b) \tag{1}称为感知机. 其中，$w \in R^n$ 称为权值(weight)，$b\in R$ 称为偏置(bias). sign是符号函数，即 sign(x) = \left \{ \begin{align} &+1, \quad x \ge0 \\ &-1, \quad x0于是，误分类点到超平面 $S$ 的距离为： -\frac{1}{||w||}y_i(w\cdot x_i+b) 于是，假设误分类点的集合为 $M$, 那么所有误分类点到超平面 $S$ 的总距离为： -\frac{1}{||w||}\sum_{x_i\in M}y_i(w\cdot x_i + b)不考虑 $\frac{1}{||w||}$ 则感知机的损失函数为： L(w,b) = -\sum_{x_i \in M} y_i(w\cdot x_i +b) \tag{4}于是，感知机的学习策略找到最小化损失函数式(4)的参数 $w,b$ . 学习算法原始形式 我们知道感知机的目标是最小化损失函数： L(w,b) = -\sum_{x_i \in M} y_i(w\cdot x_i +b) \tag{4}那么我们使用梯度下降法，得到参数 $w,b$ 的更新方程： \begin{align} w &: = w + \eta y_ix_i \\ b &:= b + \eta y_i \end{align} \tag{5}其中 $\eta$ 称为学习率(learning rate). 具体步骤： 选取初值 $w_0, b_0$. 选取数据 $(x_i,y_i)$. 如果 $y_i(w \cdot x_i+b) \le 0$，则以式(5)更新参数. 转到 2. 直到没有误分类点. 对偶形式 感知机学习算法的原始形式和对偶形式与支持向量机(SVM, Support Vector Machine) 算法的原始形式和对偶形式相对应. 由原始形式，我们得到参数的更新方程 \begin{align} w &: = w + \eta y_ix_i \\ b &:= b + \eta y_i \end{align}我们假设参数初值为0，更新 $n$ 次，那么最后学习到的 $w,b$ 可以表示为 \begin{align} w &= \sum^N_{i=1} \alpha_i y_ix_i \\ b &= \sum^N_{i=1} \alpha_i y_i \end{align} \tag{6}这里$\alpha_i = n_i\eta$. 于是，感知机模型： f(x) = sign(\sum^N_{j=1}\alpha_jy_jx_j \cdot x +b) \tag{7}具体实现步骤如下： $\alpha =0 , b =0$. 在训练集中选取 $(x_i,y_i)$. 如果 $y_i(\sum^N_{j=1}\alpha_jy_jx_j \cdot x_i +b) \le 0$ \begin{align} \alpha_i :&= \alpha_i + \eta \\ b :&= b + \eta y_i \end{align} \tag{8} 转到 2. 直到没有误分类数据. 注意到训练过程中实例以内积的形式出现，即我们可以预习把训练集的内积计算出来，并以矩阵的形式存储，即 Gram 矩阵 . G = [x_i \cdot x_j]_{N \times N} 参考资料 [1] 李航. 统计学习方法[M]. 北京: 清华大学出版社, 2012: 25-36.]]></content>
      <categories>
        <category>判别模型</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>判别模型</tag>
        <tag>感知机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[信息论基础]]></title>
    <url>%2F2018%2F07%2F20%2Finformation-theory%2Finformation_theory%2F</url>
    <content type="text"><![CDATA[信息论基础信息熵(Information Entropy) ​ 信息论认为，一条信息的信息量与其不确定性有着直接的关系. 举个例子，如果要问今年的世界杯冠军是哪支球队，答案是肯定的：法国队；但如果问下一届世界杯的冠军是哪支球队，这个问题就会有很多答案，甚至哪些球队能进入世界杯决赛圈都是难以猜测的，因此，下届世界杯冠军得主这个事件就具有了更多的信息. 对于信息量的度量, 香农(Claude Shannon)提出了信息熵的概念, 具体公式如下: H(X) = -\sum_{x\in X}P(x)logP(x)即事件的不确定性越大, 信息熵就越大, 信息量也就越多. 条件熵(Conditional Entropy) ​ 我们知道, 知道的信息越多, 事件的不确定性就越低, 那么假如我们知道了该事件的相关事件的一些信息, 那么是不是就意味着我们也能降低此事件的不确定性呢? 为了证明这些相关的信息能够消除不确定性, 我们引入一个条件熵的概念. 假定 $X$ 和 $Y$ 是两个随机变量, $X$ 是我们需要了解的. 假定我们知道了 $X$ 的随机分布 $P(X)$ , 那么也就知道了 $X$ 的熵: H(X) = -\sum_{x\in X}P(x)logP(x)假如我们还知道了 $Y$ 的一些信息, 包括它和 $X$ 的联合概率分布(Joint Probability), 以及在 $Y$ 取不同值的前提下 $X$ 的概率分布, 即条件概率分布(Conditional Probability). 那么定义在 $Y$ 的条件下的条件熵为: H(X|Y) = -\sum_{x\in X, y\in Y}P(x,y)logP(x|y)可以证得 $H(X) \ge H(X|Y)$, 即多了 $Y$ 的信息后, 关于 $X$ 的不确定性下降了. 同样我们也可以将这个结论拓展到多元模型的情况. 互信息(Mutual Information) ​ 我们知道, 当我们获取的信息跟我们要研究的事物”有关系”时, 这些信息才能帮助我们消除不确定性. 而”有关系” 这一说法过于模糊, 我们希望能够量化地度量”相关性”. 香农在信息论中提出了一个互信息的概念, 作为两个事件”相关性”的量化度量. 假定有两个随机事件 $X$ 和 $Y$, 它们的互信息定义如下: I(X;Y) = \sum_{x\in X,y\in Y}P(x,y)log \frac{P(x,y)}{P(x)P(y)}我们可以证明: I(X;Y) = H(X) - H(X|Y) 所以,所谓两个事件相关性的量化度量, 就是在了解其中一个 $Y$的前提下, 对消除另一个 $X$ 不确定性所提供的信息量. 需注意的是, 互信息是一个取值在0到$min(H(X),H(Y))$ 之间的函数, 当 $X$ 和 $Y$ 完全相关时, 它的取值是0, 同时$H(X) = H(Y)$; 当二者完全无关时, 它的取值是0. 交叉熵(相对熵 Relative Entropy) 与随机变量的互信息不同, 交叉熵用来衡量两个取值为正数的函数的相似性, 它的定义如下: KL(f(x)||g(x)) = \sum_{x\in X}f(x)log\frac{f(x)}{g(x)}有如下3条结论: 对于两个完全相同的函数, 它们的相对熵等于零. 相对熵越大, 两个函数差异越大; 反之, 相对熵越小, 函数差异越小. 对于概率分布或者概率密度函数, 如果取值均大于零, 相对熵可以度量两个随机分布的差异性. 需要指出相对熵是不对称的: KL(f(x)||g(x)) \neq KL(g(x)||f(x))为了让它对称, 詹森和香农提出一种新的相对熵的计算方法, 将上面的不等式两边取平均, 即: JS(f(x)||g(x)) = \frac{1}{2} [KL(f(x)||g(x)) + KL(g(x)||f(x))] 参考资料: [1] 吴军. 数学之美[M]. 北京: 人民邮电出版社, 2014: 60-71.]]></content>
      <categories>
        <category>信息论</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>信息论基础</tag>
        <tag>数学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[k近邻法]]></title>
    <url>%2F2018%2F07%2F19%2Fdiscrimination-model%2Fk-NN%2Fk-NN%2F</url>
    <content type="text"><![CDATA[$k$近邻法(k-nearest neighbor, k-NN)介绍 ​ $k$近邻算法简单、直观：给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的$k$个实例，这k个实例的多数属于某个类，就把该输入实例分为这个类. —— 李航 《统计学习方法》 模型 $k$近邻法的三个要素分别是：$k$ 值的选择、距离度量和分类决策规则. $k$值的选择 $k$值表示选取实例点附近的 $k$ 个点进行分类决策. $k$值的选择对分类结果的影响非常大. 我们可以看到下图的实例，其中蓝色的正方形和红色的三角形都是有确定标签的点，而绿色的圆形是我们待分类的点. 假如我们令$k=3$，即我们选取与绿点最近的3个点，显然此时集合中红点要比蓝点多，则我们将绿点分类为了与红色三角形同一类；而假如我们令$k=5$，则蓝点要比红点多，即我们将绿点与蓝色正方形分为一类. 距离度量 由不同的距离度量，会确定不同的最近邻点，从而影响分类结果. 在 $k$近邻法中，一般用到的是 欧式距离： L(x_1,x_2) = (x_1-x_2)^2但也有更一般的$L_p$ 距离(L_p distance). 设特征空间 $X$ 是 $n$ 维实数向量空间 $R^n$，$x_i,x_j\in X$，$x_i = (x_i^{(1)},x_i^{(2)},…,x_i^{(n)})^T,$，$x_j = (x_j^{(1)},x_j^{(2)},…,x_j^{(n)})^T$，$x_i,x_j$的$L_p$距离定义为 L_p(x_i,x_j) = (\sum^n_{l=1}|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}这里$p \ge 1$，当 $p=2$ 时，即欧式距离(Euclidean distance): L_2(x_i,x_j) = (\sum^n_{l=1}|x_i^{(l)}-x_j^{(l)}|^2)^{\frac{1}{2}}当 $p=1$ 时，称为曼哈顿距离(Manhattan distance): L_1(x_i,x_j) = \sum^n_{l=1}|x_i^{(l)}-x_j^{(l)}|当 $p=\infty$ 时，它是各个坐标距离的最大值: L_\infty(x_i,x_j) = \max_l|x_i^{(l)}-x_j^{(l)}|如图： 分类决策规则 $k$近邻算法中的分类决策规则往往是多数表决，即由输入实例的 $k$ 个邻近的点中的大多数决定输入实例的类. $kd$ 树(kd tree)介绍 当我们要实现 $k$ 近邻算法时，最主要的问题就是如何对训练数据进行快速的k近邻搜索. 最简单的方法就是计算输入的实例与每一个训练实例的距离，但这是一种非常耗时的方法. 于是我们考虑使用二叉树的数据结构，来实现对训练数据的快速搜索. 即 $kd$ 树. ​ $kd$ 树是一种对$k$维空间中的实例点进行存储以便对其进行快速检索的树形数据结构. $kd$ 树是二叉树，表示对$k$维数据空间的一个划分(partition). 构造 $kd$ 树相当于不断地用垂直于坐标轴的超平面将$k$维空间划分，构成一系列的$k$维超矩形区域. $kd$ 树的每一个结点对应于一个$k$维超矩形区域. —— 李航 《统计学习方法》 (注意到此处的 $k$ 表示的是数据空间的维度，与k近邻法中的 $k$ 的含义不同) 构造 $kd$ 树 首先，我们要构造一个根结点，选取 $k$ 维空间的某一特征，以所有实例的该特征的中位数为切分点，将剩下的实例按照大小分为两棵子树. 在子树中，选取 $k$ 维空间的某一特征，在找到该特征的中位数，以该点为划分点，划分为两棵子树. 递归实现1、2步骤，直到无法继续细分. 例： ​ 给定一个二维空间数据集： $T=\{(2,3)^T,(5,4)^T,(9,6)^T,(4,7)^T,(8,1)^T,(7,2)^T\}$，构造一棵 $kd 树$. 如图所示： 搜索 $kd$ 树 找到包含目标点 $x$ 的叶结点：从根结点出发，递归地向下遍历 $kd$ 树(二分查找). 以此叶结点为“当前最近结点”. 递归向上回退，在每个结点进行如下操作： (a) 如果该结点保存的实例点比当前最近点距离目标更近，则以该实例点为“当前最近点”(记录被覆盖的最近点). (b) 检查“兄弟结点”所对应的区域是否有更近的点. 具体地, 检查兄弟结点的区域是否与以目标点为球心, 以目标点与”当前最近点”间的距离为半径的球体相交. 如果有，移动到该结点. 然后递归地进行搜索，否则继续往上回退. 回退到根结点时，搜索结束. 记录的距离最小的 $k$ 个点即为目标点的 $k$ 近邻点. 分类 从 $k$ 个近邻点中找到样本数最大的类，作为目标点 $x$ 的类. 参考资料 [1] 李航. 统计学习方法[M]. 北京: 清华大学出版社, 2012: 37-45. [2] 百度百科. 邻近算法[DB/OL]. https://baike.baidu.com/item/%E9%82%BB%E8%BF%91%E7%AE%97%E6%B3%95/1151153, 2018-7-20.]]></content>
      <categories>
        <category>判别模型</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>判别模型</tag>
        <tag>k近邻法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高斯判别分析]]></title>
    <url>%2F2018%2F07%2F18%2Fgenerative-model%2FGDA%2F</url>
    <content type="text"><![CDATA[高斯判别分析(Gaussian discriminant analysis)原理相关概率论知识(概率论) 在生成模型中，我们的学习目标是： 而在高斯判别分析中，我们有如下假设： 即： 其中 $\mu$ 是高斯分布的均值向量，$\Sigma$ 是协方差矩阵. 然后，我们使用 极大似然估计 ，首先构造 $l(\phi,\mu_0,\mu_1,\Sigma)$ : 于是得到： 于是我们只要将学习到的参数 $\phi,\mu_0,\mu_1,\Sigma$ 代入 $p(x|y)$ 和 $p(y)$ 的函数，在预测时，只需要代入数据，计算出不同类别中最大的 $p(x|y)p(y)$，便可以得到预测的类别. 代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105import numpy as npimport randomimport matplotlib.pyplot as plt# 高斯判别分析(Gaussian discriminant analysis)def load_data(gauss_1, gauss_2, point_num): """ 数据输入 :param gauss_1: 类别0的μ和σ组成的数组 :param gauss_2: 类别1的μ和σ组成的数组 :param point_num: 样本总数 :return: x_0,y_0 二维高斯分布样本点(x_0,y_0) x_1,y_1 二维高斯分布样本点(x_1,y_1) label 类别标签 """ x_0 = [] x_1 = [] y_0 = [] y_1 = [] label = [] while len(label) &lt; point_num: if random.random() &gt; 0.5: x_0.append(random.gauss(gauss_1[0], gauss_1[1])) y_0.append(random.gauss(gauss_1[0], gauss_1[1])) label.append(0) else: x_1.append(random.gauss(gauss_2[0], gauss_2[1])) y_1.append(random.gauss(gauss_2[0], gauss_2[1])) label.append(1) return x_0, y_0, x_1, y_1, labeldef train(x_train_0, x_train_1, label): """ 训练高斯判别分析模型 :param x_train_0: 类别为0的样本 :param x_train_1: 类别为1的样本 :param label: 样本标签 :return: phi: Φ mu_0: μ_0 mu_1: μ_1 sigma: Σ """ m = len(label) label_cnt_0 = float(label.count(0)) label_cnt_1 = float(label.count(1)) phi = label_cnt_1 / m mu_0 = np.sum(x_train_0[0])/label_cnt_0, np.sum(x_train_0[1])/label_cnt_0 mu_1 = np.sum(x_train_1[0])/label_cnt_1, np.sum(x_train_1[1])/label_cnt_1 x0_u0 = np.mat(x_train_0.T - mu_0) x1_u1 = np.mat(x_train_1.T - mu_1) x_u = np.mat(np.concatenate([x0_u0, x1_u1])) sigma = (1.0 / m) * (x_u.T * x_u) ''' print(phi) print(mu_0) print(mu_1) print(sigma_0) print(sigma_1) ''' return phi, np.mat(mu_0), np.mat(mu_1), sigmadef predict(x, gauss): """ 预测数据 :param x: 输入的预测样本 :param gauss: 得到的高斯模型的数组 :return: p_y0 类别为0的概率 p_y1 类别为1的概率 """ p_y0 = (1-gauss[0]) * np.exp(-1/2 * (x-gauss[1]) * gauss[3].I * (x-gauss[1]).T) p_y1 = (gauss[0]) * np.exp(-1/2 * (x-gauss[2]) * gauss[3].I * (x-gauss[2]).T) if p_y1 &gt; p_y0: print(str(x) + "is 1") else: print(str(x) + "is 0") return p_y0, p_y1if __name__ == '__main__': x_0, y_0, x_1, y_1, label = load_data([15, 3], [30, 3], 5000) train_0 = np.vstack((x_0, y_0)) train_1 = np.vstack((x_1, y_1)) gauss = train(train_0, train_1, label) for i in range(10,40): pre_x = [i, i] p_0, p_1 =predict(pre_x, gauss) # 绘制训练样本点 plt.scatter(x_0, y_0, 4, "lightblue") plt.scatter(x_1, y_1, 4, "red") # 绘制分隔曲线(取样本中心点连线的中垂线) x_point = [] y_point = [] x0 = ((gauss[2] + gauss[1]) /2).T # 直线的斜率 tmp = (gauss[2] -gauss[1]).T k = float(- tmp[0] / tmp[1]) for i in np.linspace(0, 40, 100): x_point.append(i) y_point.append(k*i - k*float(x0[0])+float(x0[1])) plt.plot(x_point, y_point) plt.show() 实现后的图像如下：]]></content>
      <categories>
        <category>生成模型</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>生成模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[判别模型和生成模型]]></title>
    <url>%2F2018%2F07%2F17%2Fgenerative-model%2FDM-and-GM%2F</url>
    <content type="text"><![CDATA[判别模型和生成模型(Discriminative Model&amp; Generative Model )定义 ​ 监督学习的任务就是学习一个模型，应用这一模型，对给定的输入预测相应的输出，这个模型的一般形式为决策函数: Y = f(X)或者条件概率分布： p(Y|X)​ 监督学习方法又可以分为生成方法(generative approach) 和判别方法(discriminative approach). 所学到的模型分别称为生成模型(generative model) 和判别模型(discriminative model). —— 李航《统计学习方法》 ​ 同样地，在李航老师的这本《统计学习方法》中，对生成方法和判别方法做出了如下定义： ​ 生成方法由数据学习联合概率分布$P(X,Y)$，然后求出条件概率分布$P(Y|X)$作为预测的模型，即生成模型： P(Y|X) = \frac{P(X,Y)}{P(X)}这样的方法之所以称为生成方法，是因为模型表示了给定输入$X$产生输出$Y$的生成关系，典型的生成模型有：朴素贝叶斯法和隐马尔可夫模型. ​ 判别方法由数据直接学习决策函数$f(x)$或者条件概率分布$P(Y|X)$作为预测的模型，即判别模型. 判别模型关心的是给定输入$X$，应当输出什么样的$Y$ .典型的判别模型包括：k近邻法、感知机、决策树、逻辑斯谛回归模型、最大熵模型、支持向量机、提升方法和条件随机场等. 特点对比 生成模型(generative model)优点 能计算出联合概率分布$P(X,Y)$，可以反映同类数据本身的相似度. 学习收敛速度更快，即当样本容量增加的时候，学习到的模型可以更快地收敛于真实模型. 能够用于数据不完整的情况，且能检测异常值. 缺点 含有更多的信息，但同样也需要更多的计算资源. 仅用于分类任务时，有许多冗余信息. 判别模型(discriminative model)优点 学习的是条件概率$P(Y|X)$ 或决策函数$f(X)$，直接面对预测问题，往往准确率更高. 可以对数据进行各种程度上的抽象、定义特征并使用特征，简化学习问题. 对分类任务，冗余信息更少，能节省计算资源. 缺点 不能反映数据本身的特性. 数据缺失或者异常值对预测结果的影响较大. 举例假设有四个样本： 在生成模型的世界中是： \sum P(x,y) =1而在判别模型的世界中是： \sum_y P(y|x) = 1模型实例生成模型 判别模型 &lt;/br&gt; &lt;/br&gt; 参考资料 [1] 李航. 统计学习方法[M]. 清华大学出版社, 2012: 17, 18.[2] 知乎. 机器学习“判定模型”和“生成模型”有什么区别？[EB/OL]. https://www.zhihu.com/question/20446337, 2018-06-07/2018-07-17]]></content>
      <categories>
        <category>生成模型</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>生成模型，判别模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[广义线性模型]]></title>
    <url>%2F2018%2F07%2F16%2Flinear-model%2FGLM%2F</url>
    <content type="text"><![CDATA[广义线性模型(Generalized Linear Models）构造广义线性模型 广义线性模型有3个假设： $y|x;\theta \sim ExponentialFamily(\eta)：$固定参数 $\theta$，在给定 $x$ 的情况下，$y$ 服从指数分布族 (the exponential family)中以 $\eta$ 为参数的某个分布. 给定一个 $x$，我们需要的目标函数为 $h_\theta(x) = E[T(y) | x;\theta] $，后者为该分布的期望. 令 $\eta = \theta^Tx$. 其中，所有可以表示成如下形式的概率分布，都属于指数分布族： p(y;n) = b(y)exp(\eta^TT(y)-a(\eta)) \tag{1.1}在式$(1.1)$ 中，$\eta$ 为该分布的自然参数 (nature parameter)；$T(y)$ 是充分统计量 (sufficient statistic) ，通常情况下 $T(y) =y$. 推导线性回归线性回归 在线性回归中，我们使用了高斯分布作为误差 $e$ 的概率分布，现在我们将高斯分布表示为$(1,1)$的形式： \begin{align} p(y;\mu) &= \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y-\mu)^2}{2\sigma^2}) \\ &= \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{1}{2}y^2)exp(\mu y-\frac{1}{2}\mu^2) \end{align} \tag{2.1}其中 \begin{align} \eta &= \eta^T =\mu \\ T(y) &= y \\ a(\eta)& = \frac{1}{2}\mu^2 = \frac{1}{2}\eta^2 \\ b(y) &= \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{1}{2}y^2) \end{align} \tag{2.2} 即满足第1点要求，接下来： \begin{align} h_\theta(x) &= E[T(y)|x;\theta] \\ &= E[y|x;\theta] \\ &=\mu \end{align} \tag{2.3}即第2点满足，最后我们令 $\eta = \theta^Tx$ ，由 $(2.2) $中 $\eta=\mu$ ，有： h_\theta(x) = \mu = \eta = \theta^Tx \tag{2.4}推导逻辑回归 逻辑回归 在逻辑回归的推导中，我们假定条件概率 $P(y=c|x;\theta)$ 服从伯努利分布，现在我们同样将伯努利分布放在广义线性模型下： \begin{align} p(y;\phi) &= \phi^y(1-\phi)^{1-y} \\ &= (e^{log\phi})^y(e^{log(1-\phi)})^{(1-y)} \\ &= exp(ylog\phi+(1-y)log(1-\phi)) \\ &= exp((log\frac{\phi}{1-\phi})y+log(1-\phi)) \end{align} \tag{3.1}相应的参数为： \begin{align} \eta &= log\frac{\phi}{1-\phi} \rightarrow \frac{1}{1+e^{-\eta}} \\ T(y) &=y \\ a(\eta) &= -log(1-\phi) = log((1-\phi)^{-1}) = log(1+e^\phi) \\ b(y) &= 1 \end{align} \tag{3.2}接下来执行第2点和第3点，令 $h_\theta(x)$ 等于伯努利函数的期望，并且令$\eta = \theta^Tx$： \begin{align} h_\theta(x)& = E[T(y)|x;\theta] \\ &= E[y|x;\theta] \\ &=\phi \\ &= \frac{1}{1+e^{-\eta}} \\ &= \frac{1}{1+e^{-\theta^Tx}} \end{align} \tag{3.3}推导 Softmax 回归Softmax回归 多项式分布我们利用多项式分布进行建模，得到解决多分类问题的模型，实际上我们可以把它理解为是逻辑回归中 1重二项分布(伯努利分布) 的拓展，在多项式分布中，有： \begin{align} p(y=1) = \phi_i \\ \sum^k_{i=1}\phi_i = 1 \end{align} \tag{4.1}其中$y\in{1,2,3…,k}$，又因为 $\phi_i$ 累加为1，所有我们可以只保留 $k-1$ 个参数： \phi_k = 1-\sum^{k-1}_{i=1}\phi_i \tag{4.2}(这跟我们在二分类时只保留1个参数是一样的) 概率分布在多分类问题中，有如下的概率分布： p(y;\theta) = \phi_1^{I(y=1)} \phi_2^{I(y=2)}... \phi_k^{I(y=k)} \tag{4.2}其中$I(y=i)$ 为指示器函数 (indicator function) \begin{align} I\{true\} = 1 \\ I\{false\} = 0 \end{align} \tag{4.3}为了实现指示器函数的功能，我们给出 $T(y)$ ： \begin{align} T(1) &= [1\ 0\ ...\ 0]^T ,\\ T(2) & = [0\ 1\ ...\ 0]^T, \\ T(k-1) &= [0\ 0\ ...\ 1]^T, \\ T(k) &= [0\ 0\ ...\ 0]^T, \\ \end{align} \tag{4.4}于是 $T(y) $ 中的某个元素： T(y)_i = I\{y=i\} \tag{4.5}于是由 $(4.2)$ 有 \begin{align} p(y;\theta) &= \phi^{I(y=1)} \phi_2^{I(y=2)}...\phi_k^{1-\sum^{k-1}_{i=1}I\{y=i\}} \\ &=\phi_1^{T(y)_1}\phi_2^{T(y)_2}...\phi_k^{1-\sum^{k-1}_{i=1}T(y)_i} \end{align} \tag{4.6}在广义线性模型下的推导 首先从广义线性模型的第1点开始，我们将$(4.6)$表示为指数分布族的形式： \begin{align} p(y;\theta) &=\phi_1^{T(y)_1}\phi_2^{T(y)_2}...\phi_k^{1-\sum^{k-1}_{i=1}T(y)_i} \\ &= exp(T(y)_1log\phi_1+T(y)_2log\phi_2+⋯+((1−\sum _{i=1}^{k−1}T(y)_i)log\phi_k) \\ &= exp(T(y)_1log\frac{\phi_1}{\phi_k}+T(y)_2log\frac{\phi_2}{\phi_k}+⋯+T(y)_{k-1}log\frac{\phi_{k-1}}{\phi_k} +log_{\phi_k} )\\ &= b(y)exp(\eta^TT(y)-a(\eta)) \end{align} \tag{5.1}这里有： \begin{align} \eta &= [log\frac{\phi_1}{\phi_k}, ... log\frac{\phi_k-1}{\phi_k}]^T \\ T(y) &= 式(4.4) \\ a(\eta) &= -log\phi_k \\ b(y)&= 1 \end{align} \tag{5.2}观察发现： \eta_i = log\frac{\phi_i}{\phi_k} \rightarrow e^{\eta_i} = \frac{\phi_i}{\phi_k} \tag{5.3}令其累加，有： \sum^k_{i=1}e^{\eta_i} = \frac{\sum^k_{i=1}\phi_i}{\phi_k} = \frac{1}{\phi_k} \\ \rightarrow \phi_k = \frac{1}{\sum^k_{i=1}e^{\eta_i}} \tag{5.4}代入$(5.3)$得： \phi_i = \frac{e^{\eta_i}}{\sum^k_{i=1}e^{\eta_i}} \tag{5.5}下面推导第2点和第3点，假设$h_\theta(x)$ 等于多项式分布的期望，并令 $\eta =\theta^Tx$ : \begin{align} h_\theta(x) &= ET(y)|x;\theta] \\ & = E[p(y=1|x;\theta),p(y=2|x;\theta),...,p(y=k-1|x;\theta)^T \\ & = [\phi_1,\phi_2,...,\phi_{k-1}]^T \\ & =[\frac{e^{\eta_1}}{\sum^k_{i=1}e^{\eta_i}},\frac{e^{\eta_2}}{\sum^k_{i=1}e^{\eta_i}},...,\frac{e^{\eta_{k-1}}}{\sum^k_{i=1}e^{\eta_i}}]^T \\ & =[\frac{e^{\theta_1^Tx}}{\sum^k_{i=1}e^{\theta_i^Tx}},\frac{e^{\theta_2^Tx}}{\sum^k_{i=1}e^{\theta_i^Tx}},...,\frac{e^{\theta_{k-1}^Tx}}{\sum^k_{i=1}e^{\theta_i^Tx}}]^T \end{align} \tag{5.6}]]></content>
      <categories>
        <category>线性模型</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>广义线性模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Softmax回归]]></title>
    <url>%2F2018%2F07%2F16%2Flinear-model%2Fsoftmax-regression%2F</url>
    <content type="text"><![CDATA[在逻辑回归(logistic regression)中，我们处理的是二分类任务，即一次只能将样本划分为两类，那么如果遇到多分类问题呢？Softmax 回归便是一个解决方案. Softmax 回归（Softmax Regression）(本文省略了很多与逻辑回归类似的但又复杂很多的推导，类似推导：逻辑回归) 介绍 在 Softmax 回归中，类别标签 $y$ 可以取 $K$ 个不同的值，因此，训练集样本$\{(x^{(1)},y^{(1)}),…,(x^{(n)},y^{n})\}$ 的类别标签有 $y^{(i)} \in \{1,2,…,k\}$ . 于是我们给定输入 $x$，希望可以对同一样本在不同$k=1,2,…k$ 值下估计条件概率 $P(y = k|x)$ 的值，即估计标签取 $K$ 个不同值时的概率。因此，我们假设输出 $K$ 维向量，即 $h_\theta(x)$ : \begin{align} h_\theta(x) &= \left[ \begin{array} \ P(y=1 |x;\theta) \\ P(y=2 |x;\theta) \\ \qquad \ \ \vdots \\ P(y = K | x;\theta) \end{array} \right] \\ \\ &= \frac{1}{\sum^K_{j=1}exp(\theta^{(j)T}x)} \left[ \begin{array} \ exp(\theta^{(1)T}x) \\ exp(\theta^{(2)T}x) \\ \qquad \ \ \vdots \\ exp(\theta^{(K)T}x) \end{array} \right] \end{align}于是，参数 $\theta$ 实际上由 $K$ 列的向量组成： \theta = \theta^{(1)} , \theta^{(2)},...,\theta^{(k)}代价函数 (Cost Function)下面直接给出代价函数： J(\theta) = - \sum^m_{i=1}\sum^K_{k=1} I\{ y^{(i)} = k\} log \frac{exp(\theta^{(k)T}x^{(i)})}{\sum^K_{j=1}exp(\theta^{(j)T}x^{(i)})}其中 $1\{·\}$ 称为指示器函数 (indicator funciton) 即：$I\{true\} =1$，$I\{false\}=0$. 与逻辑回归的代价函数不同的是，Softmax回归是将 $K$ 个不同的类标签的概率值相加. 且Softmax回归中的输出概率函数为： P(y^{(i)}=k|x^{(i)};\theta) = \frac{exp(\theta^{(k)T}x^{(i)})}{\sum^K_{j=1}exp(\theta^{(j)T}x^{(i)})} 遗憾的是，对于 $ J(\theta)$ 的最优化问题，目前还没有闭式解法(closed-formway，即无需通过迭代计算而得到结果的解法)，因此，这里我们使用梯度下降的方式求解. 梯度下降(gradient descent)​ 对 $J(\theta)$ 求偏导得： \frac{\partial J(\theta)}{\partial \theta} =- \sum^m_{i=1}[x^{(i)}(I\{y^{(i)}=k\}-P(y^{(i)}=k|x^{(i)};\theta))]于是代入 \theta := \theta - \alpha\frac{1}{m}\frac{\partial J(\theta)}{\partial \theta}就能得到参数 $\theta$ 的更新公式.]]></content>
      <categories>
        <category>线性模型</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>Softmax回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯分类器]]></title>
    <url>%2F2018%2F07%2F16%2Fgenerative-model%2Fbayes%2Fnaive-bayes%2F</url>
    <content type="text"><![CDATA[朴素贝叶斯分类器(Naive Bayes Classifiers)介绍 ​ 朴素贝叶斯法是基于贝叶斯定理和特征独立假设的分类方法. 对于给定的训练数据集，首先基于特征条件独立性假设学习输入/输出的联合概率分布；然后基于此模型，对给定的输入 $x$，利用贝叶斯定理求出后验概率最大的输出 $y$. —— 李航 《统计学习方法》 原理 由条件概率公式： P(y|X) = \frac{P(X,y)}{P(X)} \tag{1}再由贝叶斯定理: P(y|X) = \frac{P(y)P(X|y)}{P(X)} \tag{2}这里 $y$ 是类变量，$X$ 是特征向量组：$x_1,x_2,…,x_n$. 我们假设各个特征相互独立，即各个特征独立地对分类结果产生影响(即”朴素“)： \begin{align} P(y|x_1,x_2...,x_n) &= P(y|x_1)P(y|x_2)...P(y|x_n) \\ &= \frac{P(x_1|y)P(x_2|y)...P(x_n|y)P(y)}{P(x_1)P(x_2)...P(x_n)} \end{align} \tag{3}注意到：$P(x_1)P(x_2)…P(x_n)$ 对于所有样本都是一样的. 所以，我们的目标即转化为： y = arg max\ P(y=c) \prod_{i=1}^n P(x_i|y=c) \tag{4} 令 $D_c$ 表示训练集 $D$ 中第 $c $ 类样本组成的集合，于是我们可以估计 类先验概率 : P(y=c) = \frac{|D_c|}{|D|} \tag{5}令$D_{c,x_i}$ 表示在 $D$ 中第 $c$ 类属性取值为 $x_i$ 的样本组成的集合，于是 条件概率 可估计为: P(x_i|y=c) = \frac{|D_{c,x_i}|}{|D_c|} \tag{6}最后，对于给定的实例，将计算后的式 $(5)$ 和 $(6)$ 代入 $(4)$ 即可确定实例的类. 另外，对连续属性，我们可以考虑概率密度函数，假定 $p(x_i) \sim N(\mu_{c,i},\sigma^2_{c,i})$ ，其中 $\mu_{c,i}$ 和 $\sigma^2_{c,i}$ 分别是第 $c$ 类样本在第 $i$ 个属性上取值的均值和方差，则有： p(x_i|c) = \frac{1}{\sqrt{2\pi}\sigma_{c,i}}exp(-\frac{(x_i-\mu_{c,i})^2}{2\sigma^2_{c,i}}) . \tag{7} 需注意的是，为避免未出现的属性值被判定为概率为0(这显然是不合理的)，同时也避免其他信息被抹去，通常进行”平滑“(smoothing)，常用拉普拉斯修正(Laplacian correction)，令 $N$ 表示训练集 $D$ 中可能的类别数，$N_i$ 表示第 $i$ 个属性可能的取值数，即将式(5)和(6)分别修正为 \begin{align} P(y=c) &= \frac{|D_c|+1}{|D|+N} \\ \tag{8} P(x_i|y=c) &= \frac{|D_{c,x_i}|+1}{|D_c|+N_i} \end{align} 参考资料： 周志华 《机器学习》 李航 《统计学习方法》]]></content>
      <categories>
        <category>生成模型</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>生成模型</tag>
        <tag>贝叶斯分类器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[极大似然估计]]></title>
    <url>%2F2018%2F07%2F16%2Foptimization-theory%2FMLE%2F</url>
    <content type="text"><![CDATA[极大似然估计(Maximum Likelihood Estimation )绪言 ​ 对概率函数 $P(X|c)$ 来说，由于它涉及关于 $x$ 所有属性的联合概率，直接根据样本出现的频率来估计将会遇到严重的困难. 例如，假设样本的 $d $ 个属性都是二值的，则样本空间将有 $2^d$种可能的取值，在现实应用中，这个值往往远大于训练样本数 $m$ ，也就是说，很多样本取值在训练集中根本没有出现，直接使用频率来估计条件概率 $P(x |c) $ 显然不可行，因为“未被观测到”与“出现的概率为零”通常是不同的. —— 周志华《机器学习》 我们设关于类别 $c$ 的条件概率为 $P(x |c)$ ，假设 $P(x|c)$ 有确定的形式并且被参数 $\theta$ 唯一确定，那么实际上我们的训练过程就是要利用数据集 $D$训练出较为合适的参数 $\theta$，即参数估计(parameter estimation)过程 . 这里，我们把 $P(x|c)$ 记为 $P(x |\theta)$. 似然(likelihood) ​ 在数理统计学中，似然函数是一种关于统计模型中的参数的函数，表示模型参数中的似然性。似然函数在统计推断中有重大作用，如在最大似然估计和费雪信息之中的应用等等。“似然性”与“或然性”或“概率“意思相近，都是指某种事件发生的可能性，但是在统计学中，“似然性”和“或然性”或“概率”又有明确的区分。概率用于在已知一些参数的情况下，预测接下来的观测所得到的结果，而似然性则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。 —— 维基百科 似然函数 上述是似然函数(通常简称似然)在数理统计中的意义，因此极大似然估计即是利用观测结果，对参数进行估计的一种方法。 极大似然估计 ​ 对于参数估计，统计学界的两个学派分别提供了不同的解决方案：频率主义学派(Frequentist) 认为参数虽然未知，但是却是客观存在的固定值，因此，可通过优化似然函数等准则来确定参数值；贝叶斯学派(Bayesian)则认为参数是未观察到的随机变量，其本身也可有分布，因此可以假定参数服从一个先验分布，然后基于观测到的数据来计算参数的后验分布. —— 周志华《机器学习》 而频率主义学派的极大似然估计(Maximum Likelihood Estimation，简称 MLE) 在机器学习的一些算法(逻辑回归、朴素贝叶斯等)的推导上有很重要的作用. 设 $D_c$ 为 训练集 $D$ 中第 $c$ 类样本所组成的集合，假设它们独立同分布，则参数 $\theta $ 对与数据$D_c$ 的似然，是 L(\theta) = P(D_c | \theta) = \prod_{x \in D_c} P(x | \theta) \tag{1.1} 式$(1.1)$ 中的连乘操作易造成下溢，通常使用对数似然(log-likelihood) \begin{align*} LL(\theta) &= log P(D_c|\theta) \\ &= \sum_{x \in D_c} log P(x|\theta) \end{align*} \tag{1.2}因此，对参数 $\theta$ 进行极大似然估计，就是去寻找能够最大化似然 $P(D_c|\theta)$ 的参数值 $\theta$ . 即在 $\theta$ 的所有可能取值中，找到一个能使数据出现的“可能性”最大的值. 参考资料：[1] 周志华. 机器学习[M]. 北京: 清华大学出版社. 2016: 147-154. [2] 维基百科. 似然函数[DB/OL], https://zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0, 2018-02-27/2018-07-16.]]></content>
      <categories>
        <category>优化理论</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>概率论</tag>
        <tag>极大似然估计</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率论与数理统计(三)]]></title>
    <url>%2F2018%2F07%2F14%2Fprobability%2Fprobability-3%2F</url>
    <content type="text"><![CDATA[本章的主要内容是随机变量的数字特征，其中包括数学期望、方差、协方差和相关系数。 随机变量的数字特征数学期望定义离散型 设离散型随机变量$X$的概率函数为 P(X =a_i) = p_i \ , \quad i=1, 2, ...当级数$\sum_ia_ip_i$ 绝对收敛时，称$\sum_ia_ip_i $为随机变量$X$的数学期望(或期望、均值)，记作$E(X).$ 连续型 设连续型随机变量$X$的概率密度函数为$f(x)$，当积分$\int_{-\infty}^{+\infty}xf(x)dx$绝对收敛时， 称$\int ^{+\infty}_{-\infty}xf(x)dx$ 为随机变量X的数学期望，记作E(X)，即 E(X) =\int_{-\infty}^{+\infty}xf(x)dx 常用离散型随机变量的数学期望离散型 $0-1 分布 B(1,P) : E(X) = p $ $二项分布 B(n,p) : E(X) = np$ $泊松分布P(\lambda) : E(X) = \lambda $ 连续型 $均匀分布 R(a,b) : E(X) = \frac{a+b}{2} $ $指数分布 E(\lambda): E(X) = \frac{1}{\lambda}$ $正态分布N(\mu, \sigma^2): E(X) = \mu$ 随机变量函数的期望定理4.1 设$X$为离散型随机变量，概率函数为 P(X =a_i) = p_i \ , \quad i=1,2,...$Y = g(X)$ 是随机变量X的函数，当$\sum_i|g(a_i)|p_i $收敛时 E(Y) = \sum_i g(a_i) p_i 若$X$为连续型随机变量，$f(x)$为其相应的密度函数，当$\int^{+\infty}_{-\infty}|g(x)|f(x) dx $收敛时， E(Y) =\int^{+\infty}_{-\infty}g(x)f(x) dx 设$(X,Y) $是二维离散型随机变量，其概率函数为 P(X = a, Y = b_j) = p_{ij} \qquad i,j = 1,2,...$Z = g(X,Y) $是 X,Y 的函数，当$\sum_i \sum_j g(a_i,b_j) p_{ij}$ 绝对收敛时 E(Z) = \sum_i \sum_j g(a_i,b_j) p_{ij} 若$(X,Y)$是二维连续型随机变量，$f(x,y)$为联合密度函数，$Z = g(X,Y) $是 $(X,Y)$ 的函数， 当$\int_{-\infty}^{+\infty}g(x,y)f(x,y)dxdy$ 绝对收敛时 E(Z) =\int_{-\infty}^{+\infty}g(x,y)f(x,y)dxdy $(X,Y)$为离散型随机变量，当$g(X,Y) = X$ 或$(Y)$时 E(X) = \sum_i\sum_ja_ip_{ij} = \sum_i a_i p_{i·}或 或 E(Y) = \sum_i\sum_jb_jp_{ij} = \sum_j b_j p_{·j} $(X,Y)$为连续型随机变量，当$g(X,Y) = X$(或$Y$) 时 \ \ \ E(X) = \int^\infty_{-\infty} \int^\infty_{-\infty} xf(x,y)dxdy \\ =\int^\infty_{-\infty} xf_X(x)dx或 \quad E(Y) = \int^\infty_{-\infty} \int^\infty_{-\infty} yf(x,y)dxdy \\ =\int^\infty_{-\infty} yf_Y(y)dy数学期望的性质退化分布 把常数$c$看做是概率函数为$P(X = c) = 1$ 的随机变量$X$，并称$X$服从参数为$c$的退化分布. 定理4.2 $设k,l,c都是常数，则有$ $E(c) = c $ $E(kX+c) = kE(X) + c$ $E(kX + lY) = kE(X) + lE(Y)$ $当X与Y相互独立时，有E(XY) = E(X)E(Y)$ 方差和协方差定义 $设X是一个随机变量，称$ D(X) \doteq E\{[X-E(X)]^2\}$为X的方差，而称\sqrt{D(X)}为X的标准差.$ 计算公式 D(X) = E(X^2) - E^2(X) \ .常见离散型分布的方差0-1分布 D(X) = p(1-p)二项分布 D(X) = np(1-p)泊松分布 D(X) = \lambda常见连续型分布的方差均匀分布 D(X) = \frac{(b-a)^2}{12}指数分布 D(X) = \frac{1}{\lambda^2}正态分布 D(X) = \sigma^2方差的性质定理4.3 $设k,l及c都是常数，则$ $D(c) = 0$ $D(kX+c) = k^2D(x)$ $D(X\pm Y) = D(X) + D(Y) \pm 2E\{[X-E(X)][Y-E(Y)]\} $ $当X与Y相互独立时，D(X \pm Y ) = D(X) + D(Y)$ 协方差定义 $设(X,Y)是二维随机变量，称$ E\{[X-E(x)][Y-E(Y)]\}$为X与Y的协方差，记为cov(X,Y) .$ 协方差反映的是$X$和$Y$之间协同发展的趋势 计算公式 cov(X,Y) = E(XY) - E(X)E(Y) cov(X,X) = D(X) D(X \pm Y) =D(X) +D(Y) \pm 2cov(X,Y) \\ D(aX \pm bY) = a^2D(X) + b^2D(Y) \pm 2abcov(X,Y) $当X与Y相互独立时，有$ cov(X,Y) = 0性质(定理4.4) $设k,l,c都是常数，则(X,Y)的协方差满足$ $cov(X,Y) = cov(Y,X)$ $cov(X,c) = 0$ $cov(kX,lY) = klcov(X,Y)$ $cov(\sum^m_{i=1}X_i, \sum^n_{j=1}Y_j) = \sum^m_{i=1}\sum^n_{j=1}cov(X_i,Yj)$ 相关系数定义 设$(X,Y)$是随机变量，当$D(X)&gt;0,D(Y)&gt;0$时，称 \rho(X,Y) = E[\frac{X-E(X)}{\sqrt{D(X)}}·\frac{Y-E(Y)}{\sqrt{D(Y)}}]为$X$和$Y$的相关系数，标准化方差. 注： \rho(X,Y) = \frac{cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}} D(X \pm Y) = D(X) + D(Y) \pm 2\rho(X,Y)\sqrt{D(X)D(Y)} 二维正态随机变量$(X,Y) \sim N(\mu_1,\mu_2,\sigma^2_1,\sigma_2^2,\rho)$中$X$与$Y$的相关系数 \rho(X,Y) = \rho性质定理4.5 当$D(X) &gt;0, D(Y) &gt;0$ 时 $\rho(X,Y) = \rho(Y,X)$ $|\rho(X,Y)|\le 1$ $|\rho(X,Y)|= 1 $的充分必要条件是：存在不为零的常数$k$与常数$c$，使得$P(Y=kX+c) = 1.$ 线性关系 当$rho(X,Y) = + 1 $时，称$X$与$Y$正线性相关 当$rho(X,Y) = - 1$ 时，称$X$与$Y$负线性相关 $X$与$Y$之间的线性联系程度随着$|\rho(X,Y)|$ 的减小而减弱，特别地有下面定义 当$\rho(X,Y) =0$ 时，称X与Y(线性)不相关. 不相关与相互独立的关系定理4.6 如果$X$与$Y$相互独立，那么$X$与$Y$一定不相关，反之不然；如果$X$与$Y$相关，则它们一定不独立，反正不然。 注：独立意味着随机变量之间没有任何关系，不相关仅意味着无线性关系，不能排除具有非线性关系。 定理4.7 $如果(X,Y)服从二维正态分布，则X与Y相互独立等价于X与Y不相关.$ $如果(X,Y)服从二维正态分布，则X与Y相互独立等价于\rho =0.$]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率论与数理统计(二)]]></title>
    <url>%2F2018%2F07%2F14%2Fprobability%2Fprobability-2%2F</url>
    <content type="text"><![CDATA[本章主要内容是连续型随机变量及其分布。 连续型随机变量及其分布基本概念 给定一个随机变量$X$，称定义域为$(-\infty, +\infty)$ 的实值函数 F(x) = P (X \le x)，-\infty < x < + \infty为随机变量$X$的分布函数. 对任意实数 $x \in R , \{X \le x\}$ 是一个随机事件，它的概率$P(X \le x)$ 就定义为点$x$处分布函数的函数值. 注：对任意满足条件 $-\infty &lt; a +\infty$ 的实数 $a,b,$ 有 P(a < X \le b) = F(b) - F(a). 定理3.1 (分布函数的性质) 设$F(x) 是随机变量$$X$的分布函数，则有： $0 \le F(x) \le 1 ;$ 分布函数单调不减； 对任意$x \in (- \infty, +\infty)，$分布函数右连续； $\lim_{x \rightarrow -\infty} F(x) = 0, \lim_{x \rightarrow + \infty} F(X)= 1.$ 定理3.2 对于任意的随机变量$X$，其分布函数$F(X)$ 在 $x=x_o$点处连续的充分必要条件是$P(X = x_o) = 0.$ 概率密度函数 给定一个连续型的随机变量$X$，如果存在一个定义域为$(-\infty, +\infty)$的非负实值函数$f(x)$，使得$X$的分布函数$F(x)$可以表示为 F(x ) = \int^x_{-\infty} f(t) dt \qquad -\infty < x < +\infty那么就称$f(x)$为连续型随机变量$X$的概率密度函数. 概率密度函数满足下面两个条件： (1) $f(x) \ge 0, \quad -\infty&lt;x&lt;+\infty$ (2) $\int^{+\infty}_{-\infty} f(x)dx = F(+\infty) = 1.$ 对照一下，设离散型随机变量的概率函数为 P(X = a_i) = p_i , \quad i =1, 2, ...则有(1) $p_i \ge 0$ (2) $\sum_i p_i =1$ 连续型随机变量的性质 设$X$ 是任意连续型的随机变量，且$F(X)$与$f(x)$分别是它的分布函数与概率密度函数，则有： （1）$F(x) $是连续函数，且在$f(x)$的连续点处，有 F'(x) = f(x) ; （2）对任意常数 $c(-\infty &lt;c &lt; +\infty),$ 有 $P(X = c) = 0;$ （3） 对任意的两个常数$a,b,-\infty&lt;a&lt;b&lt;+\infty，$有 P(a < X \le b) = F(b) - F(a) = \int^b_{-\infty} f(x) dx - \int^a_{-\infty}f(x)dx \\ =\int^b_af(x)dx注意到 P(x < X \le x+ \Delta x) = F(x + \Delta x) - F(x)\\ \approx F'(x) \Delta x \\ \approx f(x)\Delta x.进一步，对实数轴上任意一个集合$S,$ P(X > a) = \int^{+\infty}_a f(x) dx = 1- F(a) \\ P(X \le b) = \int^b_{-\infty} f(x)dx = F(b). 常见一维连续型分布均匀分布 设随机变量$X$的概率密度函数为 \begin{equation} f(x) = \left \{ \begin{aligned} c \qquad a]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性代数(二)]]></title>
    <url>%2F2018%2F07%2F13%2Flinear-algebra%2Flinear-algebra-2%2F</url>
    <content type="text"><![CDATA[本章的主要内容为向量组、线性相关性、方程组解的结构、向量空间、空间投影和最小二乘法。 线性代数(二)向量组的线性相关性向量组及其线性组合向量 $n$个有次序的数$a_1, a_2, …, a_n$ 所组成的数组称为$n$维向量，这$n$个数称为该向量的$n$个分量，第 $i$ 个数 $a_i$称为第$i$ 个分量. 向量组 若干个同维数的列向量（或同维数的行向量）所组成的集合叫做向量组. 线性组合 给定向量组$A: a_1, a_2, …, a_m$，对于任何一组实数$k_1, k_2, …, k_m$，表达式 k_1a_1 + k_2a_2 + ... + k_ma_m称为向量组$A$的一个线性组合，$k_1,k_2, …, k_m$ 称为这个线性组合的系数. 给定向量组$A: a_1, a_2, …, a_m$ 和向量$b$，如果存在一组数$\lambda_1, \lambda_2, …, \lambda_m$，使 b = \lambda_1a_1 + \lambda_2a_2 + ... + \lambda_na_n则向量$b$是向量组$A$的线性组合，这时称向量$b$能由向量组$A$线性表示. 定理 1 向量$b$能由向量组$A$线性表示，也即方程组 x_1a_1 + x_2a_2 + ... + x_ma_m = b有解. 由上章定理5，有 定理 1 向量$b$能由向量组$A:a_1, a_2, …, a_n$ 线性表示的充分必要条件是矩阵 $A=(a_1, a_2, …, a_m)$ 的秩等于矩阵 $B = (a_1, a_2, …, a_m , b)$ 的秩，即R(A) = R(B). 向量组等价 设有两个向量组$A$和$B$，若向量组$B$中每个向量都能由向量组$A$线性表示，则称向量组$B$能由向量组$A$ 线性表示，若它们能相互线性表示，那么称这两个向量组等价. 定理 2 向量组$B$能由向量组$A$线性表示的充分必要条件是矩阵$A$的秩等于矩阵$(A,B)$的秩，即$R(A) = R(B) = R(A, B)$. 推论： 向量组$A$与向量组$B$等价的充分必要条件是$R(A) = R(B) =R(A,B)$. 定理 3 设向量组 $B = (b_1, b_2, …, b_l)$ 能由向量组 $A=(a_1, a_2, …, a_m)$ 线性表示，则$R(b_1, b_2, …, b_l) \le R(a_1, a_2, …, a_m)$ . 向量组的线性相关性定义 给定向量组$A : a_1, a_2, …, a_m $，如果存在不全为零的数$k_1, k_2, …, k_m$，使 k_1a_1 + k_2a_2 + ... + k_ma_m = 0则称向量组$A$是$线性相关$的，否则称它$线性无关$. 定理 4 向量组$A : a_1, a_2, …, a_m $ 构成的矩阵$A = (a_1, a_2, …, a_m )$，向量组$A$线性相关，就是齐次线性方程组 x_1a_1 + x_2a_2 + ... + x_ma_m = 0即$Ax = 0$ 有非零解. 由上章定理4，立即可得 定理 4 向量组$A$ 线性相关的充分必要条件是它所构成的矩阵的秩小于向量个数$m$，即 R(A) &lt; m ；向量组$A$线性无关的充分必要条件是$R(A) = m$. 定理 5 (1) 若向量组$A : a_1, a_2, …, a_m $ 线性相关，则向量组$B : a_1, a_2, …, a_m, a_{m+1} $也线性相关，反之，若向量组$B$线性无关，则向量组$A$也线性无关. (2) $m$ 个 $n$ 维向量所组成的向量组，当维数 $n$ 小于向量个数 $m$ 时一定线性相关. 特别地$n+1$个 $n$ 为向量一定线性相关. (3) 设向量组 $A : a_1, a_2, …, a_m $ 线性无关，而向量组$B : a_1, a_2, …, a_m, b $ 线性相关，则向量$ b$ 必能由 向量组$A$ 线性表示，且表示式是惟一的. 向量组的秩定义 设有向量组$A$，如果在$A$中能选出$r$个向量$a_1, a_2, …, a_r$，满足 (i) 向量组 $A_0 : a_1, a_2, …, a_r$ 线性无关 (ii) 向量组$A$中任意$r+1$ 个向量都线性相关， 那么称向量组$A_0$是向量组$A$的一个最大线性无关组(简称最大无关组)，最大无关组所含向量个数$r$称为向量组$A$的秩，记作$R_A$. 推论 (最大无关组的等价定义)：设向量组$A_0: a_1,a_2,…, a_r$ 是向量组$A$的一个部分组，且满足 ​ （i) 向量组$A_0$线性无关 （ii) 向量组$A$的任意向量都能由向量组$A_0$ 线性表示. 那么向量组 $A_0$ 便是向量组 $A$ 的一个最大无关组. 定理 6 矩阵的秩等于它的列向量组的秩，也等于它的行向量组的秩. 线性方程组解的结构齐次线性方程组 A x = 0 \qquad \qquad性质 1 若 $x = \xi_1 , x = \xi_2$ 为向量方程(1)的解，则$x = \xi_1 + \xi_2 $也是向量方程(1)的解. 性质 2 若 $x = \xi_1$ 为向量方程(1)的解，k为实数，则$x = k \xi_1$ 也是向量方程(1)的解. 把方程(1)的全体解所组成的集合记作$S$，如果能求得解集$S$的一个最大无关组$S_0: \xi_1 \xi_2, …, \xi_t$，那么方程(1)的任一解都可以由最大无关组$S_0$线性表示；另一方面，有上述性质1、2可知，最大无关组$S_0$的任何线性组合 x= k_1 \xi_1 + k_2 \xi_2 + ... + k_t\xi_t \\ (k_1, ..., k_t 为任意实数)都是方程(1)的解，因此上式便是方程(1)的通解. 齐次线性方程组的解集的最大无关组称为该齐次线性方程的基础解系. 定理 7 设 $m\times n$ 矩阵 $A$ 的秩 $R(A) =r$，则$n$元齐次线性方程组 $Ax =0 $ 的解集 $S$ 的秩 $R_s = n -r$. 非齐次线性方程组 A x = b性质 3 设 $x = \eta_1$ 以及 $x = \eta_2$ 都是向量方程(2)的解，则$x=\eta_1 - \eta_2$ 为对应的齐次线性方程组 Ax = 0 \qquad \qquad的解. 性质 4 设 $x =\eta$ 是方程 (2) 的解，$x=\xi$ 是方程(3)的解，则$x = \xi + \eta$ 仍是方程 (2) 的解. 于是，如果求得方程 (2) 的一个解 $\eta^{*}$ (称为特解)，那么方程 (2) 的通解为 x= k_1 \xi_1 + k_2 \xi_2 + ... + k_{n-r}\xi_{n-r} + \eta^* \\ (k_1, ..., k_{n-r} 为任意实数)其中 $\xi_1, …, \xi_{n-r}$ 是方程 (3) 的基础解系. 非齐次方程组的通解 = 对应的齐次方程组的通解 + 非齐次方程组的一个特解 向量空间向量空间 由一组线性无关的向量及它们的线性运算所组成的所有向量，所构成的集合叫做向量空间。 向量空间应当满足下列两种运算(线性运算)： 数乘：$k v \quad (其中k是标量，v是向量)$ 向量加法：$v + w \quad (其中v,w是向量)$ 后的所有向量也在该空间中。 特别地，因为 0 \times \vec{v} = \vec{0}所以所有的向量空间都必须包含0向量. 子空间 设 $V$ 是在域 $K$ 上的向量空间，并设 $W$ 是 $V$ 的子集。则 $W$ 是个设 $V$ 是在域 $K$ 上的向量空间，并设 $W$ 是 $V$ 的子集，则 $W$ 是 $V$ 的子空间. 列空间和零空间列空间 给定 $m\times n $ 矩阵 $A$ ，$n \times 1$ 矩阵 $x$ ，$m\times1$ 矩阵 $b$，运算 $Ax =b$，则由 $A$ 的列向量所组成的子空间叫做列空间，记作 $C(A)$ $Ax = b$ 有解当且仅当 $b$ 属于$ A$ 的列空间. 零空间 给定 $m\times n $ 矩阵 $A$ ，$n \times 1$ 矩阵 $x$ ，$m\times1$ 的$0$矩阵，由$Ax = 0$ 中 $x$ 的解向量所组成的子空间叫做零空间，记作 $N(A)$. 基和维数 设 $V$ 为向量空间，如果 $r$ 个向量：$a_1,a_2,…,a_r \in V$ 且满足： (i) $a_1,a_2,…,a_n$ 线性无关 (ii) $V$ 中的任一向量都能由 $a_1,a_2,…,a_n$ 线性表示 则称 $a_1,a_2,…,a_r$ 是 $V$ 的基，$r$ 称为 $V$ 的维数. 四个基本子空间列空间和零空间 即$C(A)$和$N(A)$，上节已作详述. 行空间 给定 $m\times n $ 矩阵 $A$ ，$n \times 1$ 矩阵 $x$ ，$m\times1$ 矩阵 $b$，运算 $Ax =b$，则由 $A$ 的行向量所组成的子空间叫做行空间，记作$C(A^T)$. 行空间可看作是矩阵 $A^T$ 的列空间，即 $A^Tx = b$ 有解当且仅当 $b$ 属于$ A$ 的列空间. 左零空间​ 给定 $n \times m $ 矩阵 $A$ ，$n \times 1$ 矩阵 $x$ ，$m\times1$的 $0$ 矩阵，由$A^Tx = 0$ 中 $x$ 的解向量所组成的子空间叫做零空间，记作 $N(A)$. 空间投影和最小二乘法空间投影 在 $R^2$ 空间中，我们做向量 $b$ 对 向量 $a$ 的投影，如图： 其中$p$ 为 $b$ 在 $a$ 上的投影，于是我们可以理解为$e = b - p$ 是 $b$ 和 投影 $p$ 之间的误差。 因为 $p$ 在 $a$ 上 ，所以有 $ p = ax$，所以： a^Te = 0 \rightarrow a^T(b-p) =0即 $a^T(b - ax) = 0 $ ，所以有： x a^T a = a^T b \\ \qquad \qquad \qquad \qquad \ \ x = \frac{a^Tb}{a^Ta} = (a^Ta)^{-1}a^Tb所以又：$p = a\frac{a^Tb}{a^Ta} = a(a^Ta)^{-1}a^Tb$ 拓展到n维空间，即有 $x : x_1,x_2,…,x_n $ 与 向量组$A : a_1,a_2,…,a_n$ 相乘得到 $a_1,a_2,…,a_n$ 的线性组合 $a_1x_1,a_2x_2,…,a_nx_n$ ，即同样有向量$b$ 在 $a$ 上的投影 $p = A x$ . 所以，同样有 x = \frac{A^Tb}{A^TA} = (A^TA)^{-1}A^Tb \\ p =A\frac{A^Tb}{A^TA} = A(A^TA)^{-1}A^Tb投影矩阵 我们可以把向量 $b$ 在 向量空间 $A$ 上的投影 $p$ ，看作是一个矩阵 $P$ 作用于 $b$ 后所的得到的，即有： p = Pb由上一节的知识，我们可以得到： P = A(A^TA)^{-1}A^T 投影矩阵的性质: $P = P^T$ ，即投影矩阵是一个对称矩阵。 $P^2 = P$ ，即对一个向量做两次投影，结果还是等于第一次的投影。 最小二乘法 对于线性方程组 $A x = b$ ，我们希望它是有解的。但很多时候，方程组往往是无解的，这个时候，我们希望能够找到一组$ x : x_1,_x2,…,x_n$ 使得方程 $Ax =b$ 取得最优解，即找到最小的误差。 于是，引入向量空间和向量投影的概念，$Ax =b$ 无解等价于 $b$ 无法由向量 $a_1,a_2,…a_n$ 经过线性运算得到，即 $b$ 不在列空间 $A$ 内，但我们希望能够找到一个 $b$ 在 $A$ 上的分量 $p$ ，即找到一个有解的线性方程组 $Ax = p$，使得误差 $e = b-p $ 最小，由上两节的知识，我们便可以得到： 当 x = \frac{A^Tb}{A^TA} = (A^TA)^{-1}A^Tb \\时，$e$ 有最小值，即为线性方程组 $Ax =b$ 的最优解.]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记之局部加权线性回归]]></title>
    <url>%2F2018%2F07%2F13%2Flinear-model%2Flr_local_weighted%2F</url>
    <content type="text"><![CDATA[局部加权线性回归是线性回归的升级版，目的是为了解决普通线性回归中无法解决的非线性的问题。其实质是每次只使用预测点附近的部分数据进行回归预测，因而可以拟合出一条折线，相较于普通的线性回归，能够较好地拟合非线性数据。 局部加权线性回归普通线性回归 在线性回归的算法中，我们的最终目的是求出使得代价函数 J(\theta) = \frac{1}{2m} (h_{\theta}(x)-y)^2达到最小值的线性拟合函数： h_\theta(x) = \theta_0x_0 + \theta_1 x_1 + ... + \theta_n x_n 在普通的线性回归算法中，对于每一组 $x :x_0, x_1,…x_n$ ，我们运用梯度下降或正规方程，求得所对应的一组 $\theta : \theta_0 , \theta_1, …, \theta_n$，因此我们求得的 $h_\theta$ 是一条直线，显然对于非线性的数据拟合程度很差。 如图所示： 有关线性回归的详细笔记：线性回归 考虑局部 于是我们提出一种想法：能否不考虑全部的样本值，而是在局部进行线性回归，考虑局部的样本值得局部到整体的多条直线，所构成的折线（近似于曲线）便可以较好地拟合数据。 因此在局部加权线性回归算法中，我们赋予预测点附近的每一个点一定的权值，离它较近的赋予较大的权值，较远的赋予较小的权值，即与较近的点进行线性拟合，而忽略较远的点的贡献。 权值函数 因此，权值应当考虑充分地考虑预测点和其他点之间的距离，这里我们使用如下函数作为权值函数 W^{(i)}_{x_j} = exp(-\frac{(x^{(i)}-x_j)^2}{2\sigma^2}) 其中，$x_j$ 为预测点，$x^{(i)}$ 为该点附近的所有点，而 $\sigma$ 决定了对附近的点应该赋予多大的权值，$ \sigma $ 值越大，距离的增大对权值的减小影响越小，一次训练所用到的数据就越多，即最终拟合的函数越“线性”。 损失函数 相对于普通的线性回归，局部加权函数的损失函数应当包含有权值参数，我们这里给出局部加权线性回归的损失函数，如下 J(\theta) = \frac{1}{2m} W_{x_j}^{(i)}(h_{\theta}(x)-y)^2更新方程 得到损失函数后，使用与普通的线性回归相类似的方法，我们可以得到梯度下降法和最小二乘法下，参数 $\theta$ 的更新方程。 梯度下降法 \theta_k : = \theta_k - \frac{\alpha}{m} \sum^m_{i=1} W^{(i)} (h_{\theta}(x^{(i)})-y^{(i)}) x^{(i)}_k最小二乘法 \theta = (X^TWX)^{-1}X^TWY代码实现梯度下降实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677from numpy import *import numpy as npimport matplotlib.pyplot as pltdef load_data(dir): """ 导入训练数据 :param dir:训练数据的路径 :return: 放入学习器的数据 """ tmp = np.loadtxt(dir, dtype=np.float) x_array = tmp[:, :-1] y_array = tmp[:, -1] return x_array, y_arraydef regress(test_point, x_array, label_array ,alpha, k): """ 对一个点进行线性回归 :param test_point: 输入一个点的x值 :param x_array: 整体的数据集 :param label_array: 标签集 :param alpha: 学习率 :param k: 波长参数 :return: 预测的一个点 """ x_mat = np.mat(x_array) y_mat = np.mat(label_array) m = shape(x_mat)[0] weigh = mat(np.eye(m)) theta = eye(1) for i in range(m): weigh[i, i] = np.exp(np.square(test_point - x_mat[i])/(-2*k*k)) for i in range(30): error = theta * x_mat.T - y_mat theta = theta - (alpha) * error * weigh * x_mat return test_point * thetadef regress_all(x_array, label_array, alpha, k): """ 对全部点进行回归预测 :param x_array: 数据集 :param label_array: 标签集 :param alpha: 学习率 :param k: 波长参数 :return: 预测的标签集 """ m = shape(x_array)[0] y_label = ones(m) for i in range(m): y_label[i] = regress(x_array[i], x_array, label_array, alpha, k) return y_labeldef main(): """ 主函数 """ x_array, y_array = load_data("data.txt") y = regress_all(x_array, y_array, 0.25, 0.018) # 对 x 排序后画出回归图 x_mat = mat(x_array).transpose() y_mat = mat(y) ind = x_mat.argsort(1) x_sort = x_mat.T[ind][0] y_sort = y_mat.T[ind][0] fig = plt.figure() ax = fig.add_subplot(1, 1, 1) fig_name = "linear regression" plt.title(fig_name) ax.plot(x_sort, y_sort) ax.scatter(x_array, y_array, 8, "red") plt.show()main() 最小二乘法实现1234567891011121314151617181920# 上文与梯度下降的内容一致def regress(test_point, x_array, label_array, k): """ 对一个点进行线性回归 :param test_point: 输入一个点的x值 :param x_array: 整体的数据集 :param label_array: 标签集 :param k: 波长参数 :return: 预测的一个点 """ x_mat = np.mat(x_array) y_mat = np.mat(label_array) m = shape(x_mat)[0] weigh = mat(np.eye(m)) theta = eye(1) for i in range(m): weigh[i, i] = np.exp(np.square(test_point - x_mat[i])/(-2*k*k)) theta = (x_mat.T * weigh * x_mat).I * x_mat.T * weigh * y_mat.T return test_point * theta# 下文与梯度下降的内容一致（去除函数内的学习率alpha） 结果现在，我们再对非线性数据进行拟合，如图： 显然，拟合效果比普通的线性模型要好得多。 数据集数据集 结语 虽然局部加权线性回归在处理非线性的问题上有着不错的效果，但同时也存在着问题，比如： (1) 一次只对一个数据点进行拟合，对大量数据的处理十分缓慢 (2) 容易产生过拟合问题 因此，我们还要去寻找更优秀的模型。]]></content>
      <categories>
        <category>线性模型</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[概率论与数理统计(一)]]></title>
    <url>%2F2018%2F07%2F10%2Fprobability%2Fprobability-1%2F</url>
    <content type="text"><![CDATA[概率论是研究概率和随机现象的数学分支，是研究随机性或不确定性等现的象数学。概率论是统计学的数学基础，也是机器学习的重要数学基础之一。本章主要内容是离散型随机变量及其分布。 概率论与数理统计(一)离散型随机变量及其分布随机变量及概率函数随机变量定义： 给定一个随机试验，$\Omega$是样本空间，如果对$\Omega$中的每个样本点$\omega$，都有一个实数X($\omega$)与之对应，那么就把这个定义域为 $\Omega$ 的单值实值函数：X = X($\omega$) 称为是（一维）随机变量. 一般用大写字母 X, Y, Z, … 等表示随机变量 把随机变量 X 的值域记做 $\Omega _x $，则 $\Omega _x$ $\subseteq$ (- $\infty$, + $\infty$) 随机变量的取值规律反映了随机现象的统计规律性，描述这种规律性的各种形式称为分布。 概率函数定义： 如果一个随机变量只可能取有限个值或可列无限个值，那么称这个随机变量为（一维）离散型随机变量. 离散型随机变量的分布的表现形式称为概率函数. 常见一维离散型分布0-1分布如果随机变量X 的概率函数为 $ P(X=0) = 1-p , P(X=1) = p (0&lt; p &lt;1) $ 那么称X服从参数为p的0-1分布，记为X~B(1, p). 也可以用下面的式子表示为 $ P(X = k) = p^k(1-p)^k , k = 0, 1 $ 二项分布如果随机变量X的概率函数为 P(X = k) = C^k_n p^k (1-p)^{n-k} , k = 0, 1, 2, ..., n则称X服从参数为n, p的二项分布，记为X~B(n, p). 其中0 &lt; p &lt; 1. 注： $ \sum^n_{k=0} C^k_n p^k (1-p)^{n-k} = (p + (1-p)) = 1$ 0-1分布可以看作是二项分布在n=1时的特例 泊松分布若随机变量X的概率函数为 $P(X=k) = \frac{\lambda ^ k}{k!} e^{-\lambda}, k =0, 1, 2, …, n$ 则称X服从参数为$\lambda$的泊松分布，记作X ~ P($\lambda$). 由无穷级数的知识可以验证 $\sum^\infty_{k=0} \frac{\lambda ^ k}{k!} e^{-\lambda} = 1$ - 泊松定理设 $\lambda = n p_n &gt; 0, 0 &lt; p_n &lt;1$，对任意一个非负整数k， \lim_{n\to \infty} C^k_n p^k_n (1 - p_n) ^{n-k} = \frac{\lambda ^k}{k!} e^{-\lambda}泊松定理告诉我们：二项概率可以用服从泊松分布的概率值来近似，且当 $ n \ge 10, p \le 0.1$时近似效果比较理想. 二维随机变量和概率函数二维随机变量定义： 给定一个随机试验，$\Omega$是它的样本空间，如果对$\Omega$中的每一个样本点$\omega$，都有一对有序实数$(X, Y)$与之对应，则称$(X, Y)$ 是二维随机变量. 如果一个二维随机变量只可能取有限个或可列无限个数值，则称其为二维离散型随机变量. 联合概率函数 设$(X,Y)$的值域为$\Omega_{(X, Y)} = \{(a_i, b_i): i, j = 1, 2, …\}$ 称表达式： P(X = a_i, Y = b_j) \doteq P(\{X = a_i \} \cap \{Y = b_j\}) = p_{ij} \\i = 1, 2, ...; j = 1, 2, ...为二维随机变量$(X, Y )$的联合概率函数或联合分布律. 其中$p_{ij}$须满足下列条件： $p_{ij} \ge 0 ;$ $\sum_i \sum_j p _{ij} = 1.$ 事实上利用联合概率函数，可以求任意事件的概率： P((X,Y) \in D) = \sum_{(a_i, b_j) \in D} P (X = a_i, Y = b_j) = \sum_{(a_i, b_j) \in D} p_{ij} 边缘概率函数 对于随机变量$(X, Y)$，分量$X$或$Y$本身就说一个一维随机变量，它们各自的概率函数就称为$(X, Y)$的关于$X$或关于$Y$的边缘概率函数或边缘分布律. 设随机变量$(X,Y)$的联合概率函数为 P(X = a_i, Y = b_j) = p_{ij}, \quad i, j =1, 2, ...则随机变量$X$的值域为$\Omega_x = \{a_1, a_2, …\}$， 而$X$的边缘概率函数或边缘分布律定义为 P = (X = a_i) = \sum_j p_{ij} \doteq p_{i·}, \quad i = 1, 2 , .. 随机变量$Y$的值域为$\Omega_y = \{b_1, b_2, …\}$， 而$Y$的边缘概率函数或边缘分布律定义为 P (Y = b_j) = \sum_i p_{ij} \doteq p_{·j} , \quad j = 1, 2, ... 随机变量的独立性和条件分布随机变量的独立性 设随机变量$X$与$Y$的联合概率函数为 P = (X = a_i, Y = b_j) = p_{ij} , \quad i, j = 1, 2, ...如果等式$P (X = a_i, Y = b_j) = P (X = a_i) \times P(Y = b_j)$ 也即$p_{ij} = p_{i·} p_{·j}$对所有的i, j = 1, 2, … 都成立，那么就称随机变量$X$和$Y$是相互独立的. ​ 如果随机变量$X_1, X_2, …, X_n$的联合概率函数恰为n个边缘概率函数的乘积，即对$a_i \in \Omega_{x_i}(i = 1, …, n)$， P(X_1 = a_1, X_2 = a_2, ..., X_n = a_n) = \prod^n_{i = 1} P(X_i = a_i)则称这n个随机变量$X_1, X_2, …, X_n$相互独立. 随机变量$X$与$Y$相互独立的充分必要条件是： 对于实数轴上的任意两个集合$S_1$与$S_2$，总有 P(X \in S_1, Y \in S_2) = P(X \in S_1) \times P(Y \in S_2).定理可以推广，故当$X_1, X_2, …, X_n$相互独立时，这n个随机变量中的任意k个也是相互独立的 ($2 \le k \le n-1$). 进一步n个随机变量相互独立保证它们两两独立. 条件概率函数 设随机变量$(X,Y)$的联合分布为 P(X = a_i, Y = b_j) = p_{ij} , \quad i, j = 1, 2, ...若对任意一个固定的$j$，$P(Y = b_j) &gt;0, (j=1, 2, …)$ 则称$ P(X = a_i | Y = b_j) = \frac{p_{ij}}{p_{·j}}, \quad i= 1, 2, …$ 为已知事件$\{Y = b_j\}$发生的条件下随机变量$X$的条件概率函数. 类似地，对任意一个固定的$i, P(X = a_i) &gt; 0, (i =1, 2, …),$ 称 P (Y = b_j | X = a_i) = \frac{p_{ij}}{p_{i·}} \quad j = 1, 2, ...为已知事件$\{X = a_i\}$发生的条件下随机变量$Y$的条件概率函数. 条件分布也是分布，易知$ \frac{p_{ij}}{p_{i·}}$或$\frac{p_{ij}}{p_{·j}}$满足 $ \frac{p_{ij}}{p_{i·}} \ge 0, \quad \frac{p_{ij}}{p_{·j}} \ge0$ $\sum_j \frac{p_{ij}}{p_{i·}} = 1, \quad \sum_i \frac{p_{ij}}{p_{·j}} = 1$ 随机变量函数的分布离散型随机变量函数的分布 在随机变量$X$上定义函数$g(X)$，表示一个随机变量$Y$ ，且当$X = x$时， $Y = y = g(x)$，并记作$Y = g(X)$. 那么$Y$就称为是随机变量$X$的函数. 离散型随机变量的分布可加性 定理1 设$X_1, X_2, …, X_n$是独立同分布的随机变量，且 X_i \sim B(1, p), \quad i = 1, 2, ..., n,记$Y = \sum^n_{i =1} $，则$Y \sim B (n, p).$ 思路 \begin{align*} & \ P (Y = k) \quad (k=0, 1, 2, ..., n) \\ & =P(\{X_1, ..., X_n 中恰有k个取值是1，n-k个取值是0\}) \\ & =C^k_n p^k (1-p)^{n-k} \end{align*} 定理2（分布的可加性） 设$X$与$Y$相互独立，那么 当 $X \sim B(m,p), Y \sim B(n,p) $ 时，有 X + Y \sim B(m+n, p); 当$X \sim P(\lambda_1), Y \sim P(\lambda_2)$ 时，有 X + Y \sim P(\lambda_1 + \lambda_2)._定理2可推广到n个相互独立的随机变量的和._ 定理3 设$X_1, X_2, …, X_n$ 是相互独立的随机变量，对于任意一个正整数$m(1\le m \le n-1)$，下面两个随机变量$g(X_1,X_2, …, X_m)$与$h(X_{m+1}, …, X_n)$ 相互独立，其中$g$与$h$都是单值函数. 由定理可知，若$X,Y$相互独立，则$g(X), h(Y)$ 也相互独立.]]></content>
      <categories>
        <category>概率论与数理统计</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性代数(一)]]></title>
    <url>%2F2018%2F07%2F09%2Flinear-algebra%2Flinear-algebra-1%2F</url>
    <content type="text"><![CDATA[线性代数是主要研究向量、向量空间、线性变换和线性方程组的一门学科，在工程上有广泛的应用，同时是机器学习的重要数学基础之一。本章主要内容为矩阵、行列式、解线性方程组。 线性代数(一)矩阵定义对于一个线性方程组，如下： a_{11}x_1 + a_{12}x_2 + a_{13}x_3 + ...+ a_{1n}x_n = b_1 \\ a_{21}x_1 + a_{22}x_2 + a_{23}x_3 + ...+ a_{2n}x_n = b_2 \\ a_{31}x_1 + a_{32}x_2 + a_{33}x_3 + ...+ a_{3n}x_n = b_3 \\ ...... \\ a_{n1}x_1 + a_{n2}x_2 + a_{n3}x_3 + ...+ a_{nn}x_n = b_n我们可以把未知项和算术符号去掉，写作如下形式： \left ( \begin{matrix} \ a_{11} \ a_{12} \ a_{13} ...\ a_{1n} \ b_1 \\ a_{21} \ a_{22} \ a_{23} ...\ a_{2n} \ b_2 \\ a_{31} \ a_{32} \ a_{33} ...\ a_{3n} \ b_3 \\ ...... \\ a_{n1} \ a_{n2} \ a_{n3} ...\ a_{nn} \ b_n \\ \end{matrix} \right )这就是线性方程组的矩阵形式 性质- 矩阵加法​ 设有两个$m\times n$矩阵 $A = (a_{ij})$ 和 $B = (b_{ij})$，那么矩阵 $A$ 与 $B$ 的和记作 $A+B$，规定为 A + B = \left ( \begin{matrix} \ a_{11}+b_{11} \quad a_{12}+b_{12} \quad a_{13}+b_{13} ...\quad a_{1n}+b_{1n} \\ a_{21}+b_{21} \quad a_{22}+b_{22} \quad a_{23}+b_{23} ...\quad a_{2n}+b_{2n} \\ a_{31}+b_{31} \quad a_{32}+b_{32} \quad a_{33}+b_{33} ...\quad a_{3n}+b_{3n} \\ ...... \\ a_{n1}+b_{n1} \quad a_{n2}+b_{n2} \quad a_{n3}+b_{n3} ...\quad a_{nn}+b_{nn} \\ \end{matrix} \right )​ 应该注意，只有两个矩阵是同型矩阵时，这两个矩阵才能进行加法运算. ​ 矩阵加法满足下列运算规律（设$A, B, C$都是$m \times n$矩阵 ) ； ​ (i) $A+B = B+A$ ​ (ii) $(A + B) + C = A +(B + C)$ - 矩阵数乘​ 设数$\lambda$与矩阵$A$的乘积记作 $\lambda A$ 或 $A\lambda$ ，规定为 \lambda A = A \lambda = \left ( \begin{matrix} \ \lambda a_{11} \ \lambda a_{12} \ \lambda a_{13} ...\ \lambda a_{1n} \\ \lambda a_{21} \ \lambda a_{22} \ \lambda a_{23} ...\ \lambda a_{2n} \\ \lambda a_{31} \ \lambda a_{32} \ \lambda a_{33} ...\ \lambda a_{3n} \\ ...... \\ \lambda a_{n1} \ \lambda a_{n2} \ \lambda a_{n3} ...\ \lambda a_{nn} \\ \end{matrix} \right )​ (i) $(\lambda \mu) A = \lambda (\mu A) ;$ ​ (ii) $(\lambda + \mu) A = \lambda A + \mu A ;$ ​ (iii) $\lambda(A+B) = \lambda A + \lambda B .$ ​ 矩阵加法运算和数乘统称为矩阵的线性运算. - 矩阵相乘​ 设 $A = (a_{ij})$ 是一个 $m \times s$ 的矩阵，$B = (b_{ij})$ 是一个 $s \times n$ 的矩阵，那么规定矩阵$A$与矩阵$B$的乘积是一个$m \times n$ 矩阵$C = c_{ij}$，其中 c_{ij} = a_{i1}b_{1j} + a_{i2}b_{2j} + ... + a_{is}b_{sj} = \sum^s_{k=1} a_{ik}b_{kj} \\ (i=1, 2, ..., m ; j =1, 2, ..., n),并把此乘积记作 C = A \times B 矩阵的乘法不满足交换律，即在一般情况下，$AB \ne BA$ 矩阵乘法满足下列结合律和分配律 - 矩阵的转置 把矩阵$A$的行换成同序数的列得到一个新矩阵，叫做$A$的转置矩阵，记作$A^T$. ​ 矩阵的转置运算满足下列运算律： ​ (i) $(A^T)^T = A ;$ ​ (ii) $(A +B)^T = A^T + B^T ;$ ​ (iii) $(\lambda A)^T = \lambda A^T ;$ ​ (iv) $(AB)^T = B^T A^T.$ - 方阵的行列式​ (有关行列式的知识点在下一节) ​ 由$n$阶方阵$A$的元素所构成的行列式（各元素的位置不变），称为方阵$A$的行列式，记作$det A$ 或 $|A|.$ ​ 由$A$确定$|A|$的这个运算满足下述运算规律（设$A, B$为$n$阶方阵，$\lambda 为数$）: ​ (i) $|A^T| = |A| ;$ ​ (ii) $|\lambda A| = \lambda ^n |A| ;$ ​ (iii) $|AB| = |A| |B|.$ - 伴随矩阵​ （余子式的概念在下一节“行列式按行(列)展开“处会提到） ​ 行列式 $|A|$ 的各个元素的代数余子式 $A_{ij}$ 所构成的如下矩阵 \left ( \begin{matrix} \ A_{11} \ A_{12} \ A_{13} ...\ A_{1n} \\ A_{21} \ A_{22} \ A_{23} ...\ A_{2n} \\ A_{31} \ A_{32} \ A_{33} ...\ A_{3n} \\ ...... \\ A_{n1} \ A_{n2} \ A_{n3} ...\ A_{nn} \\ \end{matrix} \right )称为矩阵 $A$ 的伴随矩阵，简称伴随阵. - 逆矩阵​ 对于$n$阶矩阵$A$，如果有一个$n$阶矩阵$B$，使 AB = BA = E ,则说矩阵$A$是可逆的，并把矩阵$B$称为$A$的逆矩阵，简称逆阵. ​ $A$的逆矩阵记作$A^{-1}.$ 即若$AB = BA = E ,$ 则 $B = A^{-1} .$ ​ 定理 1 若矩阵A可逆，则$|A| \ne 0 .$ ​ 定理 2 若$|A| \ne 0$，则矩阵$A$可逆，且 A^{-1} = \frac{1}{|A|} A^*其中$A^*$为矩阵$A$的伴随矩阵. ​ 当$|A| = 0 $ 时，$A$称为奇异矩阵，否则称为非奇异矩阵 ​ 由定理 2 ,有如下推论 ​ 推论 若 $AB = E$ (或 $BA = E$) ，则 $B = A^{-1}.$ ​ 逆矩满足如下运算律： ​ (i) 若$A$可逆，则$A^{-1}可逆$，且$(A^{-1})^{-1} = A ;$ (ii) 若$A$ 可逆，数 $\lambda \ne 0$，则$\lambda A$可逆，且$(\lambda A) ^{-1} = \frac{1}{\lambda} A^{-1} ;$ ​ (iii) 若$A、B$为同阶矩阵且可逆，则$AB$亦可逆，且$(AB)^{-1} = B^{-1}A^{-1} .$ 行列式定义​ 设有$n^2$个数，排成$n$行$n$列的数表 a_{11} \ a_{12} \ ... \ a_{1n} \\ a_{21} \ a_{22} \ ... \ a_{2n} \\ ....... \\ a_{n1} \ a_{n2} \ ... \ a_{nn} ,作出表中位于不同行不同列的$n$个数的乘积，并冠以符号$(-1)^t$，得到形如 (-1)^t a_{1p_1} a_{2p_2} ... a_{np_n}的项，其中$p1p2…pn$为自然数$1, 2, …, n$的一个排列，$t$为这个排列的逆序数. 由于这样的排列共有$n!$个，因而形如上式的项共有$n!$ 项，所有这$n!$ 项的代数和 \sum (-1)^t a_{1p_1}a_{2p_2}...a_{np_n}为n阶行列式，记作 D = \left | \begin{matrix} \ a_{11} \ a_{12} \ a_{13} \ ... \ a_{1n} \\ ...... \\ a_{i1} \ a_{i2} \ a_{i3} \ ...\ a_{in} \\ ...... \\ a_{n1} \ a_{n2} \ a_{n3} \ ...\ a_{nn} \\ \end{matrix} \right |简记作$det(a_{ij})$，其中数$a_{ij}$为行列式$D$的$(i, j)$元. 性质​ 1.行列式与它的转置行列式相等. ​ 2.对换行列式的两行(列)，行列式变号. ​ 推论: 如果行列式有两行(列)完全相同，则此行列式为零 ​ 3.行列式的某一行(列)中所有的元素都乘同一数k，等于用数k乘此行列式. ​ 推论:行列式中某一行(列)的所有元素的公因子可以提到行列式记号的外面. ​ 4.行列式中如果有两行(列)元素成比例，则此行列式等于零. ​ 5.若行列式的某一行(列)的元素都是两数之和，例如第i行的元素都是两数之和: D = \left | \begin{matrix} \ a_{11} \qquad a_{12} \qquad a_{13} \qquad ...\qquad a_{1n} \\ ...... \\ a_{i1} + a'_{i1} \ a_{i2} + a'_{i2} \ a_{i3}+a'_{i3} ...\ a_{in}+a'_{in} \\ ...... \\ a_{n1} \qquad a_{n2} \qquad a_{n3} ...\qquad a_{nn} \\ \end{matrix} \right |则$D$等于下列两个行列式之和： D = \left | \begin{matrix} \ a_{11} \ a_{12} \ a_{13} \ ... \ a_{1n} \\ ...... \\ a_{i1} \ a_{i2} \ a_{i3} \ ...\ a_{in} \\ ...... \\ a_{n1} \ a_{n2} \ a_{n3} \ ...\ a_{nn} \\ \end{matrix} \right | + \left | \begin{matrix} \ a_{11} \ a_{12} \ a_{13} \ ...\ a_{1n} \\ ...... \\ a'_{i1} \ a'_{i2} \ a'_{i3} ...\ a'_{in} \\ ...... \\ a_{n1} \ a_{n2} \ a_{n3} ...\ a_{nn} \\ \end{matrix} \right |​ 6.把行列式的某一行(列)的各元素乘同一数然后加到另一行(列)对应的元素上去，行列式不变. 行列式按行(列)展开定义​ 在$n$阶行列式中，把$(i,j)$元$a_{ij}$所在的第i行和第j列划去后，留下来的n-1阶行列式叫做$(i,j)$元$a_{ij}$的余子式，记作$M_{ij}$；记 A_{ij} = (-1)^{i+j}M_{ij}$A_{ij}$叫做$(i,j)$元$a_{ij}$的代数余子式. 引理​ 一个$n$阶行列式，如果其中第$i$行所有元素除$(i,j)$元$a_{ij}$以外都为零，那么这行列式等于$a_{ij}$与它的代数余子式的乘积，即 D = a_{ij}A_{ij} 定理​ 行列式等于它的任一行(列)的各元素与其对应的代数余子式乘积之和，即 D = a_{i1}A_{i1} + a_{i2}A_{i2} + ... + a_{in}A_{in} \qquad (i=1, 2, ..., n) \\ 或 \\ D = a_{1j}A_{1j} + a_{2j}A_{2j} + ... + a_{nj}A_{nj} \qquad (j=1, 2, ..., n) 求解线性方程组矩阵的初等变换初等变换​ 下面三种变换称为矩阵的初等行变换： ​ (i) 对换两行 ​ (ii) 以数k乘某一行中的所有元 ​ (iii) 把某一行所有元的k倍加到另一行对应的元上去 ​ 把定义中的“行”换成“列”，即为 初等列变换 等价关系​ 如果矩阵$A$经过有限次初等变换变成矩阵$B$，就称矩阵$A$与矩阵$B$等价，记作$A \sim B.$ ​ 矩阵的等价具有下列性质： ​ (i) 反身性 $A \sim A ;$ ​ (ii) 对称性 若$A \sim B$，则$ B \sim A ;$ ​ (iii) 传递性 若 $A \sim B$，$B \sim C$，则$A \sim C.$ 定理 1 定理1 设 $A$ 与 $B$ 为 $m \times n$ 矩阵，那么 (i) $A \sim^r B$的充分必要条件是存在m阶可逆矩阵$P$，使$PA = B ;$ (ii) $A \sim^c B$的充分必要条件是存在m阶可逆矩阵$P$，使$AQ = B;$ (iii) $A \sim B$的充分必要条件的是存在m阶可逆矩阵$P$及存在m阶可逆矩阵$P$，使$PAQ = B.$ 矩阵的秩子式​ 在$m \times n$ 矩阵$A$中，任取$k$行与$k$列，位于这些交叉处的$k^2$个元素，不改变他们在$A$中所处的位置次序而得到的$k$阶行列式，称为矩阵$A$的$k$阶子式. 秩​ 设在矩阵$A$中有一个不等于0的$r$阶子式$D$，且所有$r+1$阶子式全等于0，那么$D$称为矩阵$A$的最高阶非零子式，数$r$称为矩阵$A$的秩，记作$R(A).$ 定理 2 定理2 若$A \sim B$，则 $R(A) = R(B).$ 秩的基本性质 $0 \le R(A_{m \times n}) \le min \{m, n\}.$ $R(A^T) = R(A).$ 若$A \sim b$，则$R(A) = R(B).$ 若$P、Q$可逆，则$R(PAQ) = R(A).$ $max\{ R(A), R(B)\} \le R(A, B) \le R(A) +R(B).$ $R(A+B) \le R(A) + R(B).$ $R(AB) \le min\{R(A), R(B)\}.$ 若$A_{m\times n}B_{n\times l} = 0$，则$R(A) + R(B) \le n$ 线性方程组的解行阶梯形矩阵​ 每行的第一个非零元的下方的所有元素都为零，这样的矩阵称为行阶梯形矩阵。 行最简型矩阵​ 每行的第一个非零元的所在列，除了该元素外都为零，这样的矩阵称为行阶梯形矩阵。 齐次线性方程组​ 把$Ax = 0$这样的线性方程组称为齐次线性方程组。 非齐次线性方程组​ 把$Ax = b$这样的线性方程组称为非齐次线性方程组。 解线性方程组​ 将矩阵通过行变换化成行阶梯型矩阵，或行最简型矩阵，如下所示 \left ( \begin{matrix} \ 1 \ 2 \ 3 \ 4 \\ 2 \ 3 \ 4 \ 5 \\ 3 \ 4 \ 5 \ 6 \end{matrix} \right ) \rightarrow \left ( \begin{matrix} \ 1 \ 2 \ 3 \ 4 \\ 0 \ 1 \ 2 \ 3 \\ 0 \ 0 \ 1 \ 2 \end{matrix} \right ) \rightarrow \left ( \begin{matrix} \ 1 \ 0 \ 0 \quad 0 \\ \ \ 0 \ 1 \ 0 \ -1 \\ 0 \ 0 \ 1 \quad 2 \end{matrix} \right )​ 得到行阶梯型或行最简型矩阵后，便可以容易地得到方程组的解。 定理3~7 定理3 n元线性方程组$Ax = b$ (i) 无解的充分必要条件是$R(A) &lt; R(A, b) ;​$ (ii) 有唯一解的充分必要条件是$R(A) = R(A, b) = n ;​$ (iii) 有无限多解的充分必要条件是$R(A) = R(A ,b) &lt; n $ 定理4 n元齐次方程组 $Ax = 0$ 有非零解的充分必要条件是$R(A)&lt;n$ . 定理5 线性方程组$Ax = b$ 有解的充分必要条件是$R(A) = R(A, b).$ 定理6 矩阵方程$AX = B$ 有解的充分必要条件是$R(A) = R(A, B).$ 定理7 设$AB = C$，则$R(C) \le min \{R(A), R(B) \}$]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git使用教程]]></title>
    <url>%2F2018%2F06%2F12%2Fgit%2Fgit%2F</url>
    <content type="text"><![CDATA[Git 是目前最流行的分布式版本控制系统，本文概述了Git的基本使用方法。 GIT使用教程安装与配置 官网上下载安装程序 在命令行中输入： 12git config --global user.name &quot;Your Name&quot;git config --global user.email &quot;email@example.com&quot; (—global 表示这台机器上所有仓库都使用这个配置) 创建仓库 在合适的地方创建一个空文件夹 把这个文件夹变成仓库 12cd /yourdirgit init 添加文件在仓库的目录下，我们创建或者修改了一个文件，要怎么将它添加到仓库中呢 把文件添加到仓库 1git add &lt;file&gt; 把文件提交到仓库 1git commit -m &quot;&lt;tell the git somthing&gt;&quot; （-m 后是对本次提交的说明） 注意: commit 可以一次提交很多文件，所以可以add 多个文件后一次提交 查看仓库信息 查看仓库状态 1git status 查看修改内容 1git diff 版本回退 查看提交日志 1git log 回退上一个版本 在Git中，用HEAD表示版本，HEAD^表示上一个版本 1git reset --hard HEAD^ 回退指定版本 通过git log命令可以找到各个版本的commit id，可以用来指定回到某个版本 1git reset --hard &lt;commit_id&gt; 查看命令历史 1git reflog 工作区和缓存区 工作区：电脑中能看到的目录 缓存区：add 命令将修改提交到缓存区，commit命令 将缓存区中的修改提交到仓库 撤销修改 撤销工作区的修改 1git checkout -- file 撤销暂存区中的修改 1git reset HEAD &lt;file&gt; 回到1. 撤销已经提交的修改 12git reset --hard HEAD^git reset --hard commit_id 详情参考“版本回退” 删除文件在工作区中删除文件之后，可以有两个选择 从仓库删除 12git rm &lt;file&gt;git commit -m &quot;&lt;tell the git somthing&gt;&quot; 将误删的文件还原到最新版本 1git checkout -- test.txt]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>博客搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记之决策树]]></title>
    <url>%2F2018%2F06%2F08%2Fdiscrimination-model%2Fdecision-tree%2Fdecision-tree%2F</url>
    <content type="text"><![CDATA[决策树(decision tree)定义 ​ 分类决策树模型是一种描述对实例进行分类的树形结构，决策树由结点(node)和有向边(directed edge) 组成. 结点有两种类型：内部结点(internal node)和叶结点(leaf node). 内部结点表示一个特征或属性，叶结点表示一个类. ​ 用决策树分类，从根结点开始，对实例的某一特征进行测试，根据测试结果，将实例分配到其子结点；这时，每一个子结点对应着该特征的一个取值. 如此递归地对实例进行测试并分配，直到达到叶结点. 最后将实例分到叶结点的类中. —— 李航 《统计学习方法》 流程 数据输入 数据预处理 根据划分条件选择最佳划分属性 划分左右子树 递归构造树 剪枝处理 遍历树得到预测结果 评估模型 划分选择：信息增益(information gain)： 信息熵(information entropy) : 信息熵 是度量样本纯度的一种指标，样本纯度越高，信息熵越高；反之，样本纯度越低，信息熵越低。假定当前样本集合 $D$ 中第 $k$ 类样本所占的比例为 $p_k = (k=1,2,…|\gamma|)$，信息熵的公式如下： Ent(D) = - \sum_{k=1}^{|\gamma|} p_{k} log_{2} p_{k} (k=1,2,...,|\gamma|) 信息增益 : Gain(D, a) = Ent(D) - \sum^{V}_{v=1} \frac{|D^v|}{|D|} Ent(D^v) ID3算法使用信息增益作为划分指标，因为某一属性的信息增益越大，说明其对预测结果的影响更明显。因而，采用信息增益作为划分指标，就是计算不同属性的信息增益，从中选取最大的作为最优划分属性。 增益率(gain ratio)：由于信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好带来的不利影响，C4.5算法采用“增益率”来作为划分指标。 增益率: Gain\_ratio(D, a) = \frac{Gain(D, a)}{IV(a)}其中： IV(a) = - \sum^V_{v=1} \frac{|D^v|}{|D|} log_{2} \frac{|D^v|}{|D|}使用增益率作为划分指标，会对可取值数目较少的属性有所偏好，因此C4.5算法先从划分属性中选取信息增益高于平均水平的属性，再从中选择增益率最高的属性作为最优划分属性。 基尼指数(Gini index)： 基尼值: 基尼值反映了从样本集D中选取两个样本，其类别标记不一样的概率。可用来衡量样本纯度。 Gini(D) = \sum^{|\gamma|}_{k=1} \sum_{k' \neq k} p_{k} p_{k'} \\ =1- \sum^{|\gamma|}_{k=1}p^2_{k} 基尼指数: Gini_index(D, a) = \sum^V_{v=1} \frac{|D^v|}{|D|} Gini(D^v)CART算法从候选属性集中，选取使得基尼指数最小的属性作为最优划分属性。 剪枝处理介绍 决策树在学习的过程中，过多地考虑了如何提高训练数据的正确分类，从而构建了过于复杂的决策树. 这样的决策树对训练数据的分类很准确，但未知的测试数据的分类却不那么准确，即产生了过拟合(overfitting). 解决办法是采用剪枝(pruning)，对决策树进行简化. 预剪枝(prepruning) 预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能的提升，则停止划分并将当前结点标记为叶结点. 后剪枝(post-pruning)后剪枝是先从训练集生成一颗决策树，然后自底向上的对内部结点进行考察，若将该结点对应的子树替换为叶结点能够提高决策树泛化性能，则将改子树替换为叶结点. 代码实现下面是CART算法的python实现(分类树) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174import numpy as npclass Node: """ 树的结点的类 """ def __init__(self, fea=-1, value=None, results=None, right=None, left=None): self.fea = fea # 切分数据集属性的列的索引 self.value = value # 设置划分的值 self.results = results # 存储叶节点的值 self.right = right # 右子树 self.left = left # 左子树 def load_data(dir): """ 导入数据 :param dir:数据的路径 :return: 放入学习器的数据 """ tmp = np.loadtxt(dir, dtype=np.str, delimiter=",") # 数据集的划分视具体情况而定 data = tmp[1:500, 1:-2].astype(np.float) label = tmp[1:500, -2:].astype(np.float) # 将数据进行标准化 x_std = standard_transform(data) #print(np.hstack((x_std, label))) return np.hstack((x_std, label))def standard_transform(data): """ 将数据进行标准化处理 :param data: 原始数据 :return: 处理后的数据 """ return (data - np.mean(data, axis=0))/np.std(data, axis=0)def cal_gini(data): """ 分类树的划分指标 :param data: 训练集 :return: 基尼指数 """ total_sample = len(data) if len(data) == 0: return 0 # 统计数据集中不同标签的个数 label_counts = label_uniq_cnt(data) # 计算基尼指数 gini = 0 for label in label_counts: gini = gini + pow(label_counts[label], 2) gini = 1 - float(gini) / pow(total_sample, 2) return ginidef label_uniq_cnt(data): """ 计算不同标签的数量 :param data: 数据集 :return: 含有标签和标签所对应数量的字典 """ label_uniq_cnt = &#123;0.0: 0, 1.0: 0&#125; for x in data: label = x[len(x)-1] if label not in label_uniq_cnt: label_uniq_cnt[label] = 0 label_uniq_cnt[label] = label_uniq_cnt[label] + 1 return label_uniq_cntdef split_tree(data, fea, value): """ 根据特征fea中的值value将数据集data划分成左右子树 :param data: 训练样本 :param fea: 需要划分的特征索引 :param value: 指定的划分的值 :return: set_1, set_2 : 左右子树的集合 """ set_1 = [] set_2 = [] for x in data: if x[fea] &gt;= value: set_1.append(x) else: set_2.append(x) return set_1, set_2def build_classify_tree(data, min_sample, min_gain): """ 构建分类树 :param data: 训练样本 :param min_sample: 子节点中最少的样本数 :param min_gain: 最小的划分指标(基尼指数) :return: 树的根节点 """ if len(data) &lt; min_sample: return Node(results=label_uniq_cnt(data)) pre_gini = cal_gini(data) best_gain = 0.0 best_criteria = None best_sets = None feature_num = len(data[0]) - 1 for fea in range(0, feature_num): feature_values = &#123;&#125; for sample in data: feature_values[sample[fea]] = 1 for value in feature_values.keys(): # 尝试划分节点 set_1, set_2 = split_tree(data, fea, value) now_gini = float(len(set_1) * cal_gini(set_1) + len(set_2) * cal_gini(set_2)) / len(data) gain = pre_gini - now_gini # 根据基尼指数的增量挑选最佳划分 if gain &gt; best_gain and len(set_1) &gt; 0 and len(set_2) &gt; 0: best_gain = gain best_criteria = fea, value best_sets = set_1, set_2 #print("best_gain: ", best_gain) if best_gain &gt; min_gain: right = build_classify_tree(best_sets[0], min_sample, min_gain) left = build_classify_tree(best_sets[1], min_sample, min_gain) return Node(fea=best_criteria[0], value=best_criteria[1], right=right, left=left) else: return Node(results=label_uniq_cnt(data)) def cal_accuracy_classify(data, tree): """ 评估CART回归树 :param data: 数据 :param tree: 训练好的CART分类树 :return: 正确率 """ m = len(data) # 特征数n视实际数据集而定 n = len(data[0]) - 2 acc = 0.0 label_level_mat = np.mat(data[:, -1]).transpose() for i in range(m): tmp = [] for j in range(n): tmp.append(data[i][j]) pre = predict(tmp, tree) if pre[1.0] &gt; pre[0.0] and label_level_mat[i] == 1: acc += 1.0 elif pre[1.0] &lt;= pre[0.0] and label_level_mat[i] == 0: acc += 1.0 return acc/m def main(): # 加载训练集 print("----------1.load data------------") train_data = load_data('train_data.csv') # 生成 CART print('----------2.build CART-----------') cart_tree = build_classify_tree(data, min_sample=25, min_gain=0.02) # 载入验证集并评估模型 print("----------3.cal accuracy-------------") vfc_data = load_data("verification_data.csv") vfc_accuracy = cal_accuracy_classify(vfc_data, cart_tree) print("-------4.verification_accuracy: &#123;:.2%&#125;".format(vfc_accuracy))main() 我们还可以由CART构建随机森林(Random Forest)，此处有详解：RF . 参考资料： 周志华 《机器学习》 李航 《统计学习方法》]]></content>
      <categories>
        <category>判别模型</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>判别模型</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记之逻辑回归]]></title>
    <url>%2F2018%2F06%2F07%2Flinear-model%2Flogistic-regression%2F</url>
    <content type="text"><![CDATA[在线性回归中，我们希望通过一条直线尽可能地拟合数据，从而实现对数据的预测，但是假如我们需要对数据进行分类，那么我们就需要找到一条直线（或一个超平面）尽可能地将属于不同的标签数据分开，于是我们可以使用逻辑回归算法实现该功能。 逻辑回归(logistic regression)函数模型 在线性回归中，我们使用 $h(x) = \theta^Tx$ 作为预测函数，显然 $h(x) \in (-\infty, +\infty)$ 。但在进行分类任务时，显然我们希望函数值落在 $(0,1)$ 的区间内，且函数关于原点对称，于是Sigmoid函数： g(z) = \frac{1}{1+e^{-z}} \tag{1.1}便是一个很好的选择。 其图像为 损失函数(loss function) 我们对Sigmoid函数求导，得到： g'(z) = \frac{e^{-z}}{(1+e^{-z})^2} = g(z)(1-g(z)) \tag{2.1}我们假设输出概率函数如下，这实际上是一个伯努利分布(Bernoulli distribution)也称为两点分布、0-1分布 ： \begin{align*} P(y=1|x;\theta) &= g(\theta^Tx)&= \frac{1}{1+e^{-\theta^T x}} &= h_\theta(x) \\ \tag{2.2} P(y=0|x;\theta) &= 1- g(\theta^Tx)&= \frac{e^{-\theta^Tx}}{1+e^{-\theta^T x}} &= 1- h_\theta(x) \end{align*}于是，我们可以得到一般的输出概率函数为： P(y |x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y} \tag{2.3}我们采用对数似然估计(相应内容：极大似然估计)，可以得到 \begin{align*} l(\theta) &= \sum^m_{i=1} log P(y^{(i)}|x^{(i)};\theta) \\ &= \sum^m_{i=1} log (h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}} \\ &= \sum^m_{i=1}y^{(i)} log (h_\theta(x^{(i)})) + \sum^m_{i=1}(1-y^{(i)})log(1-h_\theta(x^{(i)})) \\ \end{align*} \tag{2.4}我们希望当损失函数 $J(\theta)$ 达到最小时 $l(\theta) $ 达到最大值，因此我们希望 $J(\theta)$ 和 $l(\theta)$ 是负相关的关系，于是可取： \begin{align*} J(\theta) &= -\frac{1}{m} l(\theta) \\ &=\frac{1}{m} \sum^m_{i=1}-y^{(i)} log (h_\theta(x^{(i)})) - \sum^m_{i=1}(1-y^{(i)})log(1-h_\theta(x^{(i)})) \\ \end{align*} \tag{2.5}参数更新方法：梯度下降法(gradient descent)： 我们对损失函数 $J(\theta)$ 进行求导，得到： \begin{aligned} \frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m}[\sum^m_{i=1}-y^{(i)}\frac{1}{h_\theta(x^{(i)})} +\sum^m_{i=1}(1-y^{(i)})\frac{1}{1-h_\theta(x^{(i)})}] \frac{\partial h_\theta(x^{(i)})}{\partial\theta_j} \end{aligned} \tag{3.1}化简为： \frac{\partial J(\theta)}{\partial\theta_j} = \frac{1}{m} \sum^m_{i=1} (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} \tag{3.2}于是参数的更新方程为： \theta_{i} := \theta_{i} - \alpha \frac{1}{m} \sum^m_{i=1} (h_{\theta}(x^{(i)})-y^{(i)}) x^{(i)} \tag{3.3}(此处与线性回归的方程是一样的) 正规方程(normal equation)： \theta = (X^T X)^{-1} X^T y \tag{4.1}代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768def load_data(dir): """ 导入训练数据 :param dir:训练数据的路径 :return: 放入学习器的数据 """ tmp = np.loadtxt(dir, dtype=np.str, delimiter=",") data = tmp[1:, 1:-2].astype(np.float) label = tmp[1:, -1].astype(np.float) return data, labeldef sigmoid(in_x): """ sigmoid函数 :param in_x:特征值x矩阵 :return: 映射到sigmoid函数上的函数值 """ return 1.0/(1+exp(-in_x))def grad_ascent(data_array, label_array, alpha, max_cycles): """ 梯度下降法更新参数 :param data_array: 特征集 :param label_array: 标签集 :param alpha: 学习率 :param max_cycles: 最大迭代次数 :return: 参数 weigh """ data_mat = mat(data_array) # size:m*n label_mat = mat(label_array).transpose() # size:m*1 m, n = shape(data_mat) weigh = ones((n, 1)) for i in range(max_cycles): h = sigmoid(data_mat * weigh) error = label_mat-h # size:m*1 weigh = weigh+alpha*(data_mat.transpose())*error return weighdef classfy(dir, weigh): """ 对验证集进行分类预测，并评估模型 :param dir: 验证集的路径 :param weigh: 训练好的参数 :return: None """ data_array, label_array = load_data(dir) data_mat = mat(data_array) label_mat = mat(label_array).transpose() h = sigmoid(data_mat * weigh) # size:m*1 m = len(h) cnt_acc = 0.0 for i in range(m): if int(h[i]) &gt; 0.5 and label_mat[i] == 1: cnt_acc += 1 elif int(h[i]) &lt;= 0.5 and label_mat[i] == 0: cnt_acc += 0 acc_rate = cnt_acc/m print(acc_rate) if __name__ == '__main__': print("----------1.load data------------") # 导入数据按实际数据情况进行修改 data, label = load_data('train_data.csv') print("----------2. training------------") # 学习率alpha和最大迭代次数max_cycles根据实际情况制定 weigh = grad_ascent(data, label, alpha=0.05, max_cycles=10) print("----------3. testing------------") classfy('verification_data.csv', weigh) 参考资料: [1] 周志华. 机器学习[M], 北京: 清华大学出版社, 2016: 53-60.]]></content>
      <categories>
        <category>线性模型</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>逻辑回归</tag>
        <tag>判别模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习笔记之线性回归]]></title>
    <url>%2F2018%2F06%2F07%2Flinear-model%2Flinear-regression%2F</url>
    <content type="text"><![CDATA[线性回归是机器学习中最简单、最基础的模型，其核心思想是构造一个线性函数，通过更新参数使其与真实值的误差尽可能地小。 线性回归函数模型 我们假定目标函数 $h_\theta$ 与 $x$ 服从线性关系： h_\theta(x) = \theta_1 x_1 + \theta_2x_2 + ... + \theta_n x_n即可以用向量方程代替： h_{\theta}(x) = \theta^T x损失函数(loss function) 为了衡量该函数与真实值的偏差程度，我们定义如下函数，即损失函数(代价函数 cost function): J(\theta) = \frac{1}{2m} (h_{\theta}(x)-y)^2因此，我们训练的目的就是 最小化损失函数$J(\theta)$ 参数更新梯度下降法： 可以看到损失函数 $J(\theta)$ 是关于参数 $\theta$ 的一个函数，实际上我们就是需要调整 $\theta$ 的值，使损失函数 $J(\theta)$ 减小，因此需要找到 $\theta $ 的一个变化方向。 这里我们对 $J(\theta)$ 求偏导得到： \frac{\partial j}{\partial \theta} = \frac{1}{m} \sum^m_{i=1} (h_\theta(x) - y) \frac{\partial h_\theta}{\partial \theta} = \frac{1}{m} \sum^m_{i=1} (h_\theta(x) - y) x 可以知道，当偏导数 $\frac{\partial j}{\partial \theta} &gt; 0$ 时，$J(\theta)$ 随着 $\theta$ 的增大而增大； $\frac{\partial j}{\partial \theta} &lt; 0$ 时，$J(\theta)$ 随着 $\theta$ 的增大而减小。因此，我们使参数 $\theta $ 朝着偏导数 $\frac{\partial j}{\partial \theta} &gt; 0$ 变化，就可以使得损失函数 $J(\theta)$ 减小： \theta : = \theta - \alpha \frac{\partial j}{\partial \theta}​ 对每一个参数 $\theta_i$，同样有： \theta_i : = \theta_i - \alpha \frac{\partial j}{\partial \theta}其中，$\alpha$ 称为学习率(learning rate)，其影响着参数 $\theta$ 变化的快慢，若 $\alpha$ 太小会使学习过程变得缓慢；若$\alpha$ 太大则使得参数变化的过程震荡而无法收敛。 我们将前面求得的偏导数 $\frac{\partial j}{\partial \theta}$ 代入即得到每一个参数 $\theta_i$ 的更新方程： \theta_{i} := \theta_{i} - \alpha \frac{1}{m} \sum^m_{i=1} (h_{\theta}(x^{(i)})-y^{(i)}) x^{(i)}代码实现123456789101112131415161718192021222324252627282930313233343536373839from numpy import *import numpy as npimport pandasimport matplotlib.pyplot as pltdef load_data(dir): """ 导入训练数据 :param dir:训练数据的路径 :return: 放入学习器的数据 """ tmp = np.loadtxt(dir, dtype=np.float) x_array = tmp[:, :-1] y_array = tmp[:, -1] return x_array, y_arraydef grad_descent(data_array, label_array, alpha, max_cycles): """ 利用梯度下降法更新参数 :param data_array: 数据集 :param label_array: 标签集 :param alpha: 学习率 :param max_cycles: 最大迭代次数 :return: 参数 theta """ data_mat = mat(data_array) # size:m*n label_mat = mat(label_array).transpose() # size:m*1 m, n = shape(data_mat) one = ones((m, 1)) data_mat_1 = np.hstack((one, data_mat)) # 加入为1的列向量，使方程存在偏移项theta0 weigh = ones((n+1, 1)) for i in range(max_cycles): h = data_mat_1 * weigh error = h - label_mat theta = theta - (alpha / m) * data_mat_1.T * error return theta 正规方程：使用最小二乘的 (least square )的 闭式解法 (closed-form solution)直接对参数θ进行更新 公式： \theta = (X^T X)^{-1} X^T y代码实现12345def norm_equation(data_array, label_array): data_mat = mat(data_array) # size:m*n label_mat = mat(label_array).transpose() # size:m*1 m, n = shape(data_mat) theta = (x_mat.T * x_mat).I * x_mat.T * y_mat.T 探究损失函数 我们提出一个问题，为什么线性回归的代价函数(损失函数)是 J(\theta) = \frac{1}{2m} (h_{\theta}(x)-y)^2仅仅是因为这个函数好求导吗？或许不是。 ​ 我们假定样本库中的所有样本相互独立，且样本为： \{(x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)}),...,(x^{(m)}, y^{(m)})\}它们与输出的关系为 y^{(i)} = \theta^Tx^{(i)} + \varepsilon^{(i)}其中 $\varepsilon^{(i)}$ 为样本标签与线性回归输出的误差，我们假设 $\varepsilon^{(i)}$ 服从高斯分布，即 $\varepsilon^{(i)} \sim (0, \sigma^2)$ ，所以可得： p(\varepsilon^{(i)}) = \frac{1}{\sqrt{2 \pi} \sigma} exp(-\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2})由样本相互独立可知，它们的联合概率为： L(\theta) = \prod^m_{i=1}P(y^{(i) }| x^{(i)}, \theta)为方便运算，对上式取对数，有： l(\theta) = log L(\theta) = \sum^m_{i=1}log P(y^{(i) }| x^{(i)}, \theta) \\ \qquad \qquad \qquad \qquad \qquad \qquad \ = m log\frac{1}{\sqrt{2\pi}\sigma}-\sum^m_{i=1}\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2\sigma^2}观察上式，当式子： \sum^m_{i=1}\frac{(y^{(i)}-\theta^Tx^{(i)})^2}{2}达到最小值时，$L(\theta)$ 达到最大值，此时样本标签与线性回归输出的接近程度最大，对上式取平均，我们即可以得到代价函数 $J(\theta)$ ： J(\theta) = \frac{1}{2m} (h_{\theta}(x)-y)^2 参考资料: [1] 周志华. 机器学习[M], 北京: 清华大学出版社, 2016: 53-60.]]></content>
      <categories>
        <category>线性模型</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>线性回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F06%2F07%2Ftest%2F</url>
    <content type="text"><![CDATA[this a test file TestTesthello world !!!]]></content>
  </entry>
</search>
