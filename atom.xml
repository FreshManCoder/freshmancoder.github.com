<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>One Blog</title>
  
  <subtitle>好好学习</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-08-22T14:46:34.519Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Yao-zz</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>NLP——分词</title>
    <link href="http://yoursite.com/2020/08/22/natural-language-processing/Word-Segmentation/"/>
    <id>http://yoursite.com/2020/08/22/natural-language-processing/Word-Segmentation/</id>
    <published>2020-08-22T14:45:45.255Z</published>
    <updated>2020-08-22T14:46:34.519Z</updated>
    
    <content type="html"><![CDATA[<h1 id="NLP——分词"><a href="#NLP——分词" class="headerlink" title="NLP——分词"></a>NLP——分词</h1><p><strong>分词</strong>，即给定一份词之间无间隔符的语料，将词与词切分开。分词是中文NLP任务里的基础性任务，绝大部分任务都需要先分词，再将词的特征输入模型中进行训练和预测（现在也有基于字的模型，但分词仍是中文NLP任务中的重点）。</p><p>一般来说，中文的分词主要面临着三大问题：</p><ul><li><p><strong>分词规范问题</strong>，即“什么是词，怎么界定词”的问题，同样一份语料，不同的人可能会给出截然不同的划分结果，那么究竟哪种是对的呢。即便我国很早就制定了《分词规范》，但大部分的规定都是举例和定性的方式体现，这种规范的操作尺度，极易受主观因素影响。</p></li><li><p><strong>歧义切分问题</strong>，比如“北京大学”，究竟是切分成一个词，还是要切分成“北京”和“大学”呢，虽然我们人类知道前者是对的，但机器是不知道的。</p></li><li><strong>未登录词（unknown word，又称Out Of Vocabulary, OOV）</strong>，即不在词典里的新词，又或者指训练语料中的从未出现的词。由于词典可以由大规模的训练语料获得，所以一般这两者我们可以看作一回事。</li></ul><a id="more"></a><h2 id="基于词典切分"><a href="#基于词典切分" class="headerlink" title="基于词典切分"></a>基于词典切分</h2><p>基于词典进行切分是最简单粗暴的方式，假设有一份词典，那么给定语料，直接基于词典进行匹配即可，但也存在一些问题。</p><p>假设词典中有：[…，“北京”，“北京大学”，“大学生“，“学生”，“生物”，“系”，…]，给定句子：”北京大学生物系”，我们人类知道，应该要划分成：北京大学/生物/系。</p><p>但机器要如何划分呢？词典中既有”北京“，也有”北京大学“，这就产生了歧义性的问题。</p><p>一种简单的方式是，把所有的可能都切分出来，即切分为：[“北京”，“北京大学”，“大学生“，“学生”，“生物”，“系”]。这便是<strong>全切分方式</strong>。</p><p>事实上，常见的切分方式有：</p><ul><li><p>全切分，即把所有的可能性的分词都列出来</p></li><li><p>最长匹配，往往较长的词会比较短的词更有意义，比如”北京大学“就比”北京”/“大学“要更合适。</p><ul><li><p><strong>正向最长匹配</strong>，给定字串，从前往后匹配最长的子串作为词。使用上述的例句，切分为：[”北京大学“， ”生物“，”系“]</p></li><li><p><strong>逆向最长匹配</strong>，给定字串，从后往前匹配最长的子串作为词。即：[”北京大学”，“生物，“系”]。可以看到，虽然匹配的规则不一样，但结果也是有可能一样的。</p></li><li><p><strong>双向最长匹配</strong>，结合了上述两种规则。流程如下：</p><ol><li>同时执行正向和逆向最长匹配，若两者词数不同，返回词数较少的那一个；</li><li>否则，返回两者中<strong>单字</strong>更少的一个。如果还是相同，那么就返回逆向最长匹配的结果。</li></ol><p>这是一种启发式的算法。语言学的研究发现，汉语中单字词的数量要远远小于非单字词的数量。所以，算法应当尽量地保留更多的完整词语，而减少单字。</p></li></ul></li></ul><p>虽然有了这些匹配的规则，可以对字串进行分词，但显然并没有解决<strong>歧义性和OOV</strong>的问题。因此我们需要利用一定的统计信息，来实现更精准的分词。</p><h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><h3 id="语言模型-1"><a href="#语言模型-1" class="headerlink" title="语言模型"></a>语言模型</h3><p>​    <strong>语言模型</strong>，是指使用概率语言，去描述语言的模型。简单来说，就是判断一句话”有多少概率是人话“。可以想到最简单的方法，我们看看一句话在所有话中是否出现过，如果是，则概率为1，否则为0。但这样显然是有问题的，我们不可能穷举所有的“人话”，即便可以，从一个巨大的句子库中搜索也是一个巨大的问题，而且新句子除非加入了句子库，否则它的概率永远为0。这种现象被称为<strong>数据稀疏</strong>。</p><blockquote><p>举个例子：”我考上了北京大学“ 和 ”我考上北京大学了“，这两句话只有细微的差别，但意思上是一样的，然而按照刚才的规则，他们是截然不同的两句话。如果第一句话在句子库中，而第二句不在，那么当我们去识别第一句时，会把它的概率计算为1，但识别第二句时，却是0，这显然是不合理的。</p></blockquote><p>​    我们可以考虑另一种方式。词是句子的组成部分，那么我们可以将句子切分为一个个的词，以词的概率去估计句子的概率。这里则又回到了<strong>分词</strong>的问题，我们可以先假定句子里的词已经做好了切分。那么句子的概率则可以简单地表示为如下的形式：</p><script type="math/tex; mode=display">p(s) = p(w_1)*p(w_2)...*p(w_n)</script><p>其中，$p(s)$为句子的概率，$p(w)$为词的概率，即把句子的概率表示为词概率的连乘的形式。词的概率可以由频率估计，比如北京在词库中一共出现了200次，而总次数为20000，那么$p(北京)=200/20000=0.01$。</p><p>​    回到前面的例子，”我考上了北京大学“ 和 ”我考上北京大学了“，如果采用如上的模型，并且分词结果一样，那么概率是一样的。那如果分词结果不一样呢？很显然，不一样的分词，往往计算出来的句子概率是不一样的（因子不一样了）。也就是说：分词-&gt;句子的概率，那么是否可以由句子的概率得到分词呢？显然是可以的，只要<strong>选择概率最大的那个分词结果即可</strong>。</p><p>​    <strong>这便是”基于语言模型分词”的核心思想，选择使得句子概率最大的分词结果。</strong></p><p>​    但目前的语言模型存在着一些问题，其中最大的问题是，无序性。</p><blockquote><p>举个例子，”牛吃草”和”草吃牛”，当前模型是不考虑语序的，因此他们的概率是一样的，但我们从常识上知道，前者是更合理的。</p></blockquote><p>​    为了使得模型考虑语序，我们需要为模型加上一些上下文的信息。以上文为例，我们只需要每次考虑当前词语的”上文词“。</p><script type="math/tex; mode=display">p(s) = p(w_1)*p(w_2|w_1)*p(w_3|w_1,w_2)...*p(w_n|w_1,w_2,...,w_{n-1})</script><p>我们可以使用<strong>极大似然估计(MLE)</strong>来计算这些概率（后验概率）。即：</p><script type="math/tex; mode=display">p(w_t|w_0,...,w_{t-1}) = \frac{c(w_0,...,w_t)}{c(w_0,...,w_{t-1})}</script><p>其中，$c(w_0,…w_t)$ 表示词$w_0,…,w_t$ 的共现次数。（这些词一起出现的次数）</p><p>然而，这并没有很好地解决数据稀疏的问题，随着句子长度的增加，语料库中很可能统计不到该句子的频次，导致其中$p(w_t|w_0,…,w_{t-1})$为0，进而相乘导致整句话的概率为0。</p><h3 id="n-元语法语言模型"><a href="#n-元语法语言模型" class="headerlink" title="n 元语法语言模型"></a>n 元语法语言模型</h3><p>​    为了解决这个问题，我们需要简化模型，即每次不考虑太长的信息。比如我们<strong>只考虑当前词的前一个词</strong>，则：</p><script type="math/tex; mode=display">p(s) = p(w_0)*p(w_1|w_0)*p(w_2|w_1)...*p(w_{t}|w_{t-1})</script><p>这样一来，就不容易出现句子长度太长导致的数据稀疏问题，并且计算的速度也得到了提升。我们把这种只考虑当前词的前一个词的语言模型称为<strong>二元语法(bi-gram)模型</strong>。</p><p>​    类似的，<strong>我们可以只考虑该单词前n-1个单词，即n元语法(n-gram)</strong>。一般来说，我们不会使用n&gt;3的语法模型，因为往往这样一来数据稀疏的问题又会变得显著起来。特别地，n=1时，称为一元语法(unigram)；n=2时，称为二元语法(bi-gram)；n=3时，称为三元语法(tri-gram)。</p><p>​    事实上，为了进一步缓解数据稀疏的问题，还需要引入数据平滑的策略。常见的方式会使用低阶的（如一元）语法，来平滑高阶语法，以防止计算的概率为0。比如这样：</p><script type="math/tex; mode=display">p(w_t|w_{t-1}) = \lambda p(w_t|w_{t-1}) + (1-\lambda)p(w_t)</script><p>其中，$\lambda $ 为常数平滑因子，一般为0~1。这里的中心思想在于，引入更”稠密”的低阶语法特征，来防止高阶语法的概率为0。</p><p>​    另外，也可以使用取对数的方式，将连乘转化为累加，可以进一步防止某个因子为0而导致整句话的概率为0。这里不在赘述。</p><h2 id="基于序列标注切分"><a href="#基于序列标注切分" class="headerlink" title="基于序列标注切分"></a>基于序列标注切分</h2><p>​    近年来，深度学习的崛起改变了AI的格局，凭借深度、广度、算力，击败了各种从前基于规则的、基于统计机器学习的方法。可以说，人工智能迎来了深度学习的时代。</p><p>​    自然语言处理界也不例外，深度学习的”end-to-end”特性（即输入-模型-输出），使得各种任务都可以转化成“分类”任务。可以分为输出为一个类别的任务，和输出为一个序列的任务。前者最常见的是文本分类、情感分类；而后者本质上就是多次输出类别，常见的任务则是命名实体识别、分词、翻译、对话等。</p><p>​    回到分词任务，我们要怎么把深度学习应用在分词上呢？可以以<strong>序列标注</strong>的方式来实现。序列标注即对输入序列进行标注，可以应用于命名实体识别、句法分析、分词等。特别地，在分词任务上，可以给字标注上“B”、“M”、“E”的标签，分别代表词的开始(begin)、中间(middle)，以及结束(end)。</p><blockquote><p>再以“我考上了北京大学”为例，序列标注可以以如下的形式：</p><p>我/B，考/B，上/E，了/B，北/B，京/M，大/M，学/E</p><p>对应的分词为：我/考上/了/北京大学</p><p>实际上，也并不需要用3个标签，BM两个标签其实就足以将词划分开，只需约定下一个词的B即表示本词已结束即可，如：</p><p>我/B，考/B，上/M，了/B，北/B，京/M，大/M，学/M</p><p>划分出来和上面是一样的结果。</p></blockquote><p>​    上面提到，深度学习即”输入-模型-输出”，”输入”一般是做一些预处理，这里不再赘述，上面”序列标注”实际上就约定好了”输出”的形式，那么还剩下一个”模型”。</p><p>​    “模型”实际上就是广大深度学习的研究者研究的内容，比较常见的模型如卷积神经网络（CNN）、循环神经网络（RNN），以及近年来大热的Transformer，研究者们会根据数据和任务，对这些模型做改进和结合，成为新的模型。这里面的知识可谓是博大精深，并不是小小一篇文章就能概括完的。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;NLP——分词&quot;&gt;&lt;a href=&quot;#NLP——分词&quot; class=&quot;headerlink&quot; title=&quot;NLP——分词&quot;&gt;&lt;/a&gt;NLP——分词&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;分词&lt;/strong&gt;，即给定一份词之间无间隔符的语料，将词与词切分开。分词是中文NLP任务里的基础性任务，绝大部分任务都需要先分词，再将词的特征输入模型中进行训练和预测（现在也有基于字的模型，但分词仍是中文NLP任务中的重点）。&lt;/p&gt;
&lt;p&gt;一般来说，中文的分词主要面临着三大问题：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;分词规范问题&lt;/strong&gt;，即“什么是词，怎么界定词”的问题，同样一份语料，不同的人可能会给出截然不同的划分结果，那么究竟哪种是对的呢。即便我国很早就制定了《分词规范》，但大部分的规定都是举例和定性的方式体现，这种规范的操作尺度，极易受主观因素影响。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;歧义切分问题&lt;/strong&gt;，比如“北京大学”，究竟是切分成一个词，还是要切分成“北京”和“大学”呢，虽然我们人类知道前者是对的，但机器是不知道的。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;未登录词（unknown word，又称Out Of Vocabulary, OOV）&lt;/strong&gt;，即不在词典里的新词，又或者指训练语料中的从未出现的词。由于词典可以由大规模的训练语料获得，所以一般这两者我们可以看作一回事。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="分词" scheme="http://yoursite.com/tags/%E5%88%86%E8%AF%8D/"/>
    
  </entry>
  
  <entry>
    <title>GNN应用于Aspect-Based Sentiment Analysis任务的研究的简单概述</title>
    <link href="http://yoursite.com/2020/05/13/deep-learning/A%20brief%20survey%20of%20papers%20on%20GNN%20for%20ABSA/"/>
    <id>http://yoursite.com/2020/05/13/deep-learning/A%20brief%20survey%20of%20papers%20on%20GNN%20for%20ABSA/</id>
    <published>2020-05-12T16:16:20.630Z</published>
    <updated>2020-06-14T08:02:52.648Z</updated>
    
    <content type="html"><![CDATA[<h1 id="A-brief-survey-of-papers-on-GNN-for-Aspect-Based-Sentiment-Analysis"><a href="#A-brief-survey-of-papers-on-GNN-for-Aspect-Based-Sentiment-Analysis" class="headerlink" title="A brief survey of papers on GNN for Aspect-Based Sentiment Analysis"></a>A brief survey of papers on GNN for Aspect-Based Sentiment Analysis</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>​    In previous years, there are lots of research about Graph Neural Network(GNN), a modified neural network for graph data. And some researchers focus on using GNN for the NLP tasks, such as text classification(TextGCN, Yao et al.) and comment generation(graph2seq, Li et al.) . Sun et al. had purposed a new model called CDT, which is an application of Aspect-Based Sentiment Classification, the sub-task of Aspect-Based Sentiment Analysis(ABSA). Inspired by Sun et al., some researchers considered to using the GNN for ABSA. We focus on reviewing their rearches, and discussing about the future work in those aspects.</p><p>​    最近关于图神经网络（Graph Neural Network）的研究非常火热，这是一种适应于图数据的神经网络，并且最近在NLP界也开始大展拳脚，比如文本分类任务（TextGCN，Yao et al.）和评论生成任务（graph2seq，Li et al.）。Sun et al. 提出了一个新的模型——CDT, 应用于Aspect-Based Sentiment Classification 任务，属于ABSA下的一个分支。受Sun et al.的研究的启发，一些研究人员开始致力于将GNN应用到ABSA任务上。我们希望回顾这些研究，并讨论一下这些研究未来的发展。</p><a id="more"></a><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>​    这里简要介绍一下图神经网络，ABSA任务，以及GNN在ABSA方面的前沿研究。</p><h3 id="Graph-Nerual-Network"><a href="#Graph-Nerual-Network" class="headerlink" title="Graph Nerual Network"></a>Graph Nerual Network</h3><p>​    图神经网络，是一种能适应于图数据的神经网络，其基本的思想是，定义图上结点与他的邻居之间的信息传递方式，并通过矩阵运算实现。</p><p>​    而图这种信息的传递和聚合，在清华大学孙茂松教授组发表的<a href="https://arxiv.org/pdf/1812.08434.pdf" target="_blank" rel="noopener">图神经网络综述</a>中，划分为如下几种方式：</p><ul><li>Convolutional Aggregator：基于图卷积的信息聚合，一般分为谱域（Spectral）和空域（Spatial）；</li><li>Attention Aggregator：基于注意力机制的信息聚合；</li><li>Gated Updater：类似于GRU和LSTM的门控机制，提升在图信息上的长距离（long-term）传递；</li><li>Skip Connection：在普通的神经网络中也十分常用，受residual network的启发，通过层之间的skip connection来增加网络深度，以获得更大的表示空间；</li><li>Hierachical Graph：在一些图数据中，图和结点的层次往往也是非常重要的信息，目前的处理方式也是受CV领域的启发，通过层次化的池化（Hierachical Pooling），层层抽取一般化的特征；</li></ul><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/image-20200614150538285.png" style="zoom:45%;"></p><p>GNN还有一种常见的分类，即直推式（Transductive）和归纳式（Inductive）:</p><ul><li>Transductive：面对新的数据（新结点）时，需要重新构建图，代表模型有：GCN、ChebNet；</li><li>Inductive：可以对新结点进行归纳，不需要对图进行重构，代表模型有：GraphSAGE、GAT、fastGCN；</li></ul><h4 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h4><p>​    Graph Convolutional Network（图卷积神经网络）可以说是最流行的图神经网络了。GCN发展也经过了一定的阶段，一般分为两大类：</p><ul><li>Spatial Methods：在原始的图空间上进行信息的传递，代表模型有：GraphSAGE、DGCN、SACNN；</li><li>Spectral Methods：在经过一定变换后的谱空间上进行信息的传递，代表模型有：GCN、ChebNet；</li></ul><p>这里简单介绍Spectral Methods，其基本思想是通过拉普拉斯矩阵的运算（特征分解），实现图上的卷积。常见的拉普拉斯矩阵定义如下：</p><script type="math/tex; mode=display">L^{sym} = D^{-1/2}LD^{-1/2} = I-D^{-1/2}AD^{-1/2}</script><p>接着会涉及到一些傅里叶变换和特征分解相关的内容，后续另写一文详细讲述。这里只需要知道，传统的GCN是需要对拉普拉斯矩阵进行特征分解，才能得到卷积核，这项操作的复杂度是非常高的。</p><p>GCN提出得比较早，但一直无法广泛应用的一个重要原因是它的复杂度，后续的很多改进都是针对于复杂度的改进，基本的思路是基于Chebyshev多项式作为卷积核。目前最流行的版本由Kipf在2016年提出：</p><script type="math/tex; mode=display">H^{(l+1)}= \sigma(\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}H^{(l)}W^{(l)})</script><p>其中：</p><ul><li>$H$表示Hidden layer，即每一层的值</li><li>$W$表示Weights，即当前层的参数权重</li><li>$\sigma$表示激活函数</li><li>$\tilde{A}=A+I_N$，实际上就是一个邻接矩阵A，再加上自身的连接，其中$I_N$表示单位矩阵。</li><li>$\tilde{D}_{ii}=\sum_j\tilde{A}_{ij}$</li></ul><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/image-20200512161705951.png" alt="image-20200512161705951" style="zoom:38%;"></p><p>如此看来，GCN在形式上跟普通的NN是非常相似的，只是在前面乘上了一个$\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}$ 便可实现在图上的卷积。这一版本的GCN，优点是不需要对矩阵进行分解，极大地加快了运算，但缺点也很明显，必须要把整个图放进GCN中训练，当然这也是大部分GNN所存在的问题。并且GCN是Transductive的，这也就意味着训练集和测试集必须合并在一起构建图。</p><h4 id="GAT"><a href="#GAT" class="headerlink" title="GAT"></a>GAT</h4><p>​    Graph Attention Network由Bengio团队提出，顾名思义，GAT使用了Attention机制。</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/image-20200512170653396.png" alt="image-20200512170653396" style="zoom:34%;"></p><p>​    其核心思想还是论文（Attention is all you need）中的思想，只不过Transformer计算的是各个词对当前词的贡献度，而GAT计算的是邻居结点对当前结点$h_i$的贡献度$\alpha_{ij}$。当然，也用到了多头，做多次Attention的计算，做拼接或是取平均，得到该结点的下一个状态$h’_{1}$。</p><p>​    其中最核心的公式如下：</p><script type="math/tex; mode=display">\vec{h'}_i = \sigma(\sum_{j\in N_i}\alpha_{ij}W\vec{h}_j)</script><p>当然，对于多头而言，最后还需要对$\vec{h’}$做拼接或者取平均。</p><ul><li><p>$h$：就是隐藏层的状态，特别地，$h_0=x$。</p></li><li><p>$\alpha_{ij}$：即结点$h_j$对结点$h_i$的权重，计算方式与一般的Self-Attention无异。</p><p>GAT相对于GCN，最大的好处在于它可以Inductive Learning，即面对新的结点时，可以动态地计算$\alpha$，而不必重新构建图。且从实验数据来看，GAT即使是在Transductive任务上也要优于GCN。</p></li></ul><div class="table-container"><table><thead><tr><th style="text-align:center">Methods</th><th style="text-align:center">Cora</th><th style="text-align:center">Citeseer</th><th style="text-align:center">Pubmed</th></tr></thead><tbody><tr><td style="text-align:center">GCN</td><td style="text-align:center">81.5%</td><td style="text-align:center">70.3%</td><td style="text-align:center">79.0%</td></tr><tr><td style="text-align:center">GAT</td><td style="text-align:center">83.0%</td><td style="text-align:center">72.5%</td><td style="text-align:center">79.0%</td></tr></tbody></table></div><h3 id="Aspect-Based-Sentiment-Analysis"><a href="#Aspect-Based-Sentiment-Analysis" class="headerlink" title="Aspect-Based Sentiment Analysis"></a>Aspect-Based Sentiment Analysis</h3><p>​    Aspect-Based Sentiment Analysis（ABSA），是应用十分广泛的一类任务，比如在特定领域内（如电影、商品、新闻），需要对文本进行情感分析，那么ABSA就可以有很大的发挥空间。基本上分为两类任务：分类和抽取。Classification：每个样本给定一个Aspect（可以理解为类别/方面/主题），基于这个Aspect对该样本进行情感分类；Extraction：给定一个样本，将该样本的Aspect抽取出来。</p><p>​    ABSA的核心是Aspect，那么我们必须要明确两个名词：</p><ul><li>Aspect Term：指文本中出现的目标实体，比如：”The Avengers 4 is good, but the sound effect in this cinema is terrible. “ 这个Avengers 4就是Aspect Term。一般也称为Target。</li><li>Aspect Category：指文本中的aspect类别，不一定是具体的实体，比如上面那句话中，sound effect 可以归类为服务，即归类为Aspect Category “service”。 </li></ul><p>因此，ABSA又可以细分为四类任务：</p><ul><li>Aspect Based Sentiment Classification<ul><li>Aspect Term Sentiment Classification</li><li>Aspect Category Sentiment Classification</li></ul></li><li>Aspect Extraction<ul><li>Aspect Term Extraction</li><li>Aspect Category Recognition</li></ul></li></ul><p>此外，ABSA近两年还有一些新兴的任务变种，这里我们主要关注于上面两类任务，因此此处不再赘述。</p><h3 id="GNN-for-ABSA"><a href="#GNN-for-ABSA" class="headerlink" title="GNN for ABSA"></a>GNN for ABSA</h3><p>​    最近两年，图神经网络在ABSA，尤其是ABSC任务上有着一定的热度（主要还是ABSC）。基本的思路是根据现有的Aspect，以及文本自身存在的关系（经常是构造一棵Dependency Tree），构建出图，再通过图神经网络对图进行分类。</p><p>​    其中，基本上以GCN和GAT两种模型作为基础模型，在此之上做一些改进。下面给出一些有代表性的论文。</p><ul><li>Aspect-Level Sentiment Analysis Via Convolution over Dependency Tree, Sun et al.</li><li>Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks, Zhang et al.</li><li>Selective Attention Based Graph Convolutional Networks forAspect-Level Sentiment Classification, Hou et al.</li><li>Modeling sentiment dependencies with graph convolutional networks for aspect-level sentiment classification, Zhao et al.</li><li>Syntax-Aware Aspect Level Sentiment Classification with Graph Attention Networks, Huang et al.</li><li>Exploiting Typed Syntactic Dependencies for Targeted Sentiment Classification Using Graph Attention Neural Network, Bai et al.</li></ul><h2 id="Analysis-of-model-“CDT”"><a href="#Analysis-of-model-“CDT”" class="headerlink" title="Analysis of model “CDT”"></a>Analysis of model “CDT”</h2><p>​    从时间上来说，Sun et al. 提出的CDT应该算是最早地将图神经网络应用于ABSA任务了，且其中的方法很具有代表性，为后来的同类型的研究提供了很多思路，因此我们在这里简单地对CDT进行分析。(<a href="https://www.aclweb.org/anthology/D19-1569.pdf" target="_blank" rel="noopener">paper</a>), (<a href="https://github.com/sunkaikai/CDT_ABSA" target="_blank" rel="noopener">code</a>).</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com//image-20200512171345238.png" alt="image-20200512171345238" style="zoom:36%;"></p><p>​    论文中的原图很清晰地描绘出整个模型的架构，其中有几点需要注意：</p><ul><li><p>Word Embedding 部分采用的是Glove Embedding，这部分比较简单。</p></li><li><p>接下来输入到一个BiLSTM中，这也是一个比较常规的操作。</p></li><li><p>再接下来输入到GCN中，这里就是整个模型的核心了。这里接下来详细地讲。</p></li><li><p>最后需要注意的是，GCN的输出并不完全地进入到下一层，而是只使用Aspect所对应的隐藏层表示，做了一个Average Pooling之后就是用Softmax进行多分类了。</p><p>所有的任务，想要用图神经网络，都离不开一个问题，那就是图的构建。而在CDT中，研究者们很巧妙地使用了Dependency Tree这一结构，根据论文中的描述，他们使用的是Stanford的语义分析工具，具体的例子如下图所示（论文原图）：</p></li></ul><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/image-20200512172136062.png" alt="image-20200512172136062" style="zoom:42%;"></p><p>​    接下来，问题就变成了如何将Dependency Tree转化为图（图矩阵），CDT所使用的方法也很简单，构建一个邻接矩阵，其中的结点就是一句话里的各个单词，边就是他们之间的Dependency连接。（自身也算）</p><script type="math/tex; mode=display">A_{ij} = \textit{1 IF linked OR itself, else 0.}</script><p>​    然后就是仿照GCN中的定义：</p><script type="math/tex; mode=display">H^{(l+1)}= \sigma (AH^{(l)}W^{(l)})</script><p>（原论文中对$A$进行了标准化，这里为了方便对比省略了）</p><p>这里就是与原GCN不同的地方了，原版用的是$\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}$，而这里则是更加简化，仅仅使用邻接矩阵$A$. </p><p>以下是源代码中构建图的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># tree.py 构建图</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tree_to_adj</span><span class="params">(sent_len, tree, directed=False, self_loop=True)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Convert a tree object to an (numpy) adjacency matrix.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    ret = np.zeros((sent_len, sent_len), dtype=np.float32)</span><br><span class="line"></span><br><span class="line">    queue = [tree]</span><br><span class="line">    idx = []</span><br><span class="line">    <span class="keyword">while</span> len(queue) &gt; <span class="number">0</span>:</span><br><span class="line">        t, queue = queue[<span class="number">0</span>], queue[<span class="number">1</span>:]</span><br><span class="line"></span><br><span class="line">        idx += [t.idx]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> c <span class="keyword">in</span> t.children:</span><br><span class="line">            ret[t.idx, c.idx] = <span class="number">1</span></span><br><span class="line">        queue += t.children</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> directed:</span><br><span class="line">        ret = ret + ret.T</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self_loop:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> idx:</span><br><span class="line">            ret[i, i] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ret</span><br></pre></td></tr></table></figure><p>GCN层：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># gcn.py</span></span><br><span class="line"><span class="comment"># gcn layer</span></span><br><span class="line">denom = adj.sum(<span class="number">2</span>).unsqueeze(<span class="number">2</span>) + <span class="number">1</span>    <span class="comment"># norm</span></span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> range(self.layers):</span><br><span class="line">    Ax = adj.bmm(gcn_inputs)</span><br><span class="line">    AxW = self.W[l](Ax)</span><br><span class="line">    AxW = AxW / denom</span><br><span class="line">    gAxW = F.relu(AxW)</span><br><span class="line">    gcn_inputs = self.gcn_drop(gAxW) <span class="keyword">if</span> l &lt; self.layers - <span class="number">1</span> <span class="keyword">else</span> gAxW</span><br></pre></td></tr></table></figure><p>​    可以看到，源码中的实现与论文中的表述是一致的，使用的确实就是标准化了的邻接矩阵。而事实上，当我尝试地将其改为一般的GCN的形式时（即$H^{(l+1)}= \sigma(\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}H^{(l)}W^{(l)})$），除了速度更慢了以外，效果并没有什么变化。这也是此论文有趣的地方。</p><h2 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h2><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><ul><li><strong>Rest14</strong>  是一个餐厅评论的数据集，收录在 SemEval 2014 (Pontiki  et  al.,  2014)中。</li><li><strong>Latop</strong> 是一个笔记本电脑评论的数据集，同样收录在 SemEval 2014 中。</li><li><strong>Twitter</strong> 推特的数据集（Dong et al., 2014）</li><li><strong>Rest16</strong> 同样是餐厅评论的数据集，收录在 SemEval 2016中。</li></ul><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">Positive</th><th style="text-align:center"></th><th style="text-align:center">Neutral</th><th style="text-align:center"></th><th style="text-align:center">Negative</th><th style="text-align:center"></th></tr></thead><tbody><tr><td style="text-align:center"><strong>Dataset</strong></td><td style="text-align:center">train</td><td style="text-align:center">test</td><td style="text-align:center">train</td><td style="text-align:center">test</td><td style="text-align:center">train</td><td style="text-align:center">test</td></tr><tr><td style="text-align:center">Rest14</td><td style="text-align:center">2164</td><td style="text-align:center">727</td><td style="text-align:center">637</td><td style="text-align:center">196</td><td style="text-align:center">807</td><td style="text-align:center">196</td></tr><tr><td style="text-align:center">Latop14</td><td style="text-align:center">976</td><td style="text-align:center">337</td><td style="text-align:center">455</td><td style="text-align:center">167</td><td style="text-align:center">851</td><td style="text-align:center">128</td></tr><tr><td style="text-align:center">Rest16</td><td style="text-align:center">1657</td><td style="text-align:center">611</td><td style="text-align:center">101</td><td style="text-align:center">44</td><td style="text-align:center">748</td><td style="text-align:center">204</td></tr><tr><td style="text-align:center">Twitter</td><td style="text-align:center">1507</td><td style="text-align:center">172</td><td style="text-align:center">3016</td><td style="text-align:center">336</td><td style="text-align:center">1528</td><td style="text-align:center">169</td></tr></tbody></table></div><p><strong>注：</strong>观察发现，不同论文中的数据集划分有细微的差异，但并不会对结果的对比产生大的影响。这里记录的是CDT中的划分。</p><h3 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h3><ul><li><p><strong>CDT</strong>[Sun et al.,  2019]，将Dependency Tree构造为图，应用GCN，学习aspect的表示。</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/image-20200512171345238.png" alt="image-20200512171345238" style="zoom:34%;"></p></li><li><p><strong>ASGCN</strong>[Zhang et al., 2019]，采用两个通道，一个通道计算Attention，一个通道与CDT采用相似的结构，输出Aspect对应的隐藏层状态，再用另一个通道计算好的Attention值加权。</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/image-20200512221747773.png" alt="image-20200512221747773" style="zoom: 50%;"></p></li><li><p><strong>SDGCN</strong>[Zhao et al., 2019] 与ASGCN类似，但两个通道用的都是BiLSTM作为Encoder，GCN放在最后面（Softmax之前）。此外，还用到了较多的残差连接，模型整体较复杂。</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/image-20200512224216072.png" alt="image-20200512224216072" style="zoom:38%;"></p></li><li><p><strong>SA-GCN</strong>[Hout et al, 2020]，为了缓解Dependency Tree解析错误带来的连锁反应，不再直接使用Aspect对应的隐藏层状态，而是将Attention与GCN相结合，将Top K个值作为最后输入分类器的值。</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/image-20200512223332060.png" alt="image-20200512223332060" style="zoom: 43%;"></p></li><li><p><strong>TD-GAT</strong> [Huang and Carley, 2019],  与CDT类似，使用Graph Attention Network去捕捉语义结构。</p></li><li><p><strong>R-GAT</strong> [Bai et al., 2020]，与CDT类似，使用了Relational Graph Attention Network代替GCN。</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/image-20200512223805773.png" alt="image-20200512223805773" style="zoom:44%;"></p></li></ul><h3 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h3><div class="table-container"><table><thead><tr><th>Method</th><th>Rest14</th><th></th><th>Latop</th><th></th><th>Twitter</th><th></th><th>Rest16</th><th></th></tr></thead><tbody><tr><td></td><td>ACC</td><td>F1</td><td>ACC</td><td>F1</td><td>ACC</td><td>F1</td><td>ACC</td><td>F1</td></tr><tr><td><strong>GCN</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>CDT(Sun et al., 2019)</td><td>82.3</td><td>74.02</td><td>77.19</td><td>72.99</td><td>74.66</td><td>73.66</td><td>85.58</td><td>69.93</td></tr><tr><td>ASGCN(Zhang et al., 2019)</td><td>80.86</td><td>72.19</td><td>75.55</td><td>71.05</td><td>72.15</td><td>70.40</td><td>88.99</td><td>67.48</td></tr><tr><td>SDGCN*(Zhao et al., 2019)</td><td>83.57</td><td>76.47</td><td>81.35</td><td>78.34</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>SAGCN* (Hou et al., 2020)</td><td>86.9</td><td>80.1</td><td>82.3</td><td>80.0</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td><strong>GAT</strong></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>TD-GAT*(Huang et al., 2019)</td><td>83.0</td><td>-</td><td>80.1</td><td>-</td><td>-</td><td>-</td><td>-</td><td>-</td></tr><tr><td>R-GAT*(Bai et al., 2020)</td><td>86.59</td><td>80.51</td><td>81.25</td><td>78.55</td><td>75.84</td><td>74.65</td><td>-</td><td>-</td></tr></tbody></table></div><p>默认使用Glove Embedding，*表示使用BERT Embedding。以上数据均记录自所引用论文中的最好结果。</p><h2 id="Potential-Improvement"><a href="#Potential-Improvement" class="headerlink" title="Potential Improvement"></a>Potential Improvement</h2><p>​    目前基本上所有的图神经网络模型在各个领域的应用，都离不开的两个问题：</p><ul><li>图的构建</li><li>图神经网络结构</li></ul><p>​    回到ABSC任务，先说说第一个问题，目前基本上所有的模型所使用的图都是基于Dependency Tree构建的邻接矩阵。Dependency Tree属于句法分析领域，这一领域并不热门，但还是有一些先进的研究。再者，所构建的邻接矩阵是一个只有0和1的，往往还是稀疏的矩阵，这样显然会浪费掉很多信息。所以，一个改进的思路是，能否构建出一个信息更丰富、更稠密的图矩阵，让语料的先验知识得到更好的抽取。</p><p>​    再来看看第二个问题，目前的研究都局限于使用GCN和GAT这两种模型，以及在它们之上做的各种“魔改”，比如加BERT、加Attention、做各种拼接和残差连接等等。那我们能否尝试一些别的GNN呢？比如Recurrent型的Graph Recurrent Neural Netwok 以及带记忆力机制的 Graph Memory Neural Network。</p><p>​    再者，目前的研究主要集中在分类任务，能否在Aspect Extract任务等任务上应用GNN，也是一个研究的方向。</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>[1] Zhou et al., 2018. Graph Neural Networks: A Review of Methods and Applications. arXiv:1812.08434.</p><p>[2] Liang Yao, Chengsheng Mao, and Yuan Luo, ‘Graph convolutional net-works for text classification’, inProceedings of the AAAI Conference onArtificial Intelligence, volume 33, pp. 7370–7377, (2019).</p><p>[3] Nazir et al., 2020. Issues and Challenges of Aspect-based Sentiment Analysis: A Comprehensive Survey. IEEE-TAC-20.</p><p>[4] Sun et al., 2019. Aspect-Level Sentiment Analysis Via Convolution over Dependency Tree. EMNLP-19.</p><p>[5] Zhang et al., 2019. Aspect-based Sentiment Classification with Aspect-specific Graph Convolutional Networks. EMNLP-19.</p><p>[6] Hou et al., 2020. Selective Attention Based Graph Convolutional Networks forAspect-Level Sentiment Classification. arXiv-20.</p><p>[7] Zhao et al., 2019. Modeling sentiment dependencies with graph convolutional networks for aspect-level sentiment classification. arXiv-19.</p><p>[8] Huang et al., 2019. Syntax-Aware Aspect Level Sentiment Classification with Graph Attention Networks. EMNLP-19. </p><p>[9] Bai et al., 2020. Exploiting Typed Syntactic Dependencies for Targeted Sentiment Classification Using Graph Attention Neural Network. arXiv-20.</p><p>[10] Ye et al., 2020. Document and Word Representations Generated by Graph Convolutional Network and BERT for Short Text Classification. ECAI-20.</p><p>[11] Li et al, 2019. Coherent Comment Generation for Chinese Articles with a Graph-to-Sequence Model.  ACL-19.</p><p>[12] Thomas N. Kipf, Max Welling, 2016. Semi-Supervised Classification with Graph Convolutional Networks, ICLR-17.</p><p>[13] Jiezhou et al, 2020. Graph Neural Networks: A Review of Methods and Applications, arXiv-20.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;A-brief-survey-of-papers-on-GNN-for-Aspect-Based-Sentiment-Analysis&quot;&gt;&lt;a href=&quot;#A-brief-survey-of-papers-on-GNN-for-Aspect-Based-Sentiment-Analysis&quot; class=&quot;headerlink&quot; title=&quot;A brief survey of papers on GNN for Aspect-Based Sentiment Analysis&quot;&gt;&lt;/a&gt;A brief survey of papers on GNN for Aspect-Based Sentiment Analysis&lt;/h1&gt;&lt;h2 id=&quot;Abstract&quot;&gt;&lt;a href=&quot;#Abstract&quot; class=&quot;headerlink&quot; title=&quot;Abstract&quot;&gt;&lt;/a&gt;Abstract&lt;/h2&gt;&lt;p&gt;​    In previous years, there are lots of research about Graph Neural Network(GNN), a modified neural network for graph data. And some researchers focus on using GNN for the NLP tasks, such as text classification(TextGCN, Yao et al.) and comment generation(graph2seq, Li et al.) . Sun et al. had purposed a new model called CDT, which is an application of Aspect-Based Sentiment Classification, the sub-task of Aspect-Based Sentiment Analysis(ABSA). Inspired by Sun et al., some researchers considered to using the GNN for ABSA. We focus on reviewing their rearches, and discussing about the future work in those aspects.&lt;/p&gt;
&lt;p&gt;​    最近关于图神经网络（Graph Neural Network）的研究非常火热，这是一种适应于图数据的神经网络，并且最近在NLP界也开始大展拳脚，比如文本分类任务（TextGCN，Yao et al.）和评论生成任务（graph2seq，Li et al.）。Sun et al. 提出了一个新的模型——CDT, 应用于Aspect-Based Sentiment Classification 任务，属于ABSA下的一个分支。受Sun et al.的研究的启发，一些研究人员开始致力于将GNN应用到ABSA任务上。我们希望回顾这些研究，并讨论一下这些研究未来的发展。&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="GNN" scheme="http://yoursite.com/tags/GNN/"/>
    
      <category term="ABSA" scheme="http://yoursite.com/tags/ABSA/"/>
    
  </entry>
  
  <entry>
    <title>关于求平方根——C语言实现</title>
    <link href="http://yoursite.com/2020/05/09/ds-algorithm/sqrt/"/>
    <id>http://yoursite.com/2020/05/09/ds-algorithm/sqrt/</id>
    <published>2020-05-09T02:03:51.307Z</published>
    <updated>2020-05-13T04:40:33.826Z</updated>
    
    <content type="html"><![CDATA[<h1 id="关于求平方根——C语言实现"><a href="#关于求平方根——C语言实现" class="headerlink" title="关于求平方根——C语言实现"></a>关于求平方根——C语言实现</h1><p>​    求平方根一直是应用广泛、使用频繁的一个数学函数，比如很多距离的计算就需要用到平方根的计算，如何高效地求解平方根，一直以来都是一个有趣的问题。这里我们假设一个问题，要求不使用自带的求平方根函数，求解给定整数的平方根，返回一个整数。一般的思路有如下几种：</p><ul><li>通过其它的数学函数代替平方根函数得到精确结果，取整数部分作为答案；</li><li>通过数学方法得到近似结果，直接作为答案。</li></ul><a id="more"></a><h2 id="袖珍计算器算法"><a href="#袖珍计算器算法" class="headerlink" title="袖珍计算器算法"></a>袖珍计算器算法</h2><p>​    <strong>袖珍计算器算法</strong>是一种用指数函数 $\exp$ 和对数函数 $\ln$ 代替平方根函数的方法。我们通过有限的可以使用的数学函数，得到我们想要计算的结果。基本原理如下：</p><script type="math/tex; mode=display">\sqrt{x} = x^{1/2} = (e^{lnx})^{1/2} = e^{0.5\ln(x)}</script><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">mySqrt</span><span class="params">(<span class="keyword">int</span> x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(x==<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> ans = <span class="built_in">exp</span>(<span class="number">0.5</span> * <span class="built_in">log</span>(x));</span><br><span class="line">    <span class="keyword">return</span> ((<span class="keyword">long</span>)(ans + <span class="number">1</span>) * (ans + <span class="number">1</span>) &lt;= x ? ans + <span class="number">1</span> : ans);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>注意：</strong> 由于计算机无法存储浮点数的精确值，而指数函数和对数函数的参数和返回值均为浮点数，因此运算过程中会存在误差。例如当 x = 2147395600时，$e^{0.5\ln(x)}$的计算结果与正确值 4634046340 相差 $10^{-11}$ ，这样在对结果取整数部分时，会得到 46339这个错误的结果。</p><p>因此在得到结果的整数部分ans后，我们应当找出ans与ans+1中哪一个是真正的答案。</p><h2 id="二分法"><a href="#二分法" class="headerlink" title="二分法"></a>二分法</h2><p>​    基本的思路就是二分查找，找到一个数ans，满足它的平方等于x，由于题目要求返回整数部分，因此只需要满足$ans^2&lt;= x$且$(ans+1)^2&gt;x$即可。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">mySqrt</span><span class="params">(<span class="keyword">int</span> x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> l=<span class="number">0</span>, r=x, ans=<span class="number">-1</span>;</span><br><span class="line">    <span class="keyword">while</span>(l&lt;=r)&#123;</span><br><span class="line">        <span class="keyword">int</span> mid = (l+r)/<span class="number">2</span> ;</span><br><span class="line">        <span class="keyword">if</span>((<span class="keyword">long</span> )mid*mid &lt;= x)&#123;</span><br><span class="line">            ans = mid;</span><br><span class="line">            l = mid+<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            r = mid<span class="number">-1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h2><p>​        牛顿法是一种快速求解函数零点的方法，为了叙述方便，假设$C$为需要求解平方根的值，那么有</p><script type="math/tex; mode=display">y = x^2 - C</script><p>我们任取一个$x_0$作为初始值，在每一步的迭代中，在这一个点上作一条斜率该点导数$f^{-1}(x_i)$的直线，与x轴的交点即为下一次迭代的x值。经过多次迭代之后，我们就可以得到一个距离零点非常接近的交点。如下图所示。</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/69_fig1.png" style="zoom: 50%;"></p><p><strong>算法</strong>：</p><p>选择$x_0=C$作为初始值。</p><p>在每一步的迭代中，通过当前的点$(x_i,f(x_i))$，作一条斜率为$f^{-1}(x_i)=2x_i$的直线，直线的方程为：</p><script type="math/tex; mode=display">\begin{equation}\begin{split}y_i &= 2x_i(x-x_i)+x^2_i -C \\&= 2x_ix-(x^2_i+C)\end{split}\end{equation}</script><p>与横轴的交点为方程$2x_ix-(x^2_i+C)=0$的解，即为新的迭代结果$x_{i+1}$：</p><script type="math/tex; mode=display">x_{x+1} = \frac{1}{2}(x_i+\frac{C}{x_i})</script><p>在经过$k$次迭代之后，$x_k$的值与真实的零点$\sqrt{C}$足够接近，即可作为答案。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">mySqrt</span><span class="params">(<span class="keyword">int</span> x)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (x == <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">double</span> x0 = x;</span><br><span class="line">    <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="keyword">double</span> xi = <span class="number">0.5</span> * (x0 + x / x0);</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">fabs</span>(x0 - xi) &lt; <span class="number">1e-7</span>) &#123;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        x0 = xi;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> (<span class="keyword">int</span>)x0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="平方根倒数——Carmack算法-平方根倒数速算法"><a href="#平方根倒数——Carmack算法-平方根倒数速算法" class="headerlink" title="平方根倒数——Carmack算法(平方根倒数速算法)"></a>平方根倒数——Carmack算法(平方根倒数速算法)</h2><blockquote><p>浮点数的平方根倒数常用于计算<a href="https://zh.wikipedia.org/wiki/单位向量" target="_blank" rel="noopener">正规化向量</a><a href="https://zh.wikipedia.org/wiki/平方根倒数速算法#cite_note-FOOTNOTEBlinn2003130-2" target="_blank" rel="noopener">[文 1]</a>。3D图形程序需要使用正规化向量来实现光照和<a href="https://zh.wikipedia.org/wiki/余弦辐射体" target="_blank" rel="noopener">投影</a>效果，因此每秒都需做上百万次平方根倒数运算，而在处理<a href="https://zh.wikipedia.org/wiki/T%26L" target="_blank" rel="noopener">坐标转换与光源</a>的专用硬件设备出现前，这些计算都由软件完成，计算速度亦相当之慢；在1990年代这段代码开发出来之时，多数浮点数操作的速度更是远远滞后于整数操作<a href="https://zh.wikipedia.org/wiki/平方根倒数速算法#cite_note-Beyond3D-1" target="_blank" rel="noopener">[1]</a>，因而针对正规化向量算法的优化就显得尤为重要。——Wikipedia</p></blockquote><p>相较于求平方根，求平方根的倒数的应用可能会更加广泛，尤其是在对向量进行正规化时，就需要求解平方根倒数。一般而言，上面的几个方法都是可以用来求平方根倒数的，但还有一种更神奇的方法——Carmack算法。</p><p><strong>注：</strong>真实的起源已经无从考究，但最广为人知的是工程师约翰·卡马克在《<a href="https://zh.wikipedia.org/wiki/雷神之锤III竞技场" target="_blank" rel="noopener">雷神之锤III竞技场</a>》的源代码中的应用。不过后来据说他本人予以否认，但源代码和这个叫法已经广为流传。</p><p>源代码也很简单，如下:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">Q_rsqrt</span><span class="params">( <span class="keyword">float</span> number )</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">long</span> i;</span><br><span class="line"><span class="keyword">float</span> x2, y;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">float</span> threehalfs = <span class="number">1.5F</span>;</span><br><span class="line"></span><br><span class="line">x2 = number * <span class="number">0.5F</span>;</span><br><span class="line">y  = number;</span><br><span class="line">i  = * ( <span class="keyword">long</span> * ) &amp;y;<span class="comment">// evil floating point bit level hacking</span></span><br><span class="line">i  = <span class="number">0x5f3759df</span> - ( i &gt;&gt; <span class="number">1</span> );               <span class="comment">// what the fuck?</span></span><br><span class="line">y  = * ( <span class="keyword">float</span> * ) &amp;i;</span><br><span class="line">y  = y * ( threehalfs - ( x2 * y * y ) );   <span class="comment">// 1st iteration</span></span><br><span class="line"><span class="comment">//y  = y * ( threehalfs - ( x2 * y * y ) );   // 2nd iteration, this can be removed</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> Q3_VM</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> __linux__</span></span><br><span class="line">assert( !isnan(y) ); <span class="comment">// bk010122 - FPE?</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"><span class="keyword">return</span> y;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>博主<a href="http://aicdg.com/oldblog/" target="_blank" rel="noopener">菊长的菊花田</a>给出的简化版如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">InvSqrt</span> <span class="params">(<span class="keyword">float</span> x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> xhalf = <span class="number">0.5f</span>*x;</span><br><span class="line">    <span class="keyword">int</span> i = *(<span class="keyword">int</span>*)&amp;x;</span><br><span class="line">    i = <span class="number">0x5f3759df</span> - (i &gt;&gt; <span class="number">1</span>); <span class="comment">// 计算第一个近似根</span></span><br><span class="line">    x = *(<span class="keyword">float</span>*)&amp;i;</span><br><span class="line">    x = x*(<span class="number">1.5f</span> - xhalf*x*x); <span class="comment">// 牛顿迭代法</span></span><br><span class="line">    <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    Carmack算法的本质其实是还是上面提到的牛顿迭代法，但最核心的部分在那行0x5f3759df。众所周知，牛顿迭代法需要设置一个初始值，然后不断的迭代接近真实值，而神奇的地方在于这行代码在第一次迭代就已经非常接近于这个值了。</p><h2 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h2><p>【1】<a href="https://leetcode-cn.com/problems/sqrtx/solution/x-de-ping-fang-gen-by-leetcode-solution/" target="_blank" rel="noopener">求平方根——LeetCode 官方题解</a></p><p>【2】<a href="[https://zh.wikipedia.org/wiki/%E5%B9%B3%E6%96%B9%E6%A0%B9%E5%80%92%E6%95%B0%E9%80%9F%E7%AE%97%E6%B3%95](https://zh.wikipedia.org/wiki/平方根倒数速算法">平方根倒数速算法，维基百科</a>)</p><p>【3】<a href="http://aicdg.com/oldblog/math/2017/03/13/carmark-qsqrt.html" target="_blank" rel="noopener">卡马克快速平方根,菊长的菊花田</a></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;关于求平方根——C语言实现&quot;&gt;&lt;a href=&quot;#关于求平方根——C语言实现&quot; class=&quot;headerlink&quot; title=&quot;关于求平方根——C语言实现&quot;&gt;&lt;/a&gt;关于求平方根——C语言实现&lt;/h1&gt;&lt;p&gt;​    求平方根一直是应用广泛、使用频繁的一个数学函数，比如很多距离的计算就需要用到平方根的计算，如何高效地求解平方根，一直以来都是一个有趣的问题。这里我们假设一个问题，要求不使用自带的求平方根函数，求解给定整数的平方根，返回一个整数。一般的思路有如下几种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过其它的数学函数代替平方根函数得到精确结果，取整数部分作为答案；&lt;/li&gt;
&lt;li&gt;通过数学方法得到近似结果，直接作为答案。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="数据结构与算法" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="算法" scheme="http://yoursite.com/tags/%E7%AE%97%E6%B3%95/"/>
    
      <category term="C语言" scheme="http://yoursite.com/tags/C%E8%AF%AD%E8%A8%80/"/>
    
  </entry>
  
  <entry>
    <title>关于链表——C语言实现</title>
    <link href="http://yoursite.com/2020/05/03/ds-algorithm/LikedList/"/>
    <id>http://yoursite.com/2020/05/03/ds-algorithm/LikedList/</id>
    <published>2020-05-03T11:31:44.414Z</published>
    <updated>2020-05-10T12:45:18.549Z</updated>
    
    <content type="html"><![CDATA[<h1 id="关于链表——C语言实现"><a href="#关于链表——C语言实现" class="headerlink" title="关于链表——C语言实现"></a>关于链表——C语言实现</h1><p>​    链表（LinkedList），是一种链式存储的数据结构，类似于数组，但在内存中非连续的存放，通过链表指针连接。好处是插入删除等操作时，相对于数组复杂度更低，缺点是查找起来不如数组方便。</p><a id="more"></a><h2 id="链表基础"><a href="#链表基础" class="headerlink" title="链表基础"></a>链表基础</h2><h3 id="链表的建立"><a href="#链表的建立" class="headerlink" title="链表的建立"></a>链表的建立</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 定义链表结构体</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">Node</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> data;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">Node</span> *<span class="title">next</span>;</span></span><br><span class="line">&#125;ListNode;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 建立链表</span></span><br><span class="line"><span class="function">ListNode *<span class="title">buildList</span><span class="params">(<span class="keyword">int</span> n, <span class="keyword">int</span> num[])</span></span>&#123;</span><br><span class="line">    ListNode *head,*end;</span><br><span class="line">    head = (ListNode*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(ListNode));</span><br><span class="line">    end = head;         </span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">        ListNode node = (ListNode*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(ListNode));</span><br><span class="line">        node-&gt;data = num[i];</span><br><span class="line">        end-&gt;next = node;</span><br><span class="line">        end = node;</span><br><span class="line">    &#125;</span><br><span class="line">    end-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">return</span> head;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="遍历链表"><a href="#遍历链表" class="headerlink" title="遍历链表"></a>遍历链表</h3><p>​    链表的遍历非常简单，不断地访问下一个结点即可，但它也是绝大部分链表算法的基础。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//遍历链表</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">ListIter</span><span class="params">(ListNode* p)</span></span>&#123;</span><br><span class="line">    ListNode* h;</span><br><span class="line">    h = p-&gt;next;</span><br><span class="line">    <span class="keyword">while</span>(h != <span class="literal">NULL</span>)&#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">"%d-&gt;"</span>, h-&gt;data);</span><br><span class="line">        h = h-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"\n"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">ListNode *<span class="title">insertList</span><span class="params">(ListNode* p, <span class="keyword">int</span> data, <span class="keyword">int</span> index)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">    ListNode *h, *t;</span><br><span class="line">    t-&gt;data = data;</span><br><span class="line">    t-&gt;next = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">if</span>(p == <span class="literal">NULL</span>)&#123;</span><br><span class="line">        <span class="keyword">return</span> t;</span><br><span class="line">    &#125;</span><br><span class="line">    h = p;</span><br><span class="line">    <span class="keyword">while</span>(h)&#123;</span><br><span class="line">        <span class="keyword">if</span> (i == index)&#123;</span><br><span class="line">            t-&gt;next = h-&gt;next;</span><br><span class="line">            h-&gt;next = t;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        h = h-&gt;next;</span><br><span class="line">        i+=<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> p;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h3><p>​    按值删除（删除第一个对应的值的结点，适当修改可以删除全部）</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">struct ListNode* <span class="title">deleteNode</span><span class="params">(struct ListNode* head, <span class="keyword">int</span> val)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(head==<span class="literal">NULL</span>)<span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">if</span>(head-&gt;val==val)<span class="keyword">return</span> head-&gt;next;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span> *<span class="title">p</span>=<span class="title">head</span>-&gt;<span class="title">next</span>, *<span class="title">pr</span>=<span class="title">head</span>;</span></span><br><span class="line">    <span class="keyword">while</span>(p)&#123;</span><br><span class="line">        <span class="keyword">if</span>(p-&gt;val==val)&#123;</span><br><span class="line">            pr-&gt;next=p-&gt;next;</span><br><span class="line">            <span class="keyword">return</span> head;</span><br><span class="line">        &#125;</span><br><span class="line">        pr = p;</span><br><span class="line">        p=p-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> head;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​    按下标删除</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">struct ListNode* <span class="title">deleteNode</span><span class="params">(struct ListNode* head, <span class="keyword">int</span> index)</span></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(head==<span class="literal">NULL</span>)<span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">if</span>(head-&gt;val==val)<span class="keyword">return</span> head-&gt;next;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span> *<span class="title">p</span>=<span class="title">head</span>-&gt;<span class="title">next</span>, *<span class="title">pr</span>=<span class="title">head</span>;</span></span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(p)&#123;</span><br><span class="line">        <span class="keyword">if</span>(i == index)&#123;</span><br><span class="line">            pr-&gt;next=p-&gt;next;</span><br><span class="line">            <span class="keyword">return</span> head;</span><br><span class="line">        &#125;</span><br><span class="line">        pr = p;</span><br><span class="line">        p=p-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> head;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="经典题目"><a href="#经典题目" class="headerlink" title="经典题目"></a>经典题目</h2><h3 id="移除链表中给定值的所有结点"><a href="#移除链表中给定值的所有结点" class="headerlink" title="移除链表中给定值的所有结点"></a>移除链表中给定值的所有结点</h3><p>​    跟普通的移除结点类似，但不能马上返回，且要注意删除的一个或多个节点位于链表的头部时的情况。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">struct ListNode* <span class="title">removeElements</span><span class="params">(struct ListNode* head, <span class="keyword">int</span> val)</span></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span> *<span class="title">h</span> = <span class="title">head</span>;</span></span><br><span class="line">    <span class="keyword">while</span> (head &amp;&amp; head-&gt;val == val) &#123;</span><br><span class="line">        head = head-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span>(h &amp;&amp; h-&gt;next)&#123;</span><br><span class="line">        <span class="keyword">if</span> (h-&gt;next-&gt;val == val)&#123;</span><br><span class="line">            <span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span> *<span class="title">t</span> = <span class="title">h</span>-&gt;<span class="title">next</span>;</span></span><br><span class="line">            h-&gt;next = h-&gt;next-&gt;next;</span><br><span class="line">            <span class="built_in">free</span>(t);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            h = h-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> head;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="反转链表"><a href="#反转链表" class="headerlink" title="反转链表"></a>反转链表</h3><p>​    反转链表的基本思路就是，用两个指针分别指向当前结点和上一个结点，再通过改变指向来使链表反转，同时需要多一个指针来保存状态。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">struct ListNode* <span class="title">reverseList</span><span class="params">(struct ListNode* head)</span></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span> *<span class="title">p</span>, *<span class="title">tmp</span>, *<span class="title">pr</span>;</span></span><br><span class="line">    p = head;</span><br><span class="line">    pr = <span class="literal">NULL</span>;</span><br><span class="line">    <span class="keyword">while</span>(p != <span class="literal">NULL</span>)&#123;</span><br><span class="line">        tmp = p-&gt;next;</span><br><span class="line">        p-&gt;next = pr;</span><br><span class="line">        pr = p;</span><br><span class="line">        p = tmp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> pr;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="回文链表"><a href="#回文链表" class="headerlink" title="回文链表"></a>回文链表</h3><p>​    这道题的核心是：反转链表、快慢指针。基本思路是，用一个快指针每次走两步，慢指针每次走一步，那么快指针到末尾时，慢指针正好到一半。这个时候，只需要将慢指针的那一半反转，再和前面的一半做对比，即可知道改链表是否是回文链表。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">struct ListNode *<span class="title">isPalindrome</span><span class="params">(struct ListNode* head)</span></span>&#123;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span> *<span class="title">slow</span>=<span class="title">head</span>, *<span class="title">fast</span>=<span class="title">head</span>;</span></span><br><span class="line">        <span class="comment">//链表调用next之前一定确保其不为null</span></span><br><span class="line">        <span class="keyword">while</span>(fast &amp;&amp; fast-&gt;next)&#123;</span><br><span class="line">            slow=slow-&gt;next;</span><br><span class="line">            fast=fast-&gt;next-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//slow就是指向链表的中间</span></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span> *<span class="title">pre</span>=<span class="title">NULL</span>;</span></span><br><span class="line">    <span class="comment">//反转链表</span></span><br><span class="line">        <span class="keyword">while</span>(slow)&#123;</span><br><span class="line">            <span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span> *<span class="title">next</span>=<span class="title">slow</span>-&gt;<span class="title">next</span>;</span></span><br><span class="line">            slow-&gt;next=pre;</span><br><span class="line">            pre=slow;</span><br><span class="line">            slow=next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">//结束之后，pre指向翻转链表的第一个节点。</span></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span> *<span class="title">forward</span>=<span class="title">head</span>;</span></span><br><span class="line">        <span class="keyword">while</span>(pre)&#123;</span><br><span class="line">            <span class="keyword">if</span>(pre-&gt;val!=forward-&gt;val) <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">            pre=pre-&gt;next;</span><br><span class="line">            forward=forward-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="链表倒数第K个结点"><a href="#链表倒数第K个结点" class="headerlink" title="链表倒数第K个结点"></a>链表倒数第K个结点</h3><p>​    同样是使用快慢指针，只要先让快指针走K步，再让快慢指针同时启动，那么快指针到尾端时，慢指针就是倒数第K个结点。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">struct ListNode* <span class="title">getKthFromEnd</span><span class="params">(struct ListNode* head, <span class="keyword">int</span> k)</span></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span> *<span class="title">slow</span>=<span class="title">head</span>;</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt;k;i++)&#123;</span><br><span class="line">        head = head-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span>(head)&#123;</span><br><span class="line">        slow = slow-&gt;next;</span><br><span class="line">        head = head-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> slow;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="判断链表是否有环"><a href="#判断链表是否有环" class="headerlink" title="判断链表是否有环"></a>判断链表是否有环</h3><p>​    快慢指针，快指针走2步，慢指针走1步，只要有环，那么他们一定会相遇。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">hasCycle</span><span class="params">(struct ListNode *head)</span> </span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span> *<span class="title">p1</span>=<span class="title">head</span>, *<span class="title">p2</span>=<span class="title">head</span>;</span></span><br><span class="line">    <span class="keyword">while</span>(p1 &amp;&amp; p2 &amp;&amp; p2-&gt;next)&#123;</span><br><span class="line">        p1 = p1-&gt;next;</span><br><span class="line">        p2 = p2-&gt;next-&gt;next;</span><br><span class="line">        <span class="keyword">if</span> (p1==p2)&#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="判断两个链表是否相交"><a href="#判断两个链表是否相交" class="headerlink" title="判断两个链表是否相交"></a>判断两个链表是否相交</h3><p>​    使用两个指针，分别走完各自的链表以后，再从对方的头结点开始走，如果相交，那么他们必定会相遇。简单的证明如下，假设两个链表ACBC相交于点D，那么有AD+DC+BD=BD+DC+AD，即他们走过的路程相等，又因为步长相等，所以他们一定会在D点相遇。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">struct ListNode *<span class="title">getIntersectionNode</span><span class="params">(struct ListNode *headA, struct ListNode *headB)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(headA == <span class="literal">NULL</span>|| headB == <span class="literal">NULL</span>) <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span> *<span class="title">t1</span>=<span class="title">headA</span>, *<span class="title">t2</span>=<span class="title">headB</span>;</span></span><br><span class="line">    <span class="keyword">while</span>(t1 != t2)&#123;</span><br><span class="line">        <span class="keyword">if</span>(!t1)</span><br><span class="line">            t1 = headB;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            t1 = t1-&gt;next;</span><br><span class="line">        <span class="keyword">if</span>(!t2)</span><br><span class="line">            t2 = headA;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            t2 = t2-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> t1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="合并两个有序链表"><a href="#合并两个有序链表" class="headerlink" title="合并两个有序链表"></a>合并两个有序链表</h3><p>​    实际上就是归并排序，跟数组类似，我们只需要让两个指针，根据结点值的大小，交替地往后运动。使用一个伪头部记录结果，在链表移动的时候将该结点记录下来。如果两个链表长度不等，会有一个多出来尾端，最后再把多出来的尾部拼到结果后面即可。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">struct ListNode* <span class="title">mergeTwoLists</span><span class="params">(struct ListNode* l1, struct ListNode* l2)</span></span>&#123;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ListNode</span> *<span class="title">dum</span>, *<span class="title">h</span>;</span></span><br><span class="line">    dum = (struct ListNode*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(struct ListNode));</span><br><span class="line">    h = dum;</span><br><span class="line">    <span class="keyword">while</span>(l1 &amp;&amp; l2)&#123;</span><br><span class="line">        <span class="keyword">if</span>(l1-&gt;val &lt; l2-&gt;val)&#123;</span><br><span class="line">            h-&gt;next = l1;</span><br><span class="line">            l1 = l1-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span>&#123;</span><br><span class="line">            h-&gt;next = l2;</span><br><span class="line">            l2 = l2-&gt;next;</span><br><span class="line">        &#125;</span><br><span class="line">        h = h-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    h-&gt;next = l1?l1:l2;</span><br><span class="line">    <span class="keyword">return</span> dum-&gt;next;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="二进制链表转整数"><a href="#二进制链表转整数" class="headerlink" title="二进制链表转整数"></a>二进制链表转整数</h3><p>​    实际上是一道进制转换题，思路也非常简单，运用反向运算操作，我们在获得二进制的时候是除于2取余数，要计算被除数则是要商乘于2加余数。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">getDecimalValue</span><span class="params">(struct ListNode* head)</span></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> ret = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span>(head)&#123;</span><br><span class="line">        ret = ret*<span class="number">2</span> + head-&gt;val;</span><br><span class="line">        head = head-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ret;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;关于链表——C语言实现&quot;&gt;&lt;a href=&quot;#关于链表——C语言实现&quot; class=&quot;headerlink&quot; title=&quot;关于链表——C语言实现&quot;&gt;&lt;/a&gt;关于链表——C语言实现&lt;/h1&gt;&lt;p&gt;​    链表（LinkedList），是一种链式存储的数据结构，类似于数组，但在内存中非连续的存放，通过链表指针连接。好处是插入删除等操作时，相对于数组复杂度更低，缺点是查找起来不如数组方便。&lt;/p&gt;
    
    </summary>
    
    
      <category term="数据结构与算法" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="链表" scheme="http://yoursite.com/tags/%E9%93%BE%E8%A1%A8/"/>
    
      <category term="数据结构与算法" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>Latex 笔记</title>
    <link href="http://yoursite.com/2020/04/24/Latex-Note/"/>
    <id>http://yoursite.com/2020/04/24/Latex-Note/</id>
    <published>2020-04-24T11:59:11.034Z</published>
    <updated>2020-04-24T11:51:58.089Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Latex-Note"><a href="#Latex-Note" class="headerlink" title="Latex Note"></a>Latex Note</h1><p>​    一些简单的latex笔记，发现记录一下还是很有必要的，以免写论文的时候又忘了，到处去找，费时费神。</p><a id="more"></a><h2 id="Font"><a href="#Font" class="headerlink" title="Font"></a>Font</h2><h3 id="字体大小"><a href="#字体大小" class="headerlink" title="字体大小"></a>字体大小</h3><p>全局字体大小</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\documentclass&#123;article&#125;[12pt]</span><br></pre></td></tr></table></figure><p>局部字体大小</p><blockquote><p>\tiny<br>\scriptsize<br>\footnotesize<br>\small<br>\normalsize<br>\large<br>\Large<br>\LARGE<br>\huge<br>\Huge</p></blockquote><p>加粗、斜体、下划线</p><blockquote><p>显示直立文本： \textup{文本}</p><p>意大利斜体： \textit{文本}</p><p>slanted斜体： \textsl{文本}</p><p>显示小体大写文本： \textsc{文本}</p><p>中等权重： \textmd{文本}</p><p>加粗命令： \textbf{文本}</p><p>默认值： \textnormal{文本}</p><p>斜体字：\textit{italic}，或者 \emph{italic}</p><p>细体字：\textlf{light font}</p><p>使用等宽字体：\texttt{code}</p><p>使用无衬线字体：\textsf{sans-serif}</p><p>所有字母大写：\uppercase{CAPITALS}</p><p>所有字母大写，但小写字母比较小：\textsc{Small Capitals}</p><p>下划线：\underline{text}</p></blockquote><h2 id="Color"><a href="#Color" class="headerlink" title="Color"></a>Color</h2><p>导入包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\usepackage[table, dvipsnames]&#123;xcolor&#125;</span><br></pre></td></tr></table></figure><p>文本颜色</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\textcolor&#123;color&#125;&#123;text&#125;</span><br></pre></td></tr></table></figure><p>表格填充</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\rowcolor&#123;color&#125;</span><br></pre></td></tr></table></figure><p><img src="C:\Users\uni_c\AppData\Roaming\Typora\typora-user-images\image-20200212143319726.png" alt="image-20200212143319726"></p><p>自定义颜色</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\definecolor&#123;rgb&#125;&#123;r, g, b&#125;</span><br></pre></td></tr></table></figure><p><a href="http://latexcolor.com/" target="_blank" rel="noopener">http://latexcolor.com/</a></p><h2 id="Page"><a href="#Page" class="headerlink" title="Page"></a>Page</h2><p>导入包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">\usepackage&#123;fancyhdr&#125;</span><br><span class="line">\usepackage&#123;lastpage&#125;</span><br></pre></td></tr></table></figure><p>页面格式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">% 页眉页脚</span><br><span class="line">\pagestyle&#123;fancy&#125;</span><br><span class="line"></span><br><span class="line">% 无页眉页脚等</span><br><span class="line">\pagestyle&#123;empty&#125;</span><br></pre></td></tr></table></figure><p>页眉、页脚</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">% 页眉左上角</span><br><span class="line">\lhead&#123;left head note&#125;</span><br><span class="line"></span><br><span class="line">% 页眉右上角</span><br><span class="line">\rhead&#123;Page \thepage\ of\ \pageref&#123;LastPage&#125;&#125;</span><br><span class="line"></span><br><span class="line">% 页脚中部</span><br><span class="line">\cnote&#123;&#125;</span><br></pre></td></tr></table></figure><h2 id="Table-Of-Content"><a href="#Table-Of-Content" class="headerlink" title="Table Of Content"></a>Table Of Content</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\setcounter&#123;tocdepth&#125;&#123;3&#125; % the depth of toc</span><br><span class="line">% 标题居中</span><br><span class="line">\begin&#123;center&#125;</span><br><span class="line">    \tableofcontents</span><br><span class="line">\end&#123;center&#125;</span><br></pre></td></tr></table></figure><h2 id="Table"><a href="#Table" class="headerlink" title="Table"></a>Table</h2><p>导入包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">\usepackage&#123;textcomp,booktabs&#125;</span><br><span class="line">\usepackage[usenames,dvipsnames]&#123;color&#125;</span><br><span class="line">\usepackage&#123;colortbl&#125;</span><br><span class="line">\definecolor&#123;mygray&#125;&#123;gray&#125;&#123;.9&#125;</span><br></pre></td></tr></table></figure><p>三线表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">% 位置参数h:here,t:top,b:bottom,p:</span><br><span class="line">\begin&#123;table*&#125;[htp]</span><br><span class="line">\centering</span><br><span class="line">    \begin&#123;tabular&#125;&#123;cc&#125;</span><br><span class="line"></span><br><span class="line">    \toprule[1.1pt] %表头直线</span><br><span class="line">    \bf&#123;Notation&#125; &amp; \bf&#123;Specification&#125;\\</span><br><span class="line">    \midrule[1.1pt] </span><br><span class="line"></span><br><span class="line">    ...&amp; ...\\</span><br><span class="line">    \rowcolor&#123;grey&#125;&#123;0.9&#125; % 填充颜色</span><br><span class="line">    ...&amp; ...\\</span><br><span class="line">    ...&amp; ...\\</span><br><span class="line"></span><br><span class="line">    \bottomrule[1.1pt] %表底直线</span><br><span class="line">    \end&#123;tabular&#125;</span><br><span class="line">\caption&#123;notations&#125;% 表格标题</span><br><span class="line">\label&#123;tab:my_label&#125; % 表格标签（引用需要）</span><br><span class="line">\end&#123;table*&#125;</span><br></pre></td></tr></table></figure><h2 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a>Graph</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">\usepackage[pdftex]&#123;graphicx&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;figure&#125;[htbp]</span><br><span class="line">\centering</span><br><span class="line">\includegraphics[height=6.0cm,width=9.5cm]&#123;fig/universe.jpg&#125;</span><br><span class="line">\caption&#123;Campus environment detection system&#125; % 图片标题</span><br><span class="line">\label&#123;fig:my_fig&#125;% 图片标签（引用需要）</span><br><span class="line">\end&#123;figure&#125;</span><br></pre></td></tr></table></figure><h2 id="Enumerate-and-Itemsize"><a href="#Enumerate-and-Itemsize" class="headerlink" title="Enumerate and Itemsize"></a>Enumerate and Itemsize</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;enumerate&#125;</span><br><span class="line">    \item .</span><br><span class="line">    \item .</span><br><span class="line">\end&#123;enumerate&#125;</span><br></pre></td></tr></table></figure><ol><li></li><li></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;itemize&#125;</span><br><span class="line">    \item .</span><br><span class="line">    \item .</span><br><span class="line">\end&#123;itemize&#125;</span><br></pre></td></tr></table></figure><ul><li></li><li></li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>导入包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">\usepackage&#123;cite&#125;</span><br><span class="line">% references format</span><br><span class="line">\bibliographystyle&#123;acm&#125;</span><br></pre></td></tr></table></figure><p>引用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">% 文献引用</span><br><span class="line">\cite&#123;&#125;</span><br><span class="line"></span><br><span class="line">% 表格、图片等引用</span><br><span class="line">\ref&#123;tab:my_label&#125;</span><br><span class="line">\ref&#123;fig:my_label&#125;</span><br><span class="line"></span><br><span class="line">% 脚注</span><br><span class="line">\footnote&#123;&#125;</span><br></pre></td></tr></table></figure><p>章节位置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">% References</span><br><span class="line">\bibliography&#123;references&#125;</span><br><span class="line">% add to toc</span><br><span class="line">\addcontentsline&#123;toc&#125;&#123;section&#125;&#123;References&#125;</span><br></pre></td></tr></table></figure><p>BiTex类型</p><p><a href="https://wenku.baidu.com/view/0f2096643968011ca300916d.htmlMath" target="_blank" rel="noopener">https://wenku.baidu.com/view/0f2096643968011ca300916d.htmlMath</a></p><h3 id="导入包"><a href="#导入包" class="headerlink" title="导入包"></a>导入包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">\usepackage&#123;amsmath,amssymb,amsthm&#125;</span><br><span class="line">\usepackage&#123;newtxmath&#125; % must come after amsXXX</span><br></pre></td></tr></table></figure><h3 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h3><p>分段函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;equation&#125;</span><br><span class="line">f(n) =</span><br><span class="line">\begin&#123;cases&#125;</span><br><span class="line">n/2,  &amp; \text&#123;if $n$ is even&#125; \\</span><br><span class="line">3n+1, &amp; \text&#123;if $n$ is odd&#125;</span><br><span class="line">\end&#123;cases&#125;</span><br><span class="line">\end&#123;equation&#125;</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{equation}f(n) =\begin{cases}n/2,  & \text{if $n$ is even} \\3n+1, & \text{if $n$ is odd}\end{cases}\end{equation}</script><p>方程组</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;equation*&#125;</span><br><span class="line">\left\&#123; </span><br><span class="line">\begin&#123;array&#125;&#123;c&#125;</span><br><span class="line">a_1x+b_1y+c_1z=d_1 \\ </span><br><span class="line">a_2x+b_2y+c_2z=d_2 \\ </span><br><span class="line">a_3x+b_3y+c_3z=d_3</span><br><span class="line">\end&#123;array&#125;</span><br><span class="line">\right.  </span><br><span class="line">\end&#123;equation*&#125;</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{equation*}\left\{ \begin{array}{c}a_1x+b_1y+c_1z=d_1 \\ a_2x+b_2y+c_2z=d_2 \\ a_3x+b_3y+c_3z=d_3\end{array}\right.  \end{equation*}</script><p>公式推导</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;equation&#125;</span><br><span class="line">\begin&#123;split&#125;</span><br><span class="line">\cos 2x &amp;= \cos^2 x - \sin^2 x\\</span><br><span class="line">&amp;= 2\cos^2 x - 1</span><br><span class="line">\end&#123;split&#125;</span><br><span class="line">\end&#123;equation&#125;</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{equation}\begin{split}\cos 2x &= \cos^2 x - \sin^2 x\\&= 2\cos^2 x - 1\end{split}\end{equation}</script>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Latex-Note&quot;&gt;&lt;a href=&quot;#Latex-Note&quot; class=&quot;headerlink&quot; title=&quot;Latex Note&quot;&gt;&lt;/a&gt;Latex Note&lt;/h1&gt;&lt;p&gt;​    一些简单的latex笔记，发现记录一下还是很有必要的，以免写论文的时候又忘了，到处去找，费时费神。&lt;/p&gt;
    
    </summary>
    
    
      <category term="latex" scheme="http://yoursite.com/categories/latex/"/>
    
    
      <category term="latex" scheme="http://yoursite.com/tags/latex/"/>
    
  </entry>
  
  <entry>
    <title>Python collections 简单上手</title>
    <link href="http://yoursite.com/2020/04/24/python/collections/"/>
    <id>http://yoursite.com/2020/04/24/python/collections/</id>
    <published>2020-04-24T04:52:25.133Z</published>
    <updated>2020-04-24T05:08:33.348Z</updated>
    
    <content type="html"><![CDATA[<h1 id="collections-库简单上手"><a href="#collections-库简单上手" class="headerlink" title="collections 库简单上手"></a>collections 库简单上手</h1><h2 id="Counter"><a href="#Counter" class="headerlink" title="Counter"></a>Counter</h2><p>   Counter 是 dictionary 对象的子类。collections 模块中的 Counter() 函数会接收一个诸如 list 或 tuple 的迭代器，然后返回一个 Counter dictionary。这个 dictionary 的键是该迭代器中的唯一元素，每个键的值是迭代器元素的计数。</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lst = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">counter = Counter(lst)</span><br><span class="line">print(counter)</span><br></pre></td></tr></table></figure><pre><code>Counter({1: 7, 2: 5, 3: 3})</code></pre><p>most_common()函数会以元组列表的形式，返回出现频率最高的n个元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">counter.most_common(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><pre><code>[(1, 7), (2, 5)]</code></pre><h2 id="defaultdict"><a href="#defaultdict" class="headerlink" title="defaultdict"></a>defaultdict</h2><p>defaultdict 的工作方式和平常的 python dictionary 完全相同，只是当你试图访问一个不存在的键时，它不会报错，而是会使用默认值初始化这个键。默认值是根据在创建 defaultdict 对象时作为参数输入的数据类型自动设置的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br></pre></td></tr></table></figure><p>如上所示，在访问不存在的键”Sara”时，程序并不会像一般字典一样报错，而是为其设置一个int型的默认值。<br>同理，当我们设置默认值类型为list时，看看会发生什么。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">names_dict = defaultdict(int)</span><br><span class="line">names_dict[<span class="string">"Bob"</span>] = <span class="number">1</span></span><br><span class="line">names_dict[<span class="string">"Katie"</span>] = <span class="number">2</span></span><br><span class="line">sara_number = names_dict[<span class="string">"Sara"</span>]</span><br><span class="line">print(names_dict)</span><br></pre></td></tr></table></figure><pre><code>defaultdict(&lt;class &#39;int&#39;&gt;, {&#39;Bob&#39;: 1, &#39;Katie&#39;: 2, &#39;Sara&#39;: 0})</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">names_dict1 = defaultdict(list)</span><br><span class="line">names_dict1[<span class="string">"Bob"</span>] = <span class="number">1</span></span><br><span class="line">names_dict1[<span class="string">"Katie"</span>] = <span class="number">2</span></span><br><span class="line">sara_number = names_dict1[<span class="string">"Sara"</span>]</span><br><span class="line">print(names_dict1)</span><br></pre></td></tr></table></figure><pre><code>defaultdict(&lt;class &#39;list&#39;&gt;, {&#39;Bob&#39;: 1, &#39;Katie&#39;: 2, &#39;Sara&#39;: []})</code></pre><p>可以看到，默认值变成了一个空列表</p><h2 id="OrderedDict"><a href="#OrderedDict" class="headerlink" title="OrderedDict"></a>OrderedDict</h2><p>返回一个字典的子类，具有可以将字典顺序重新排列的方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d = OrderedDict.fromkeys(<span class="string">'abcde'</span>)</span><br><span class="line">d.move_to_end(<span class="string">'b'</span>)</span><br><span class="line"><span class="string">''</span>.join(d.keys())</span><br></pre></td></tr></table></figure><pre><code>&#39;acdeb&#39;</code></pre><p>last 参数默认为True，即将元素挪到最右边，False则将元素挪到最左边</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d.move_to_end(<span class="string">'b'</span>, last=<span class="keyword">False</span>) </span><br><span class="line"><span class="string">''</span>.join(d.keys())</span><br><span class="line"><span class="string">'bacde'</span></span><br></pre></td></tr></table></figure><pre><code>&#39;bacde&#39;</code></pre><p>popitem()方法将最后的元素弹出，参数last默认为True，即弹出最右边的元素，False则为最左边。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"poped item: &#123;&#125;"</span>.format(d.popitem()))</span><br><span class="line">print(d)</span><br></pre></td></tr></table></figure><pre><code>poped item: (&#39;e&#39;, None)OrderedDict([(&#39;b&#39;, None), (&#39;a&#39;, None), (&#39;c&#39;, None), (&#39;d&#39;, None)])</code></pre><h2 id="deque"><a href="#deque" class="headerlink" title="deque"></a>deque</h2><p>   collections.deque是python中使用队列的一种非常好的方法，这个方法的一个关键特性是保持队列长度一直不变，也就是说，如果你将 queue 的最大大小设置为 10，那么 deque 将根据 FIFO 原则添加和删除元素，以保持 queue 的最大大小为 10。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">my_queue = deque(maxlen=<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    my_queue.append(i+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(my_queue)</span><br></pre></td></tr></table></figure><pre><code>deque([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], maxlen=10)</code></pre><p>   接下来，我们尝试继续往里面添加元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">11</span>,<span class="number">16</span>):</span><br><span class="line">    my_queue.append(i)</span><br><span class="line"></span><br><span class="line">print(my_queue)</span><br></pre></td></tr></table></figure><pre><code>deque([6, 7, 8, 9, 10, 11, 12, 13, 14, 15], maxlen=10)</code></pre><p>可以看到，最前面的5个元素被弹出，而新元素被追加到队列的后面</p><h2 id="nametuple"><a href="#nametuple" class="headerlink" title="nametuple"></a>nametuple</h2><p>nametuple是tuple的子类，意味着它可以像tuple一样通过索引下标访问，同时，它还具备了通过字段访问元素的功能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Point = namedtuple(<span class="string">'Point'</span>, [<span class="string">'x'</span>, <span class="string">'y'</span>])</span><br><span class="line">p = Point(<span class="number">11</span>, y=<span class="number">22</span>) </span><br><span class="line"><span class="comment"># 像普通元组一样访问元素</span></span><br><span class="line">print(p[<span class="number">0</span>] + p[<span class="number">1</span>]) </span><br><span class="line"><span class="comment"># 通过字段访问元素</span></span><br><span class="line">print(p.x + p.y)</span><br></pre></td></tr></table></figure><pre><code>3333</code></pre><p>为了防止与字段名冲突，nametuple的类方法和属性前都加了下划线_。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># _make方法将列表转化为nametuple</span></span><br><span class="line">t = [<span class="number">11</span>, <span class="number">22</span>]</span><br><span class="line">Point._make(t)</span><br></pre></td></tr></table></figure><pre><code>Point(x=11, y=22)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># _asdict()将nametuple转为字典</span></span><br><span class="line">p = Point(x=<span class="number">11</span>, y=<span class="number">22</span>)</span><br><span class="line">p._asdict()</span><br></pre></td></tr></table></figure><pre><code>OrderedDict([(&#39;x&#39;, 11), (&#39;y&#39;, 22)])</code></pre><p>需要注意，由于nametuple本质上还是一个tuple，因此直接更改其中的元素是会报错的。正确的做法是使用_replace函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># p.x = 12 报错</span></span><br><span class="line">p._replace(x=<span class="number">12</span>)</span><br></pre></td></tr></table></figure><pre><code>Point(x=12, y=22)</code></pre><p>可以通过_fields属性访问nametuple的字段。一个很常见的用法是从现有的nametuple创建一个新的nametuple。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p._fields</span><br></pre></td></tr></table></figure><pre><code>(&#39;x&#39;, &#39;y&#39;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Color = namedtuple(<span class="string">'Color'</span>, <span class="string">'red green blue'</span>)</span><br><span class="line">Pixel = namedtuple(<span class="string">'Pixel'</span>, Point._fields + Color._fields)</span><br><span class="line">Pixel(<span class="number">11</span>, <span class="number">22</span>, <span class="number">128</span>, <span class="number">255</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><pre><code>Pixel(x=11, y=22, red=128, green=255, blue=0)</code></pre><p> 如果希望将字典转换为nametuple，可以用如下方式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d = &#123;<span class="string">'x'</span>: <span class="number">11</span>, <span class="string">'y'</span>: <span class="number">22</span>&#125;</span><br><span class="line">Point(**d)</span><br></pre></td></tr></table></figure><pre><code>Point(x=11, y=22)</code></pre><h2 id="ChainMap"><a href="#ChainMap" class="headerlink" title="ChainMap"></a>ChainMap</h2><p>可以快速链接许多映射，因此可以将它们视为一个单元，通常比创建新字典并运行多个update()要快得多。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> ChainMap</span><br></pre></td></tr></table></figure><p>注意，ChainMap的迭代顺序是通过扫描从前往后的映射决定的（与官方文档说的从后往前相反，研究了甚久，有可能是版本问题）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">baseline = &#123;<span class="string">'music'</span>: <span class="string">'bach'</span>, <span class="string">'art'</span>: <span class="string">'rembrandt'</span>&#125;</span><br><span class="line">adjustments = &#123;<span class="string">'art'</span>: <span class="string">'van gogh'</span>, <span class="string">'opera'</span>: <span class="string">'carmen'</span>&#125;</span><br><span class="line">print(list(ChainMap(adjustments, baseline)))</span><br><span class="line">print(dict(ChainMap(adjustments, baseline)))</span><br></pre></td></tr></table></figure><pre><code>[&#39;art&#39;, &#39;opera&#39;, &#39;music&#39;]{&#39;art&#39;: &#39;van gogh&#39;, &#39;opera&#39;: &#39;carmen&#39;, &#39;music&#39;: &#39;bach&#39;}</code></pre><p>这与从最后一个映射开始的一系列dict.update()调用的顺序相反:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">combined = baseline.copy()</span><br><span class="line">combined.update(adjustments)</span><br><span class="line">print(list(combined))</span><br><span class="line">print(dict(combined))</span><br></pre></td></tr></table></figure><pre><code>[&#39;music&#39;, &#39;art&#39;, &#39;opera&#39;]{&#39;music&#39;: &#39;bach&#39;, &#39;art&#39;: &#39;van gogh&#39;, &#39;opera&#39;: &#39;carmen&#39;}</code></pre><p>ChainMap最常用的例子是在程序传入参数的优先级设置上，例如：参数可以通过命令行传入，可以通过环境变量传入，还可以有默认参数，我们可以设置优先级：命令行&gt;环境变量&gt;默认值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jupyter notebook中无法使用argparse</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> ChainMap</span><br><span class="line"><span class="keyword">import</span> os, argparse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造缺省参数:</span></span><br><span class="line">defaults = &#123;</span><br><span class="line">    <span class="string">'color'</span>: <span class="string">'red'</span>,</span><br><span class="line">    <span class="string">'user'</span>: <span class="string">'guest'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造命令行参数:</span></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-u'</span>, <span class="string">'--user'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'-c'</span>, <span class="string">'--color'</span>)</span><br><span class="line">namespace = parser.parse_args()</span><br><span class="line">command_line_args = &#123; k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> vars(namespace).items() <span class="keyword">if</span> v &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 组合成ChainMap:</span></span><br><span class="line">combined = ChainMap(command_line_args, os.environ, defaults)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印参数:</span></span><br><span class="line">print(<span class="string">'color=%s'</span> % combined[<span class="string">'color'</span>])</span><br><span class="line">print(<span class="string">'user=%s'</span> % combined[<span class="string">'user'</span>])</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;collections-库简单上手&quot;&gt;&lt;a href=&quot;#collections-库简单上手&quot; class=&quot;headerlink&quot; title=&quot;collections 库简单上手&quot;&gt;&lt;/a&gt;collections 库简单上手&lt;/h1&gt;&lt;h2 id=&quot;Counter&quot;&gt;&lt;a href=&quot;#Counter&quot; class=&quot;headerlink&quot; title=&quot;Counter&quot;&gt;&lt;/a&gt;Counter&lt;/h2&gt;&lt;p&gt;   Counter 是 dictionary 对象的子类。collections 模块中的 Counter() 函数会接收一个诸如 list 或 tuple 的迭代器，然后返回一个 Counter dictionary。这个 dictionary 的键是该迭代器中的唯一元素，每个键的值是迭代器元素的计数。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="collections" scheme="http://yoursite.com/tags/collections/"/>
    
  </entry>
  
  <entry>
    <title>Byte Pair Encoding</title>
    <link href="http://yoursite.com/2020/04/23/natural-language-processing/Byte-Pair-Encoding/"/>
    <id>http://yoursite.com/2020/04/23/natural-language-processing/Byte-Pair-Encoding/</id>
    <published>2020-04-23T03:15:00.587Z</published>
    <updated>2020-04-24T08:20:28.181Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Byte-Pair-Encoding"><a href="#Byte-Pair-Encoding" class="headerlink" title="Byte Pair Encoding"></a>Byte Pair Encoding</h1><p>​    <strong>Byte Pari Encoding（BPE）</strong>）是一种简单的数据压缩技巧，最初在1994年被提出，而如今广泛应用于各种现代的NLP模型（如BERT、GPT-2、XLM…）。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>​    如今NLP的发展，由刚开始的基于频率的稀疏词向量（如词袋、N-gram），到前几年通过预训练（word2vec，glove）生成语义表示的稠密词向量，再到划时代意义的BERT，NLP界越来越重视通过预训练学习语义，从而得到好的词向量表示。</p><p>​    然而这其中有个问题需要解决，基本上所有的NLP任务都需要构造一个词表，往往是选取出现频率最高的N个词加入词表。这其中有什么问题呢，举个例子，BERT的预训练语料是维基百科，如此庞大的语料库，词量肯定也是惊人的高。如果我们构造的词表太小，很多频率较低的词语就会被表示为一个掩码（又或是被去除），在模型中就根本“看不到”，这不仅会让这些低频词无法被学习，还会影响上下文中的高频词的学习。但如果词表过大，就会带来效率问题。</p><p>​    BPE就是一种很好的方法，在不扩大词表的情况下，能够让更多的词语“被模型看到”。</p><a id="more"></a><h2 id="原始BPE"><a href="#原始BPE" class="headerlink" title="原始BPE"></a>原始BPE</h2><p>​    BPE最早在1994年由Philip Gage提出（<a href="https://www.drdobbs.com/a-new-algorithm-for-data-compression/184402829" target="_blank" rel="noopener">A New Algorithm for Data Compression</a>），是一种数据压缩的技术，通过使用数据中未出现的字节代替常见的连续字节对。</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/bpe1.png" alt></p><h2 id="子词标记（Subword-Tokenization）的BPE"><a href="#子词标记（Subword-Tokenization）的BPE" class="headerlink" title="子词标记（Subword Tokenization）的BPE"></a>子词标记（Subword Tokenization）的BPE</h2><p>​    方法最早在<a href="https://arxiv.org/pdf/1508.07909.pdf" target="_blank" rel="noopener">Neural Machine Translation of Rare Words with Subword Units</a>提出，最初是为了解决神经网络翻译（NMT）中的OOV（Out of vocabulary）问题，即处理的词表是定长的，而翻译却往往会在词表之外。同样的，这个问题各种NLP任务也会遇到。</p><p>​    为了进行子词标记，BPE简单地做了一些调整，经常出现的子字对合并在一起，而不是被另一个字节代替。这样一来，低频的词就会被分割成几个高频的子词，例如athazagoraphobia可能就会被分割成[‘▁ath’, ‘az’, ‘agor’, ‘aphobia’]。</p><ul><li>第0步，初始化词表；</li><li>第1步，把每个词表示为字符的组合，为了防止与其他词混淆，在词的后面再加入一个特殊的标识”&lt;\w&gt;”。例如hello，就会被分割成：”h e l l o <_ w>“ 。</_></li><li>第2步，遍历词表，统计所有的字符对（其实就是2-gram），如：(h, e), (e, l), (l, l), (l, o), (o, <_ w>)。</_></li><li>第3步，合并频率最高的字符对，将合成的新子词加入词表（或词频+1）。例如，(h, e)出现频率最高，那么就将他们合并，变成新子词he，再将这个新词加入词表，同时原来的单词就变成“he l l o <_ w>”。</_></li><li>第4步，重复2、3步，直到达到迭代次数或达到所需的词汇量。（超参数）</li></ul><h2 id="编码与解码"><a href="#编码与解码" class="headerlink" title="编码与解码"></a>编码与解码</h2><p>​    得到子词的词表后，我们先需要对子词表进行长度从大到小的排序。编码时，我们对每个单词，遍历子词表，寻找是否有子词是该单词的子串，然后将该单词分割成若干子词。若某一子串没有找到对应的子词，那么将它替换为一个特殊的标记如“<_ unk>”。</_></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 给定单词序列</span><br><span class="line">[“the&lt;/w&gt;”, “biggest&lt;/w&gt;”, “stone&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"># 假设已有排好序的subword词表</span><br><span class="line">[“er&lt;/w&gt;”, “tain&lt;/w&gt;”, “tone”, “est&lt;/w&gt;”, “big”, “the&lt;/w&gt;”, “one&lt;/w&gt;”, &quot;s&quot;, &quot;g&quot;]</span><br><span class="line"></span><br><span class="line"># 迭代结果</span><br><span class="line">&quot;the&lt;/w&gt;&quot; -&gt; [&quot;the&lt;/w&gt;&quot;]</span><br><span class="line">&quot;biggest&lt;/w&gt;&quot; -&gt; [&quot;big&quot;, &quot;g&quot;,&quot;est&lt;/w&gt;&quot;]</span><br><span class="line">&quot;stone&lt;/w&gt;&quot; -&gt; [&quot;st&quot;, &quot;tone&lt;/w&gt;&quot;]</span><br></pre></td></tr></table></figure><p>​    这种方式的好处是，可以很好地适应新语料，缺点是，如果语料很大，这一编码方式效率较慢，因此一般是在训练模型之前，预编码好语料。还有一种方式是，在构建子词表的时候，其实就可以对构建子词表的语料进行编码了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 给定单词序列</span><br><span class="line">[..., “the&lt;/w&gt;”, “biggest&lt;/w&gt;”, “stone&lt;/w&gt;”, ...] # 假设语料库中还有其他词语</span><br><span class="line"></span><br><span class="line"># 第一步</span><br><span class="line"># [..., &quot;t h e &lt;/w&gt;&quot;, &quot;b i g g e s t &lt;/w&gt;&quot;, &quot;s t o n e &lt;/w&gt;&quot;, ...]</span><br><span class="line"></span><br><span class="line"># 第二步</span><br><span class="line"># ..., (t, h), (h, e), (e, &lt;/w&gt;), (b, i), ...</span><br><span class="line"></span><br><span class="line"># 第三步，假设he频率最高</span><br><span class="line"># [..., &quot;t he &lt;/w&gt;&quot;, &quot;b i g g e s t &lt;/w&gt;&quot;, &quot;s t o n e &lt;/w&gt;&quot;, ...]</span><br><span class="line"></span><br><span class="line"># 继续第二步</span><br><span class="line"># ..., (t , he), (he, &lt;/w&gt;), (e, &lt;/w&gt;), (b, i), ...</span><br><span class="line"></span><br><span class="line"># ... 第三步</span><br><span class="line"># ... 迭代，直到次数满足，或词表大小满足</span><br><span class="line"></span><br><span class="line"># 最终结果</span><br><span class="line"># [..., &quot;the&lt;/w&gt;&quot;, &quot;big g est&lt;/w&gt;&quot;, &quot;s tone&lt;/w&gt;&quot;]</span><br></pre></td></tr></table></figure><p>​    可以看到，这个时候，我们直接将改变后的每个单词，按照空格符分割即可。</p><p>​    解码的过程就简单多了，以”<_ w>“作为分割符，将子词合并为单词即可</_></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 假设模型预测后得到</span><br><span class="line">[“the&lt;/w&gt;”, &quot;big&quot;, &quot;g&quot;, &quot;est&lt;/w&gt;&quot;, &quot;s&quot;, &quot;tone&lt;/w&gt;&quot;]</span><br><span class="line"></span><br><span class="line"># 解码</span><br><span class="line">the biggest stone</span><br></pre></td></tr></table></figure><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter, defaultdict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(corpus: str)</span> -&gt; dict:</span></span><br><span class="line">    <span class="string">"""Step 1. 建立词表"""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把每个字符分割出来，并在词语最后加上&lt;/w&gt;，如:"hello"-&gt;"h e l l o &lt;/w&gt;"</span></span><br><span class="line">    tokens = [<span class="string">" "</span>.join(word) + <span class="string">" &lt;/w&gt;"</span> <span class="keyword">for</span> word <span class="keyword">in</span> corpus.split()]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 统计语料中的词频</span></span><br><span class="line">    vocab = Counter(tokens)  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vocab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_stats</span><span class="params">(vocab: dict)</span> -&gt; dict:</span></span><br><span class="line">    <span class="string">"""Step 2. 分割字符串，并统计字符对的频率"""</span></span><br><span class="line"></span><br><span class="line">    pairs = defaultdict(int)</span><br><span class="line">    <span class="keyword">for</span> word, frequency <span class="keyword">in</span> vocab.items():</span><br><span class="line">        symbols = word.split() <span class="comment"># 以空格分割</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 统计2-gram频率</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(symbols) - <span class="number">1</span>):</span><br><span class="line">            pairs[symbols[i], symbols[i + <span class="number">1</span>]] += frequency</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_vocab</span><span class="params">(pair: tuple, v_in: dict)</span> -&gt; dict:</span></span><br><span class="line">    <span class="string">"""合并频率最高的对"""</span></span><br><span class="line">    </span><br><span class="line">    v_out = &#123;&#125;</span><br><span class="line">    bigram = re.escape(<span class="string">' '</span>.join(pair))</span><br><span class="line">    p = re.compile(<span class="string">r'(?&lt;!\S)'</span> + bigram + <span class="string">r'(?!\S)'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> v_in:</span><br><span class="line">        <span class="comment"># 将词表中所有的最高词频对替换，如："h e l l o" -&gt; "he l l o"</span></span><br><span class="line">        w_out = p.sub(<span class="string">''</span>.join(pair), word)</span><br><span class="line">        v_out[w_out] = v_in[word]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> v_out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">corpus = <span class="string">"I had seen the biggest stone."</span></span><br><span class="line">vocab = build_vocab(corpus)  <span class="comment"># Step 1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">num_merges = <span class="number">50</span> <span class="comment"># 迭代次数，超参数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_merges):</span><br><span class="line"></span><br><span class="line">    pairs = get_stats(vocab)  <span class="comment"># Step 2</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> pairs:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># step 3</span></span><br><span class="line">    best = max(pairs, key=pairs.get)</span><br><span class="line">    vocab = merge_vocab(best, vocab)</span><br></pre></td></tr></table></figure><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>​     [1] Sennrich, Rico, Barry Haddow, and Alexandra Birch. “Neural machine translation of rare words with subword units.”<em>arXiv preprint arXiv:1508.07909</em>(2015).</p><p>​    [2] <a href="https://towardsdatascience.com/@JaswalAkash?source=post_page-----eb36c7df4f10----------------------" target="_blank" rel="noopener">Akashdeep Singh Jaswal</a>, <a href="https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10" target="_blank" rel="noopener">Byte Pair Encoding — The Dark Horse of Modern NLP</a>, 2019.</p><p>​    [3] <a href="mailto:plmsmile@126.com" target="_blank" rel="noopener">PLM</a>, <a href="https://plmsmile.github.io/2017/10/19/subword-units/" target="_blank" rel="noopener">subword-units</a>, 2017.</p><p>​    [4] <a href="https://www.zhihu.com/people/luke-china" target="_blank" rel="noopener">Luke</a>, 深入理解NLP Subword算法：BPE、WordPiece、ULM, 2020.</p><p>​    [5] Byte pair encoding - Wikipedia - <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/B</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Byte-Pair-Encoding&quot;&gt;&lt;a href=&quot;#Byte-Pair-Encoding&quot; class=&quot;headerlink&quot; title=&quot;Byte Pair Encoding&quot;&gt;&lt;/a&gt;Byte Pair Encoding&lt;/h1&gt;&lt;p&gt;​    &lt;strong&gt;Byte Pari Encoding（BPE）&lt;/strong&gt;）是一种简单的数据压缩技巧，最初在1994年被提出，而如今广泛应用于各种现代的NLP模型（如BERT、GPT-2、XLM…）。&lt;/p&gt;
&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;​    如今NLP的发展，由刚开始的基于频率的稀疏词向量（如词袋、N-gram），到前几年通过预训练（word2vec，glove）生成语义表示的稠密词向量，再到划时代意义的BERT，NLP界越来越重视通过预训练学习语义，从而得到好的词向量表示。&lt;/p&gt;
&lt;p&gt;​    然而这其中有个问题需要解决，基本上所有的NLP任务都需要构造一个词表，往往是选取出现频率最高的N个词加入词表。这其中有什么问题呢，举个例子，BERT的预训练语料是维基百科，如此庞大的语料库，词量肯定也是惊人的高。如果我们构造的词表太小，很多频率较低的词语就会被表示为一个掩码（又或是被去除），在模型中就根本“看不到”，这不仅会让这些低频词无法被学习，还会影响上下文中的高频词的学习。但如果词表过大，就会带来效率问题。&lt;/p&gt;
&lt;p&gt;​    BPE就是一种很好的方法，在不扩大词表的情况下，能够让更多的词语“被模型看到”。&lt;/p&gt;
    
    </summary>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="BERT" scheme="http://yoursite.com/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>ELU and GELU</title>
    <link href="http://yoursite.com/2019/01/11/deep-learning/ELU&amp;GELU/"/>
    <id>http://yoursite.com/2019/01/11/deep-learning/ELU&amp;GELU/</id>
    <published>2019-01-11T08:47:03.905Z</published>
    <updated>2020-04-27T07:55:34.380Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h2><p>&emsp; <strong>ELU</strong>(<em>Exponential Linear Units</em>)，是对ReLU负半区为0的改进，使得激活单元均值更趋向于0.</p><h3 id="图像和公式"><a href="#图像和公式" class="headerlink" title="图像和公式"></a>图像和公式</h3><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/ELU1.png?raw=true" alt="GELU1" style="zoom: 33%;"></p><blockquote><p>其中，LReLU($\alpha=0.1$)，ELU($\alpha=1$) </p></blockquote><a id="more"></a><p>公式如下</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/ELU2.png?raw=true" alt="GELU1"></p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/ELU3.png?raw=true" alt="GELU1"></p><blockquote><p>在MINST数据集上的实验结果，左边是激活值的平均值，右边是交叉熵损失，可以看到ELU的激活值均值要更接近0，且交叉熵损失下降得更快.</p></blockquote><h2 id="GELU"><a href="#GELU" class="headerlink" title="GELU"></a>GELU</h2><p>&emsp; <strong>GELU</strong>(<em>Gaussian Error Linear Unit</em>)，是ReLU的一种改进，改进了ReLU缺乏概率解释的缺陷，在一些任务上表现得要更好. </p><h3 id="图像和公式-1"><a href="#图像和公式-1" class="headerlink" title="图像和公式"></a>图像和公式</h3><p>&emsp;<strong>GELU</strong>的曲线如下</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/GELU1.png?raw=true" alt="GELU1" style="zoom: 33%;"></p><blockquote><p>其中，GELU($\mu =0,\sigma =1$)，ELU($\alpha = 1$)</p></blockquote><p>公式如下：</p><script type="math/tex; mode=display">GELU(x) = xP(X<x) = x\Phi(x)</script><p>并且可以用如下公式估计</p><script type="math/tex; mode=display">0.5x(1+tanh[\sqrt{2/\pi}(x+0.044715x^3)])</script><p>或是</p><script type="math/tex; mode=display">x \sigma(1.702x)</script><h3 id="实验结果-1"><a href="#实验结果-1" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/GELU2.png?raw=true" alt="GELU1" style="zoom:67%;"></p><blockquote><p>在MINST数据集上的结果，左边是未使用Dropout，右边是使用了Dropout.</p></blockquote><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/GELU3.png?raw=true" alt="GELU1" style="zoom:67%;"></p><blockquote><p>在MINST数据集上的鲁棒性结果，可以看到，随着噪声的增加，GELU要表现得更稳定.</p></blockquote><p>&lt;/br&gt;</p><p>&lt;/br&gt;</p><blockquote><p>参考资料</p><p><a href="https://blog.csdn.net/mao_xiao_feng/article/details/53242235" target="_blank" rel="noopener">https://blog.csdn.net/mao_xiao_feng/article/details/53242235</a></p><p><a href="https://arxiv.org/abs/1511.07289" target="_blank" rel="noopener">https://arxiv.org/abs/1511.07289</a></p><p><a href="https://blog.csdn.net/dgyuanshaofeng/article/details/80209816" target="_blank" rel="noopener">https://blog.csdn.net/dgyuanshaofeng/article/details/80209816</a></p><p><a href="https://arxiv.org/abs/1606.08415---" target="_blank" rel="noopener">https://arxiv.org/abs/1606.08415---</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;ELU&quot;&gt;&lt;a href=&quot;#ELU&quot; class=&quot;headerlink&quot; title=&quot;ELU&quot;&gt;&lt;/a&gt;ELU&lt;/h2&gt;&lt;p&gt;&amp;emsp; &lt;strong&gt;ELU&lt;/strong&gt;(&lt;em&gt;Exponential Linear Units&lt;/em&gt;)，是对ReLU负半区为0的改进，使得激活单元均值更趋向于0.&lt;/p&gt;
&lt;h3 id=&quot;图像和公式&quot;&gt;&lt;a href=&quot;#图像和公式&quot; class=&quot;headerlink&quot; title=&quot;图像和公式&quot;&gt;&lt;/a&gt;图像和公式&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://blog-fig.oss-cn-shenzhen.aliyuncs.com/ELU1.png?raw=true&quot; alt=&quot;GELU1&quot; style=&quot;zoom: 33%;&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;其中，LReLU($\alpha=0.1$)，ELU($\alpha=1$) &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="激活函数" scheme="http://yoursite.com/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>BERT</title>
    <link href="http://yoursite.com/2019/01/11/natural-language-processing/BERT/"/>
    <id>http://yoursite.com/2019/01/11/natural-language-processing/BERT/</id>
    <published>2019-01-11T02:28:04.569Z</published>
    <updated>2020-04-24T08:21:16.115Z</updated>
    
    <content type="html"><![CDATA[<h1 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h1><p>&emsp; <strong>BERT</strong>全称是<strong>Bidirectional Encoder Representations from Transformers</strong>，从字面意思可以知道，这是一个基于Transformer的双向的编码器表征模型.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/BERT1.png?raw=trueF:\blog-fig\BERT1.png" alt="BERT1"></p><blockquote><p>上面是BERT和GPT、ELMo的结构对比.</p></blockquote><a id="more"></a><h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/BERT3.png?raw=true" alt></p><p>&emsp; <strong>BERT</strong>的Embedding是3个Embedding的相加，即Token Embeddings、Segment Embeddings、Position Embeddings.</p><ul><li>Token Embeddings：词向量，其中第一个单词是分类任务的标记，可用于后面的分类任务</li><li>Segment Embeddings：用来区别上句和下句</li><li>Position Embeddings：位置信息向量，和Transformer中使用三角函数不一样，这里的是通过学习得到的</li></ul><h2 id="预训练任务"><a href="#预训练任务" class="headerlink" title="预训练任务"></a>预训练任务</h2><p>&emsp; BERT有两个无监督预训练任务，第一步是随机遮盖一些词，然后依赖上下文来预测这些词，得到初步的预训练模型；第二步在第一步的基础上，是随机改变一半的句子对中的第二句，预测一个句子对的第二句是否是第一句的下一句。</p><h3 id="Task-1"><a href="#Task-1" class="headerlink" title="Task 1"></a>Task 1</h3><p>&emsp; 第一个任务称为Mask-LM，常规的LanguageModel是这样的</p><script type="math/tex; mode=display">P(X_i|X_{i-1},...X_1)</script><p>或是这样的</p><script type="math/tex; mode=display">P(X_i|X_{i+1},...,X_{n})</script><p>而Mask-LM充分利用上下文信息，具体地，随机Mask掉15%的词，然后只去预测那些被Mask的词，即</p><script type="math/tex; mode=display">P(masked|X_{else})</script><p>&emsp; 实际上，BERT并不总是Mask那15%的词，而是</p><ul><li>80%的概率Mask</li><li>10%的概率用一个随机的词代替</li><li>10%的概率不变</li></ul><blockquote><p>需注意的是模型本身并不知道它将要被要求预测哪些单词，或哪些单词被替换.</p></blockquote><h3 id="Task-2"><a href="#Task-2" class="headerlink" title="Task 2"></a>Task 2</h3><p>&emsp; 第二个任务则是预测句子的下一句，即给定句子对$<s_1,s_2>$，预测$S_2$是否是$S_1$的下一句。具体地，BERT随机替换50%的句子对中的$S_2$，并自动给定标签，然后去对所有句子对做预测. 例如：</s_1,s_2></p><blockquote><p>Input = [CLS] the man went to [MASK] store [SEP]</p><p>he bought a gallon [MASK] milk [SEP]</p><p>Label = IsNext</p><p>Input = [CLS] the man [MASK] to the store [SEP]</p><p>penguin [MASK] are flight ##less birds [SEP]</p><p>Label = NotNext</p></blockquote><h2 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine-tuning"></a>Fine-tuning</h2><p>&emsp; 预训练好的BERT模型，再去做一些简单调整，再在新的数据集上做训练，可以在许多自然语言处理的任务上有很好的表现. 例如，文本分类、问答、命名实体识别、上下文预测、对话等.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/BERT2.png?raw=true" alt></p><p>&lt;/br&gt;</p><p>&lt;/br&gt;</p><blockquote><p>参考资料</p><p><a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">https://arxiv.org/abs/1810.04805</a></p><p><a href="https://zhuanlan.zhihu.com/p/46652512---" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46652512---</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;BERT&quot;&gt;&lt;a href=&quot;#BERT&quot; class=&quot;headerlink&quot; title=&quot;BERT&quot;&gt;&lt;/a&gt;BERT&lt;/h1&gt;&lt;p&gt;&amp;emsp; &lt;strong&gt;BERT&lt;/strong&gt;全称是&lt;strong&gt;Bidirectional Encoder Representations from Transformers&lt;/strong&gt;，从字面意思可以知道，这是一个基于Transformer的双向的编码器表征模型.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog-fig.oss-cn-shenzhen.aliyuncs.com/BERT1.png?raw=trueF:\blog-fig\BERT1.png&quot; alt=&quot;BERT1&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;上面是BERT和GPT、ELMo的结构对比.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="BERT" scheme="http://yoursite.com/tags/BERT/"/>
    
      <category term="Pre-training" scheme="http://yoursite.com/tags/Pre-training/"/>
    
  </entry>
  
  <entry>
    <title>Attention</title>
    <link href="http://yoursite.com/2019/01/11/deep-learning/attention/"/>
    <id>http://yoursite.com/2019/01/11/deep-learning/attention/</id>
    <published>2019-01-11T01:35:08.450Z</published>
    <updated>2020-04-23T11:03:05.115Z</updated>
    
    <content type="html"><![CDATA[<h1 id="注意力模型"><a href="#注意力模型" class="headerlink" title="注意力模型"></a>注意力模型</h1><p>&emsp; <strong>注意力模型</strong>(<em>Attention Model</em>)被广泛使用在自然语言处理、图像识别及语音识别等各种不同类型的深度学习任务中，它的基本思想是仿照人类在做阅读文本、观察物体和听声音等工作时，会对特定的词语、物体、声音给予更多的关注，因而能更好地完成对语言、图像和声音的识别以及理解.</p><a id="more"></a><h2 id="Encoder-Decoder模型"><a href="#Encoder-Decoder模型" class="headerlink" title="Encoder-Decoder模型"></a>Encoder-Decoder模型</h2><p>&emsp; 注意力模型的一个广泛的应用是在机器翻译和对话生成上，而这两个应用最广泛使用的即是Encoder-Decoder模型，AM可以对Encoder-Decoder模型进行改进.</p><p>&emsp; 如图所示，这是Encoder-Decoder模型的基本架构.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/attention1.jpg" alt></p><p>具体地，Encoder负责将输入的序列$(x_1,x_2,…,x_m)$，转化为语义编码$C$，而Decoder则负责将$C$生成$(y_1,y_2,…,y_n)$. </p><script type="math/tex; mode=display">\begin{align}C &= F(x_1,x_2,...,x_m)    \\y_i &= G(C,y_1,y_2,...,y_{i-1})\end{align}</script><p>一般在文本处理领域，Encoder和Decoder都是RNN模型.</p><p>&emsp; 但普通的Encoder-Decoder模型存在一个问题，即Encoder生成的语义编码$C$都是一样的，这样一来，无论是生成哪个单词，输入的每个单词对它的影响力都是一样的.  </p><script type="math/tex; mode=display">\begin{align}y_1 &= f(C)    \\y_2 &= f(C,y_1)        \\y_3 &= f(C,y_1,y_2)\end{align}</script><p>这样一来，无法体现输入的不同单词对于目标单词的影响，于是需要引入注意力机制.</p><h2 id="引入注意力"><a href="#引入注意力" class="headerlink" title="引入注意力"></a>引入注意力</h2><p>&emsp; 引入注意力机制的Encoder-Decoder模型的结构如下，</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/attention2.jpg" alt></p><p>相比于普通的Encoder-Decoder模型，引入注意力机制的模型在生成语义编码上面更加多样化，对于每个生成的词语$y_i$，都有语义编码$C_i$与之对应.  于是目标单词的生成变成了如下的形式：</p><script type="math/tex; mode=display">\begin{align}y_1 &= f(C_1)    \\y_2 &= f(C_2,y_1)        \\y_3 &= f(C_3,y_1,y_2)\end{align}</script><p>那么，如何计算不同的目标单词对应的语义编码$C_i$呢？</p><p>&emsp; 为了体现不同输入单词对目标单词的影响，而又尽可能不要损失信息，一般采用加权求和的方式计算$C_i$.</p><script type="math/tex; mode=display">C_i = \sum_{j=1}^{L_x}a_{ij}h_j</script><p>其中，$L_x$代表输入句子的长度，$a_{ij}$代表输出第$i$个单词的注意力分配系数，而$h_j$是输入句子中第$j$个单词的语义编码.</p><p>&emsp; 于是问题就转变成如何求得注意力分配系数$a_{ij}$.  我们把引入注意力机制的Encoder-Decode模型展开，如下所示.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/attention4.jpg" alt></p><p>(图中有误，$H_n$应该为$H_{n-1}$)那么我们可以采用如下方式计算注意力分配系数.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/attention5.jpg" alt></p><p>如图所示(图中有误，$H_{i-1}$应该是$H_{i-2}$，$H_{i}$应该是$H_{i-1 }$)，使用函数F，使得$h_j$与Decoder中的$y_i$之前的隐藏状态$h_i$对齐，最后以Softmax函数输出概率，即可得到目标单词$y_i$所对应的输入单词$x_j$的注意力分配系数$a_{ij}$.</p><h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><p>​    我们可以总结出一个通用的注意力机制模式，即如下所示.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/attention6.jpg" alt></p><p>将Source中的元素看成是<key,value>的键值对，然后计算Query和各个key之间的相似度或相关度，就能得到每个key对应value的权重系数，然后加权求和就能得到最终的AttentionValue.</key,value></p><script type="math/tex; mode=display">Attention(Query,Source)=\sum_{i=1}^{L_x}Similarity(Query,Key_i)*Value_i</script><p>&emsp; 详细一点，我们可以将注意力机制分为3个阶段，第1个阶段，计算Query和Key的相似度或相关度；第2个阶段，进行归一化处理；第3个阶段，根据权重对Value进行加权求和得到AttentionValue.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/atttention7.jpg" alt></p><p>&lt;/br&gt;</p><p>&lt;/br&gt;</p><blockquote><p>参考资料：</p><p>[1]. 张俊林, 深度学习中的注意力机制, <a href="https://blog.csdn.net/tg229dvt5i93mxaq5a6u/article/details/78422216" target="_blank" rel="noopener">https://blog.csdn.net/tg229dvt5i93mxaq5a6u/article/details/78422216</a>. 2017-11-02.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;注意力模型&quot;&gt;&lt;a href=&quot;#注意力模型&quot; class=&quot;headerlink&quot; title=&quot;注意力模型&quot;&gt;&lt;/a&gt;注意力模型&lt;/h1&gt;&lt;p&gt;&amp;emsp; &lt;strong&gt;注意力模型&lt;/strong&gt;(&lt;em&gt;Attention Model&lt;/em&gt;)被广泛使用在自然语言处理、图像识别及语音识别等各种不同类型的深度学习任务中，它的基本思想是仿照人类在做阅读文本、观察物体和听声音等工作时，会对特定的词语、物体、声音给予更多的关注，因而能更好地完成对语言、图像和声音的识别以及理解.&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Transformer</title>
    <link href="http://yoursite.com/2019/01/10/deep-learning/Transformer/"/>
    <id>http://yoursite.com/2019/01/10/deep-learning/Transformer/</id>
    <published>2019-01-10T10:33:16.532Z</published>
    <updated>2020-04-23T12:16:06.794Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>&emsp; 在Google的“Attention is all you need”论文中，在Sequence-to-Sequence的任务下，打破了以往Encoder-Decoder模型以RNN、CNN为代表的神经网络架构，对序列的建模摒弃了RNN的时序观点和CNN的结构观点，而是直接使用self-attention机制，提出了一种新的神经网络结构——Transformer.</p><a id="more"></a> <h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p>&emsp; Transformer的结构如下</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/transformer1.png?raw=true" alt="transformer1"></p><h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><p>&emsp; Self-Attention的好处是可以建立句子内部的联系，可以直接跨越多个词捕获长期依赖知识，而无需像RNN那样按照时序建模.</p><p>​    RNN对序列建模的结构如下：</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/transformer3.png?raw=true" alt="transformer3"></p><p>&emsp; Self-Attention对序列的建模如下：</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/transformer4.png?raw=true" alt="transformer4"></p><p>&emsp;在Attention机制中，我们对Source和Target进行Attention建模，如果把Source看作是存储器存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attetion值. 即把计算Attention值看作是一种软寻址，即根据Query和Key的相似性来决定取出对应Value的可能性.</p><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(QK^T)V</script><p>&emsp; 而Self-Attention则可看作是词自身为Query，整句都是<key,value>的软寻址. </key,value></p><h2 id="Self-Attention的变种"><a href="#Self-Attention的变种" class="headerlink" title="Self-Attention的变种"></a>Self-Attention的变种</h2><p>&emsp; Transformer里面的Self-Attention机制是一种新的变种，具体来说有两个方面，一个是加入了<strong>缩放因子</strong>(<em>Scaling Factor</em>)，另一方面，引入了<strong>多头注意力机制</strong>(<em>muti-head attention</em>).</p><p>&emsp; 具体地，Attention的计算公式中的分母多了一个向量的维度的平方根，为的是防止维度过大导致点乘结果过大，而进入SoftMax的饱和区，引起梯度饱和. 公式如下:</p><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>&emsp; 而多头机制指的是，引入多组的参数矩阵来分别做Self-Attention，最后将所有结果拼接起来，如下图所示</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/transformer2.png?raw=true" alt="transformer2"></p><p>左边是单层的Attention，而右边是多头机制的Attention.</p><p>公式如下：</p><script type="math/tex; mode=display">\begin{align}MultiHead(Q,K,V) &= Concat(head_1,...,head_h)W^O    \\Where \ head_i &= Attention(QW_i^Q,KW_i^Q,VW_i^Q)\end{align}</script><p>这样一来，模型有多套比较独立的Attention参数，有一定集成的效果，理论上可以增强模型的能力.</p><h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><p>&emsp; 由于Self-Attetion机制是一种扁平化的词袋的结构，导致不论距离多远的词，他们在Self-Attention中的距离都是1，这样的话，会丢失词之间的相对距离关系。导致像“牛 吃 草”、“草 吃 牛”在Selft-Attention下可能是一个意思.  因此，词向量中应尽可能地包含词的位置信息.</p><p>&emsp; Transformer的做法是，将词在句子中所处的位置映射为向量，补充到其Embedding当中.  具体来说，Transformer使用了一种非常新颖的时序建模方式——利用三角函数的周期性，来构建词之间的相对位置关系.</p><script type="math/tex; mode=display">\begin{align}PE(pos,2i) = sin(pos/10000^{2i/d_{model}})    \\PE(pos, 2i+1) = cos(pos/10000^{2i/d_{model}})\end{align}</script><p>其中$pos$是词的绝对位置，而$i$是词的维度下标，$d_{model}$是Embedding的维度，由上述公式可以看出，Transformer将绝对位置，做为三角函数中的变量，利用三角函数的周期性去表示词的相对位置.</p><h2 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h2><p>&emsp; 除了在Embedding时引入词的位置信息外，Transformer还采用一种”位置智能的前馈神经网络“，实际上是进行两次线性变换，一次ReLU激活，公式如下：</p><script type="math/tex; mode=display">FFN(x) = max(0,xW_1+b_1)W_2+b_2</script><p>这样一来，如果在不同位置的词具有相同的线性变换，再经过ReLU非线性激活和再一次的线性变换后，在更新参数时能得到不同的参数.</p><p>&lt;/br&gt;</p><p>&lt;/br&gt;</p><blockquote><p>参考资料</p><p>[1] 腾讯云+社区, “变形金刚”为何强大：从模型到代码全面解析Google Tensor2Tensor系统, <a href="https://segmentfault.com/a/1190000015575985#articleHeader6" target="_blank" rel="noopener">https://segmentfault.com/a/1190000015575985#articleHeader6</a>, 2018-07-09.</p><p>[2] Ashish Vaswani,Noam Shazeer,Niki Parmar,Jakob Uszkorei,Llion Jones,Aidan N. Gomez,Łukasz Kaise and Illia Polosukhin. Attention Is All You need. <em>arXiv preprint</em> <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">arXiv:1706.03762</a> , 2017.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Transformer&quot;&gt;&lt;a href=&quot;#Transformer&quot; class=&quot;headerlink&quot; title=&quot;Transformer&quot;&gt;&lt;/a&gt;Transformer&lt;/h1&gt;&lt;p&gt;&amp;emsp; 在Google的“Attention is all you need”论文中，在Sequence-to-Sequence的任务下，打破了以往Encoder-Decoder模型以RNN、CNN为代表的神经网络架构，对序列的建模摒弃了RNN的时序观点和CNN的结构观点，而是直接使用self-attention机制，提出了一种新的神经网络结构——Transformer.&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="注意力机制" scheme="http://yoursite.com/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    
  </entry>
  
  <entry>
    <title>LSTM</title>
    <link href="http://yoursite.com/2018/11/13/deep-learning/LSTM/"/>
    <id>http://yoursite.com/2018/11/13/deep-learning/LSTM/</id>
    <published>2018-11-13T13:49:00.576Z</published>
    <updated>2020-04-23T04:23:30.139Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>&emsp; <strong>LSTM</strong>(<em>Long Short Term Memory</em>)，是<strong>RNN</strong>的一种，通过门控机制，有选择地遗忘先前的状态和输出当前状态，可以很好地解决<strong>长期依赖</strong>(<em>Long-Term Dependencies</em>)的问题.</p><a id="more"></a><h2 id="长期依赖"><a href="#长期依赖" class="headerlink" title="长期依赖"></a>长期依赖</h2><p>&emsp; RNN的一个重要特性，就是可以对具有时序关系的数据进行很好地学习，但普通的RRN存在一个问题，在输入序列较长的情况下，处在靠后的位置往往很难得到靠前的输入的信息，如下图所示.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lstm-1.png" alt></p><h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p>&emsp; 首先先看普通的RNN，我们把从输入到输出中间的结构看作是一个“细胞”，那么普通的RNN的细胞中只有一个tanh函数，用来处理旧信息$c_t$和当前输入$X_t$.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lstm-2.png" alt></p><p>那么LSTM的结构是这样的，相比普通RNN，多了几个门的结构.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lstm-3png.png" alt></p><p>下面分块来解释各个门及其应用.</p><h3 id="遗忘门"><a href="#遗忘门" class="headerlink" title="遗忘门"></a>遗忘门</h3><p>&emsp; 首先我们需要遗忘门来控制我们需要丢弃哪些先前的信息，具体来说，遗忘门是一个Sigmoid函数，输出一个0~1之间的数$f_t$，“1”表示完全保留，“0”表示完全舍弃.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lstm-4.png" alt></p><h3 id="输入门"><a href="#输入门" class="headerlink" title="输入门"></a>输入门</h3><p>&emsp; 接着我们用输入门控制哪些信息被更新，并且作为新信息输入到下一个时间点. 具体地，和遗忘门一样，用一个Sigmoid函数输出保留信息的比率$i_t$.  另外，和普通的RRN一样，使用一个tanh函数计算生成的状态信息$\hat{C_t}$ .</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lstm-5.png" alt></p><p>这里我们把新信息和旧信息结合，成为最终的状态输出到下个时间点. 这里就用到了前面计算得出的两个保留信息的比率$f_t$和$i_t$，让它们分别乘上新旧信息$C_{t-1}$和$\hat{C_t}$，最后相加得到最终的状态信息$C_t$，并输出到下一时间点$t+1$.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lstm-6.png" alt></p><h3 id="输出门"><a href="#输出门" class="headerlink" title="输出门"></a>输出门</h3><p>&emsp; 最后我们需要输出门来控制最终输出的值，跟前面的思路基本一样，用一个Sigmoid函数输出0~1的值，然后用这个值乘上tanh函数包裹的状态$C_t$，便得到了此时间点$t$的输出$h_t$，并输出到下一时间点$t+1$.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lstm-7.png" alt></p><h2 id="Peephole-Connection"><a href="#Peephole-Connection" class="headerlink" title="Peephole Connection"></a>Peephole Connection</h2><p>&emsp; LSTM的一种比较流行的变体是增加了一种叫“Peephole Connection”的连接，即将细胞状态也输入到门中.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lstm-9.png" alt></p><h1 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h1><p>&emsp;<strong>GRU</strong>(<em>Gated Recurrent Unit</em>)是比较常见的LSTM的变体，其最大的改动是将遗忘门和输入门合成了单一的更新门，同时将原来的细胞状态信息$c_t$与$h_t$混合，成为单一的$h_t$，并且做了其他改动.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lstm-8.png" alt></p><p>&lt;/br&gt;</p><p>&lt;/br&gt;</p><blockquote><p>参考资料：</p><p>[1]. wangduo, <a href="https://www.cnblogs.com/wangduo/p/6773601.html" target="_blank" rel="noopener">[译] 理解 LSTM(Long Short-Term Memory, LSTM) 网络</a> , 2017-04-27.</p><p>[2]. colah, Understanding LSTM Networks, <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>  , 2015-08-27.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;LSTM&quot;&gt;&lt;a href=&quot;#LSTM&quot; class=&quot;headerlink&quot; title=&quot;LSTM&quot;&gt;&lt;/a&gt;LSTM&lt;/h1&gt;&lt;p&gt;&amp;emsp; &lt;strong&gt;LSTM&lt;/strong&gt;(&lt;em&gt;Long Short Term Memory&lt;/em&gt;)，是&lt;strong&gt;RNN&lt;/strong&gt;的一种，通过门控机制，有选择地遗忘先前的状态和输出当前状态，可以很好地解决&lt;strong&gt;长期依赖&lt;/strong&gt;(&lt;em&gt;Long-Term Dependencies&lt;/em&gt;)的问题.&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Dropout</title>
    <link href="http://yoursite.com/2018/11/13/deep-learning/dropout/"/>
    <id>http://yoursite.com/2018/11/13/deep-learning/dropout/</id>
    <published>2018-11-13T03:19:12.049Z</published>
    <updated>2020-04-23T11:05:35.602Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p>&emsp; Dropout是指在深度学习网络的训练过程中，按照一定概率暂时将神经元从网络中丢弃，这样一来，每一批的训练数据都在训练不同的网络.  实验证明，Dropout具有提高神经网络训练速度和防止过拟合的效果.</p><a id="more"></a><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><p>&emsp; 假设我们要训练如下的一个神经网络</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/dropout-1.png" alt></p><p>使用Dropout时，我们临时随机删除一些神经元，得到如下的子网络：</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/dropout-2.png" alt></p><p>使用反向传播算法，训练此子网络的参数，然后再重新从完整的神经网络，随机删除一些神经元，不断重复这一过程.</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>首先我们要设置一个参数p，代表的是神经网络中被随机删除的神经元比例.</p><h3 id="训练阶段"><a href="#训练阶段" class="headerlink" title="训练阶段"></a>训练阶段</h3><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/dropout-3.png" alt></p><ul><li><strong>Standard network</strong><script type="math/tex; mode=display">\begin{align}z_i^{(l+1)} &= w_i^{(l+1)}y^l+b_i^{(l+1)}    \\y_i^{(l+1)} &= f(z_i^{l+1})\end{align}</script></li></ul><ul><li><strong>Drop network</strong><script type="math/tex; mode=display">\begin{align}r_j^{(l)} &\sim Bernoulli(p)    \\\hat{y} &= r^{(l)}*y^{(l)}    \\z_i^{(l+1)} &= w_i^{(l+1)} \hat{y}^l + b_i^{(l+1)}    \\y_i^{(l+1)} &= f(z_i^{(l+1)})\end{align}</script></li></ul><h3 id="测试阶段"><a href="#测试阶段" class="headerlink" title="测试阶段"></a>测试阶段</h3><p>&emsp; 测试时，每一个单元的参数都要乘以p</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/dropout-4.png" alt></p><p>即</p><script type="math/tex; mode=display">w_{test}^{(l)} = pW^{(l)}</script><h2 id="缓解过拟合的原因"><a href="#缓解过拟合的原因" class="headerlink" title="缓解过拟合的原因"></a>缓解过拟合的原因</h2><p>&emsp; Dropout的原理很简单直接，但为什么Dropout能够缓解神经网络中过拟合的问题呢？</p><ul><li><p>集成(Ensemble)的思想</p><p>&emsp; 如果把Dropout后的每个子网络看成是一个学习器，那么就可以把Dropout看作是Bagging的一种近似，且消耗更少的内存和运算力.  </p></li><li><p>神经元共享的集成</p><p>&emsp; Dropout不仅仅是训练一个Bagging的集成模型，而且是共享神经元的集成模型.  这意味着，无论其他神经元是否在模型中，每个神经元都必须表现良好.  这样一来，每一个神经元在很多情况下都能表现良好，也就提高了神经网络的泛化能力.</p></li><li><p>引入噪声</p><p>&emsp; 我们可以认为Dropout实际上是对神经元施加了噪声扰动，增强了神经网络的学习能力和鲁棒性. 传统地，我们为了在神经网络中引入噪声，会对输入的原始值进行破坏，而Dropout直接作用于神经元，相当于是屏蔽掉一部分的输入，这比原始的方法更加智能和高效.</p></li></ul><p>&lt;/br&gt;</p><p>&lt;/br&gt;</p><blockquote><p>参考资料：</p><p>[1]  Micorstrong0305.  深度学习中Dropout原理解析. <a href="https://blog.csdn.net/program_developer/article/details/80737724" target="_blank" rel="noopener">https://blog.csdn.net/program_developer/article/details/80737724</a></p><p>[2]  Srivastava N, Hinton G, Krizhevsky A, et al. Dropout: A simple way to prevent neural networks from overfitting[J]. The Journal of Machine Learning Research, 2014, 15(1): 1929-1958.</p><p>[3] Dropout as data augmentation. <a href="http://arxiv.org/abs/1506.08700" target="_blank" rel="noopener">http://arxiv.org/abs/1506.08700</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Dropout&quot;&gt;&lt;a href=&quot;#Dropout&quot; class=&quot;headerlink&quot; title=&quot;Dropout&quot;&gt;&lt;/a&gt;Dropout&lt;/h1&gt;&lt;p&gt;&amp;emsp; Dropout是指在深度学习网络的训练过程中，按照一定概率暂时将神经元从网络中丢弃，这样一来，每一批的训练数据都在训练不同的网络.  实验证明，Dropout具有提高神经网络训练速度和防止过拟合的效果.&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>批标准化</title>
    <link href="http://yoursite.com/2018/11/13/deep-learning/batch-normalization/"/>
    <id>http://yoursite.com/2018/11/13/deep-learning/batch-normalization/</id>
    <published>2018-11-13T00:14:30.646Z</published>
    <updated>2020-04-23T04:33:29.766Z</updated>
    
    <content type="html"><![CDATA[<h1 id="批标准化-Batch-Normalization"><a href="#批标准化-Batch-Normalization" class="headerlink" title="批标准化(Batch Normalization)"></a>批标准化(Batch Normalization)</h1><p>&emsp; <strong>批标准化</strong>(<em>Batch Normalization</em>)，是解决随着网络深度加深，训练困难的问题的一种方法，一般用在激活函数之前，其基本思想是把每层的输入值的分布拉回到标准正态分布当中，使得每一层中，每一次的输入独立同分布，使得神经网络可以很好地学习数据；针对于非线性激活函数，还能将处于饱和区的值，拉回到梯度较大的区域，一定程度上解决梯度消失的问题.</p><a id="more"></a><h2 id="独立同分布"><a href="#独立同分布" class="headerlink" title="独立同分布"></a>独立同分布</h2><p>&emsp; 机器学习有一个重要的假设：<strong>独立同分布</strong>(<em>Independently and Identically Distributed, IID</em>)。即假设训练数据和测试数据是满足相同分布的，它是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障. </p><p>&emsp; 如果将神经网络的每一层都看作是一次的输入和输出，由于隐藏层之间存在非线性映射的关系，那么显然难以保证每一层中每一次的输入独立同分布，并且随着网络的加深，这种情况会越来越糟糕.</p><p>&emsp; 而批标准化可以很好地解决这一问题，具体来说，批标准化将输入变换到均值为0、方差为1的正态分布中，这样一来就实现了输入的同分布.</p><h2 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h2><p>&emsp; 深度神经网络中，非线性激活函数往往会遇到一个问题，梯度消失.  以Logistic函数为例，</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/logistic.jpg" alt="boy"></p><p>如图，当输入值太大或太小的时候，函数值容易进入饱和区，这个时候斜率几乎为0，这样在反向传播的时候，梯度显然是无法继续传递的，也就导致了前面几层的参数无法更新.</p><p>&emsp; 批标准化将输入标准化为均值为0、方差为1的正态分布，如下图所示.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/01.png" alt></p><p>可以看到，在此正态分布中，数据落在[-1,1]的概率为68%，落在[-2,2]的概率为95%，结合Logisitc函数的图像来看，批标准化后的数据大部分都会落在斜率较大的区间内，这样一来，梯度消失的问题就得到了很好的解决.</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>&emsp; 在神经网络中，往往是在激活函数之前加入对数据的批标准化操作，如下图所示.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/bn.png" alt></p><p>BN算法的核心即将数据进行正态分布的变换，即</p><script type="math/tex; mode=display">\hat{x}^{(k)} = \frac{x^{(k)-E[x^{(k)}]}}{\sqrt{Var[x^{(k)}]}}</script><p>但这也会导致网络的表达能力变差，于是引入两个调节参数，可以通过训练来学习，对变换后的值进行修正.</p><script type="math/tex; mode=display">y^{(k)} = \gamma^{(k)}\hat{x}^{(k)}+\beta^{(k)}</script><p>具体算法如下所示</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/bn-1.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>深度神经网络使用批标准化有如下好处：</p><ol><li>使得隐藏层的每一次输入同分布，模型能更好地从数据中学习；</li><li>提升了训练速度，收敛过程加快；</li><li>一定程度上缓解了过拟合问题；</li><li>使得神经网络对初始化的要求降低，可以使用较大的学习率等；</li></ol><p>&lt;/br&gt;</p><p>&lt;/br&gt;</p><blockquote><p>参考资料</p><p>[1] 郭耀华 <a href="https://www.cnblogs.com/guoyaohua/p/8724433.html" target="_blank" rel="noopener">深入理解Batch Normalization批标准化</a> , 2018-04-05.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;批标准化-Batch-Normalization&quot;&gt;&lt;a href=&quot;#批标准化-Batch-Normalization&quot; class=&quot;headerlink&quot; title=&quot;批标准化(Batch Normalization)&quot;&gt;&lt;/a&gt;批标准化(Batch Normalization)&lt;/h1&gt;&lt;p&gt;&amp;emsp; &lt;strong&gt;批标准化&lt;/strong&gt;(&lt;em&gt;Batch Normalization&lt;/em&gt;)，是解决随着网络深度加深，训练困难的问题的一种方法，一般用在激活函数之前，其基本思想是把每层的输入值的分布拉回到标准正态分布当中，使得每一层中，每一次的输入独立同分布，使得神经网络可以很好地学习数据；针对于非线性激活函数，还能将处于饱和区的值，拉回到梯度较大的区域，一定程度上解决梯度消失的问题.&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>循环神经网络</title>
    <link href="http://yoursite.com/2018/10/08/deep-learning/recurrent-neural-network/"/>
    <id>http://yoursite.com/2018/10/08/deep-learning/recurrent-neural-network/</id>
    <published>2018-10-08T14:47:11.163Z</published>
    <updated>2018-12-03T16:04:28.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;循环神经网络&quot;&gt;&lt;a href=&quot;#循环神经网络&quot; class=&quot;headerlink&quot; title=&quot;循环神经网络&quot;&gt;&lt;/a&gt;循环神经网络&lt;/h1&gt;&lt;h2 id=&quot;结构&quot;&gt;&lt;a href=&quot;#结构&quot; class=&quot;headerlink&quot; title=&quot;结构&quot;&gt;&lt;
      
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络</title>
    <link href="http://yoursite.com/2018/10/08/deep-learning/convolutional-neural-network/"/>
    <id>http://yoursite.com/2018/10/08/deep-learning/convolutional-neural-network/</id>
    <published>2018-10-08T14:45:42.623Z</published>
    <updated>2018-12-03T16:04:28.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><h2 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;卷积神经网络&quot;&gt;&lt;a href=&quot;#卷积神经网络&quot; class=&quot;headerlink&quot; title=&quot;卷积神经网络&quot;&gt;&lt;/a&gt;卷积神经网络&lt;/h1&gt;&lt;h2 id=&quot;卷积&quot;&gt;&lt;a href=&quot;#卷积&quot; class=&quot;headerlink&quot; title=&quot;卷积&quot;&gt;&lt;
      
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="计算机视觉" scheme="http://yoursite.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>神经网络</title>
    <link href="http://yoursite.com/2018/10/08/deep-learning/neural-network/"/>
    <id>http://yoursite.com/2018/10/08/deep-learning/neural-network/</id>
    <published>2018-10-08T10:35:46.825Z</published>
    <updated>2020-04-23T04:24:34.855Z</updated>
    
    <content type="html"><![CDATA[<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><p>&emsp; <strong>神经网络</strong>(<em>neural network</em>)，是<strong>深度学习</strong>(<em>deep learning</em>)领域的基础和核心工具，是一种模仿生物神经网络的结构和功能的机器学习模型。神经网络由大量的人工神经元(neron)组成，基本思想是神经元超过一个<strong>阈值</strong>(<em>threshold</em>)后被激活，输出值传到下一神经元，一层层传播，最终输出结果。</p><a id="more"></a><h2 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h2><p>&emsp; 神经网络的基本结构如下图所示，由输入层、隐藏层和输出层组成。</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com//neural-network1.png" alt="图片来源维基百科"></p><h2 id="隐藏单元"><a href="#隐藏单元" class="headerlink" title="隐藏单元"></a>隐藏单元</h2><p>&emsp; 在神经网络中，我们把隐藏层的神经元称为隐藏单元，其作用是对输入的特征进行非线性变换，将原有的特征空间拓展到更高维度，使原来线性不可分的样本点线性可分。</p><p>&emsp; 由于隐藏单元要对输入值做非线性运算，因此隐藏单元中的激活函数必须是非线性的。常见的激活函数有：Sigmoid、Tanh和ReLU。</p><h3 id="Logistic-Sigmoid"><a href="#Logistic-Sigmoid" class="headerlink" title="Logistic Sigmoid"></a>Logistic Sigmoid</h3><p>函数式如下：</p><script type="math/tex; mode=display">f(z) = \frac{1}{1+exp{(-z)}}</script><p>使用Logistic Sigmoid函数作为神经网络的激活函数，会有如下几个问题：</p><ul><li>计算量大，反向传播求误差梯度时，求导涉及除法；</li><li>函数值易饱和，反向传播时，容易出现<strong>梯度消失</strong>的情况；</li></ul><h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p>函数式如下：</p><script type="math/tex; mode=display">f(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}}</script><p>与Logistic Sigmoid函数相比，Tanh函数有如下特点：</p><ul><li>同样存在计算量大的问题；</li><li>同样容易出现梯度消失的情况，但延迟了函数值的饱和期；</li><li>在0附近与单位函数类似，在激活保持地很小的情况下，类似于训练一个线性模型，使得训练神经网络更容易；</li></ul><h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p>ReLU的函数式很简单：</p><script type="math/tex; mode=display">f(z) = max(0,z)</script><ul><li>没有函数值饱和导致的梯度消失的问题；</li><li>只需要设置阈值就可以得到激活值，运算量小；</li><li>可以较好地模拟生物神经网络中，部分神经元未处于激活状态的情景；</li><li>可能出现神经元”大面积死亡”的情况；</li></ul><p><strong>现如今的深度神经网络多使用ReLU作为隐藏单元。</strong></p><h2 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h2><p>&emsp; 跟其他机器学习算法一样，神经网络同样涉及到模型训练的问题。训练一个神经网络，最常用的算法是<strong>反向传播算法</strong>(<em>back propagation algorithm</em>)，经常简称为<strong>backprop</strong>或<strong>BP算法</strong>。</p><p>&emsp; 其基本原理是复合函数求导的<strong>链式法则</strong>，具体地，我们首先对神经网络进行<strong>前向传播</strong>，即从输入值传播到输出值，然后计算误差，再将误差梯度<strong>后向传播</strong>，更新参数。</p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>&emsp; 在机器学习中，我们使用许多策略来减少模型在测试集上的误差，即使这些策略可能会增大训练误差，这些策略统称为<strong>正则化</strong>(<em>normalization</em>)。实际上，正则化就是希望显著地减少<strong>方差</strong>(<em>varience</em>)而不过度增加<strong>偏差</strong>(<em>bias</em>)的策略。</p><h3 id="参数范数惩罚"><a href="#参数范数惩罚" class="headerlink" title="参数范数惩罚"></a>参数范数惩罚</h3><h4 id="L2参数正则化"><a href="#L2参数正则化" class="headerlink" title="L2参数正则化"></a>L2参数正则化</h4><p>&emsp; L2正则化策略，通过向目标函数添加一个正则项 $\Omega(\theta) = \frac{1}{2}||w||_2^2$ ，使权值更加接近原点。与线性回归中添加L2正则项一样，神经网络使用L2正则化，目的是通过允许一定的训练误差，来防止过拟合。</p><h4 id="L1参数正则化"><a href="#L1参数正则化" class="headerlink" title="L1参数正则化"></a>L1参数正则化</h4><p>&emsp; 与L2正则化类似，L1正则化也是通过向目标函数添加一个正则项 $\Omega(\theta)=||w||_1=\sum_i |w_i|$ ，来使得权重进行一定程度的衰减。不同的是，L1正则化回产生更<strong>稀疏</strong>(<em>sparse</em>)的解，即最终会得到一些权重为0的神经网络，因而对<strong>特征选择</strong>(<em>feature selection</em>)有一定的作用。</p><h3 id="数据集增强"><a href="#数据集增强" class="headerlink" title="数据集增强"></a>数据集增强</h3><p>&emsp; 与其他的机器学习算法一样，提高神经网络的泛化能力的最好办法是使用更多的数据进行训练，然而，在实际情况中，我们所拥有的数据量是有限的。一种解决方案是创造一些数据添加到训练集中。</p><p>&emsp; 在计算机视觉中，图像是高维的并且包含着各种巨大的变化因素，我们可以模拟其中的一些，像是通过放大缩小，移动像素，来模拟现实中同一物体在不同位置的情景。</p><p>&emsp; 同时，为了改善神经网络对噪声的健壮性，可以<strong>将随机噪声添加到输入</strong>中进行训练，使得训练出来的模型在具有一定噪声的情况下也能很好地完成任务。</p><p>&emsp; 对于某些模型而言，向输入添加方差极小的噪声等价于对权重施加范数惩罚，而一般情况下，注入噪声远比简单地收缩参数要更强大。于是将<strong>噪声添加到权重</strong>中也是一种正则化的手段，这项技术主要用于<strong>循环神经网络(RNN)</strong>。实际上，这种做法是对权重添加了随机扰动，可以反映贝叶斯学习过程的权重不确定性。</p><p>&emsp; 在现实的应用场景中，大多数的数据集的标签都存在一定错误，错误的标签是不利于最大化似然的，于是，我们可以通过显式地对标签上的噪声进行建模。例如假设一个小常数 $\epsilon $ ，训练集标签正确的概率是 $1-\epsilon$ ，其他的可能标签也可能是正确的，这样就不用显式地将噪声样本抽取出来。</p><h3 id="提前终止"><a href="#提前终止" class="headerlink" title="提前终止"></a>提前终止</h3><p>&emsp; 当我们训练一个有足够表示能力的模型时，往往可以观察到，训练误差会随着时间的推移逐渐降低，但验证集的误差会再次上升。这意味着，我们应当在验证集误差有所改善时，存储训练好的模型，而当验证集误差在一定循环次数内没有改善时，我们应当终止训练。这种策略被称为<strong>提前终止</strong>(<em>early stopping</em>)。</p><p>&lt;/br&gt;</p><blockquote><p>参考资料：</p><p>[1] 深度学习[M], 北京: 人民邮电出版社, 2017: 105-156.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;神经网络&quot;&gt;&lt;a href=&quot;#神经网络&quot; class=&quot;headerlink&quot; title=&quot;神经网络&quot;&gt;&lt;/a&gt;神经网络&lt;/h1&gt;&lt;p&gt;&amp;emsp; &lt;strong&gt;神经网络&lt;/strong&gt;(&lt;em&gt;neural network&lt;/em&gt;)，是&lt;strong&gt;深度学习&lt;/strong&gt;(&lt;em&gt;deep learning&lt;/em&gt;)领域的基础和核心工具，是一种模仿生物神经网络的结构和功能的机器学习模型。神经网络由大量的人工神经元(neron)组成，基本思想是神经元超过一个&lt;strong&gt;阈值&lt;/strong&gt;(&lt;em&gt;threshold&lt;/em&gt;)后被激活，输出值传到下一神经元，一层层传播，最终输出结果。&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Word2Vec</title>
    <link href="http://yoursite.com/2018/09/09/natural-language-processing/Word2Vec/"/>
    <id>http://yoursite.com/2018/09/09/natural-language-processing/Word2Vec/</id>
    <published>2018-09-09T11:19:59.257Z</published>
    <updated>2020-04-23T04:55:11.492Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><p>&emsp; Word2Vec是一种以<strong>无监督</strong>的方式来得到词向量的模型，其核心思想是通过上下文信息表达一个词语，将词语从原先的空间映射到新的空间中(本质上是一种降维)。这样得到的词向量相对于<strong>独热编码</strong>具有维度较低、词语之间相似度易衡量等优点.</p><p>&emsp; 具体地，Word2Vec首先将词语进行独热编码，然后将编码后的词向量输入神经网络，通过最小化误差来更新权值矩阵，最后将训练好的权值矩阵作为处理后的词向量矩阵. </p><a id="more"></a><p>&emsp; Word2Vec主要有CBOW、Skip-gram两种模型.  CBOW是根据上下文词语预测当前词；而Skip-Gram正好相反，根据当前词预测上下文. 如下图所示:   </p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/word2vec1.jpg" alt="CBOW and Skip-Gram"></p><p>对于CBOW，我们的目标函数为：</p><script type="math/tex; mode=display">L = \sum_{w\in C} \log p(w|context(w))</script><p>而对于Skip-gram，目标函数为：</p><script type="math/tex; mode=display">L = \sum_{w\in C}\log p(context(w)|w)</script><p>&emsp; Word2Vec有两个trick，分别是Hierarchical Softmax 和 Nagative Sampling，他们是Word2Vec提高训练效率和效果的两种技巧(trick).</p><h2 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h2><h2 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h2><h2 id="Nagative-Sampling"><a href="#Nagative-Sampling" class="headerlink" title="Nagative Sampling"></a>Nagative Sampling</h2>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Word2Vec&quot;&gt;&lt;a href=&quot;#Word2Vec&quot; class=&quot;headerlink&quot; title=&quot;Word2Vec&quot;&gt;&lt;/a&gt;Word2Vec&lt;/h1&gt;&lt;p&gt;&amp;emsp; Word2Vec是一种以&lt;strong&gt;无监督&lt;/strong&gt;的方式来得到词向量的模型，其核心思想是通过上下文信息表达一个词语，将词语从原先的空间映射到新的空间中(本质上是一种降维)。这样得到的词向量相对于&lt;strong&gt;独热编码&lt;/strong&gt;具有维度较低、词语之间相似度易衡量等优点.&lt;/p&gt;
&lt;p&gt;&amp;emsp; 具体地，Word2Vec首先将词语进行独热编码，然后将编码后的词向量输入神经网络，通过最小化误差来更新权值矩阵，最后将训练好的权值矩阵作为处理后的词向量矩阵. &lt;/p&gt;
    
    </summary>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>特征选择</title>
    <link href="http://yoursite.com/2018/08/10/feature-selection/"/>
    <id>http://yoursite.com/2018/08/10/feature-selection/</id>
    <published>2018-08-10T12:31:18.430Z</published>
    <updated>2018-12-03T16:04:28.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><p>&emsp; 特征选择是数据预处理中非常重要的技术，具体来说，在一般的场景下，数据的特征往往很多，会出现数据样本稀疏、距离计算困难等问题，称为<strong>维数灾难</strong>(<em>curse of dimensionality</em>).  另外，去除那些不那么重要的特征，能使得重要的特征对结果的预测有更好的作用.  因此，特征的选择就变得非常重要.  具体的，在特征选择中，我们的目标是去除那些与预测结果相关性小的特征，而保留那些相关性大的特征.</p><a id="more"></a><h2 id="过滤式"><a href="#过滤式" class="headerlink" title="过滤式"></a>过滤式</h2><h2 id="包裹式"><a href="#包裹式" class="headerlink" title="包裹式"></a>包裹式</h2><h2 id="嵌入式"><a href="#嵌入式" class="headerlink" title="嵌入式"></a>嵌入式</h2>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;特征选择&quot;&gt;&lt;a href=&quot;#特征选择&quot; class=&quot;headerlink&quot; title=&quot;特征选择&quot;&gt;&lt;/a&gt;特征选择&lt;/h1&gt;&lt;p&gt;&amp;emsp; 特征选择是数据预处理中非常重要的技术，具体来说，在一般的场景下，数据的特征往往很多，会出现数据样本稀疏、距离计算困难等问题，称为&lt;strong&gt;维数灾难&lt;/strong&gt;(&lt;em&gt;curse of dimensionality&lt;/em&gt;).  另外，去除那些不那么重要的特征，能使得重要的特征对结果的预测有更好的作用.  因此，特征的选择就变得非常重要.  具体的，在特征选择中，我们的目标是去除那些与预测结果相关性小的特征，而保留那些相关性大的特征.&lt;/p&gt;
    
    </summary>
    
    
      <category term="特征工程" scheme="http://yoursite.com/categories/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
    
      <category term="特征选择" scheme="http://yoursite.com/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>线性判别分析</title>
    <link href="http://yoursite.com/2018/08/10/descend-dimension/LDA/"/>
    <id>http://yoursite.com/2018/08/10/descend-dimension/LDA/</id>
    <published>2018-08-10T01:53:45.379Z</published>
    <updated>2020-04-23T04:38:41.301Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性判别分析-LDA"><a href="#线性判别分析-LDA" class="headerlink" title="线性判别分析(LDA)"></a>线性判别分析(LDA)</h1><p>&emsp; <strong>线性判别分析</strong>(<em>Linear Discriminant Analysis</em>，简称LDA)，是一种有监督的降维方法.  其思想是希望投影到超平面上的点具有如下性质：</p><ul><li>同类的样本点尽可能地接近</li><li>不同类的样本点尽可能远离</li></ul><p>如下图所示，右图的降维效果显然要好于左图.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lda1.png" alt="图片来源：博客园刘建平"></p><a id="more"></a><blockquote><p>(图片来源于博客园的刘建平大神，文末分享链接)</p></blockquote><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><h3 id="二分类情况"><a href="#二分类情况" class="headerlink" title="二分类情况"></a>二分类情况</h3><p>&emsp; 首先，我们从二分类的情况来解释原理。给定 $n$ 维数据集 $D= \{(x_i,y_i)\}^m_{i=1},  y_i \in \{0,1\}$, 令 $X_i、\mu_i、\Sigma_i$ 分别表示第 $i\in \{0,1\}$ 类的样本的集合、均值向量、协方差矩阵. </p><p>&emsp; 在二分类的情况下，我们将 $n$ 维数据压缩到 $1$ 维，即数据投影到直线 $w$ 上，则两类样本的中心在直线上的投影分别为：$w^T\mu_0 $ 和 $w^T\mu_1$ ; 两类样本的协方差分别为：$w^T\Sigma_0 w$ 和 $w^T \Sigma_1 w$ . 显然，在一维空间中，它们都是一个实数.</p><p>&emsp; 回到开头我们讲LDA的两个目标：</p><ul><li><strong>同类的样本点尽可能地接近，则同类样本的投影点的协方差尽可能小，即 $w^T\Sigma_0w+w^T\Sigma_1w$ 尽可能小；</strong></li><li><strong>不同类样本点尽可能远离，则不同类的中心之间的距离尽可能大，即 $||w^T\mu_0-w^T\mu_1||_2^2$ 尽可能大.</strong></li></ul><p>同时考虑二者，我们就可以得到要最大化的目标：</p><script type="math/tex; mode=display">\begin{align}J &= \frac{||w^T\mu_0-w^T\mu_1||_2^2}{w^T\Sigma_0w+w^T\Sigma_1w}    \\&= \frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T(\Sigma_0+\Sigma_1)w}\end{align}    \tag{1}</script><p>我们给出 <strong>类内散度矩阵</strong>(<em>within-class scatter matrix</em>) 的定义</p><script type="math/tex; mode=display">\begin{align}S_w &= \Sigma_0 + \Sigma_1     \\&= \sum_{x\in X_0}(x-\mu_0)(x-u_0)^T+ \sum_{x\in X_1}(x-\mu_1)(x-\mu_1)^T\end{align}    \tag{2}</script><p>以及 <strong>类间散度矩阵</strong>(<em>between-class scatter matrix</em>) 的定义</p><script type="math/tex; mode=display">S_b =(\mu_0 -\mu_1)(\mu_0-\mu_1)^T    \tag{3}</script><p>于是 式$(1)$ 可以重写为</p><script type="math/tex; mode=display">J = \frac{w^TS_bw}{w^TS_w w}        \tag{4}</script><p>这个就是 LDA 要最大化的目标函数，即 $S_b$ 与 $S_w$ 的 <strong>广义瑞利商</strong>(<em>generalized Rayleigh quotient</em>) .</p><p>&emsp; 注意到式$(4)$的分子分母都是关于 $w $ 的二次项，因此式$(4)$的解与 $w$ 的长度无关，只与其方向有关.  于是，我们可以令 $w^TS_ww=1$ ，可得</p><script type="math/tex; mode=display">\begin{align}\min_w& \quad -w^TS_bw    \\s.t.& \quad w^TS_ww =1\end{align}    \tag{5}</script><p>应用拉格朗日乘子法，上式等价于</p><script type="math/tex; mode=display">S_bw = \lambda S_ww    \tag{6}</script><p>即</p><script type="math/tex; mode=display">S_w^{-1}S_b w = \lambda w    \tag{7}</script><p>这正好是<strong>特征值分解</strong>的形式，因此我们可以对 $S_w^{-1}S_b$ 特征值分解，得到的最大的特征值对应的特征向量即为降维后的投影方向 $w$ .  同理，如果是降到 $d$ 维的情况，只需选取最大的 $d$ 个特征值对应的特征向量张成投影矩阵即可.</p><p>另外，对于二类的情况，注意到 $S_bw$ 的方向恒为 $\mu_0 -\mu_1$,不妨令</p><script type="math/tex; mode=display">S_bw = \lambda(\mu_0-\mu_1)    \tag{8}</script><p>代入式$(6)$可得</p><script type="math/tex; mode=display">w = S^{-1}(\mu_0-\mu_1)    \tag{9}</script><p>即为最佳的投影方向.</p><h3 id="多分类情况"><a href="#多分类情况" class="headerlink" title="多分类情况"></a>多分类情况</h3><p>&emsp; 设$W=(w_1,w_2,…,w_d)$，即我们将数据压缩到 $d$ 维，$X_j$ 是第 $i$ 类样本的集合，$N_j$ 是第 $ j$ 类样本的个数，将式$(4)$推广到多分类情形时，有</p><script type="math/tex; mode=display">J = \frac{W^TS_bW}{W^TS_w W}    \tag{10}</script><p>其中，$S_b = \sum^k_{j=1}N_j(\mu_j-\mu)(\mu_j-\mu)^T$，$\mu$ 为所有样本的均值向量，$S_w = \sum^k_{j=1}S_{wj}=\sum^k_{j=1}\sum_{x\in X_j}(x-\mu_j)(x-\mu_j)^T$ .</p><p>&emsp; 问题在于，式$(10)$的分子和分母都是矩阵，而不是标量，没有办法作为一个标量函数来优化.  而针对这个问题，常见的一种方法是采用优化目标</p><script type="math/tex; mode=display">\arg\max_W\frac{tr(W^TS_bW)}{tr(W^TS_w W)}    \tag{11}</script><p>其中 $tr(·)$ 表示矩阵的<strong>迹</strong>(<em>trace</em>)，即<strong>主对角线元素的乘积</strong>. 于是，我们可以通过如下问题求解</p><script type="math/tex; mode=display">S_bW = \lambda S_w W    \tag{12}</script><p>正好和二类中的式(6)形式一样，因此我们也是同样采用对 $S_W^{-1}S_b$ 求特征值的方法，<strong>选取最大的 $d$ 个特征值对应的特征向量，把它们张成的矩阵作为降维的投影矩阵 $W$.</strong></p><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><blockquote><p>输入：数据集 $D=\{(x_1,y_1),(x_2,y_2),…,(x_m,y_m)\} $，其中任意样本 $x_i$ 为 $n$ 维向量，$y_i \in \{C_1,C_2,…,C_k\}$，降维后的维度 $d$.</p><ol><li><p>计算类内散度矩阵</p><script type="math/tex; mode=display">S_w = \sum^k_{j=1}S_{wj}=\sum^k_{j=1}\sum_{x\in X_j}(x-\mu_j)(x-\mu_j)^T</script></li><li><p>计算类间散度矩阵</p></li></ol><script type="math/tex; mode=display">S_b = \sum^k_{j=1}N_j(\mu_j-\mu)(\mu_j-\mu)^T</script><ol><li><p>计算矩阵</p><script type="math/tex; mode=display">S_w^{-1}S_b</script></li><li><p>对矩阵 $S^{-1}_wS_b$ 进行特征值分解，选取特征值最大的 $d$ 个特征值对应的特征向量，张成投影矩阵 $W =(w_1,w_2,…,w_d)$</p></li><li><p><strong>for</strong> i =1,2,…m <strong>do</strong></p><script type="math/tex; mode=display">z_i = W^Tx_i</script><p><strong>end for</strong></p></li><li><p>得到输出样本集 $D^{‘} = \{(z_1,y_1),(z_2,y_2),…,(z_m,y_m)\}$ .</p></li></ol><p>输出：降维后的样本集 $D^{‘}$ .</p></blockquote><p>&lt;/br&gt;</p><blockquote><p>参考资料：</p><p>[1] 刘建平Pinard. 博客园: 线性判别分析LDA原理总结, <a href="https://www.cnblogs.com/pinard/p/6244265.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6244265.html</a> , 2017-01-03/2018-08-10 .</p><p>[2] 周志华. 机器学习[M], 北京: 清华大学出版社, 2016: 60-63. </p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;线性判别分析-LDA&quot;&gt;&lt;a href=&quot;#线性判别分析-LDA&quot; class=&quot;headerlink&quot; title=&quot;线性判别分析(LDA)&quot;&gt;&lt;/a&gt;线性判别分析(LDA)&lt;/h1&gt;&lt;p&gt;&amp;emsp; &lt;strong&gt;线性判别分析&lt;/strong&gt;(&lt;em&gt;Linear Discriminant Analysis&lt;/em&gt;，简称LDA)，是一种有监督的降维方法.  其思想是希望投影到超平面上的点具有如下性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;同类的样本点尽可能地接近&lt;/li&gt;
&lt;li&gt;不同类的样本点尽可能远离&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如下图所示，右图的降维效果显然要好于左图.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lda1.png&quot; alt=&quot;图片来源：博客园刘建平&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="特征工程" scheme="http://yoursite.com/categories/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="数据降维" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E9%99%8D%E7%BB%B4/"/>
    
      <category term="线性判别分析" scheme="http://yoursite.com/tags/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
</feed>
