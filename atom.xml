<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>One Blog</title>
  
  <subtitle>要做爱 &lt;br&gt; 学习的好孩子</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-04-24T11:51:58.089Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Yao-zz</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Latex 笔记</title>
    <link href="http://yoursite.com/2020/04/24/Latex-Note/"/>
    <id>http://yoursite.com/2020/04/24/Latex-Note/</id>
    <published>2020-04-24T11:59:11.034Z</published>
    <updated>2020-04-24T11:51:58.089Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Latex-Note"><a href="#Latex-Note" class="headerlink" title="Latex Note"></a>Latex Note</h1><p>​    一些简单的latex笔记，发现记录一下还是很有必要的，以免写论文的时候又忘了，到处去找，费时费神。</p><a id="more"></a><h2 id="Font"><a href="#Font" class="headerlink" title="Font"></a>Font</h2><h3 id="字体大小"><a href="#字体大小" class="headerlink" title="字体大小"></a>字体大小</h3><p>全局字体大小</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\documentclass&#123;article&#125;[12pt]</span><br></pre></td></tr></table></figure><p>局部字体大小</p><blockquote><p>\tiny<br>\scriptsize<br>\footnotesize<br>\small<br>\normalsize<br>\large<br>\Large<br>\LARGE<br>\huge<br>\Huge</p></blockquote><p>加粗、斜体、下划线</p><blockquote><p>显示直立文本： \textup{文本}</p><p>意大利斜体： \textit{文本}</p><p>slanted斜体： \textsl{文本}</p><p>显示小体大写文本： \textsc{文本}</p><p>中等权重： \textmd{文本}</p><p>加粗命令： \textbf{文本}</p><p>默认值： \textnormal{文本}</p><p>斜体字：\textit{italic}，或者 \emph{italic}</p><p>细体字：\textlf{light font}</p><p>使用等宽字体：\texttt{code}</p><p>使用无衬线字体：\textsf{sans-serif}</p><p>所有字母大写：\uppercase{CAPITALS}</p><p>所有字母大写，但小写字母比较小：\textsc{Small Capitals}</p><p>下划线：\underline{text}</p></blockquote><h2 id="Color"><a href="#Color" class="headerlink" title="Color"></a>Color</h2><p>导入包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\usepackage[table, dvipsnames]&#123;xcolor&#125;</span><br></pre></td></tr></table></figure><p>文本颜色</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\textcolor&#123;color&#125;&#123;text&#125;</span><br></pre></td></tr></table></figure><p>表格填充</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\rowcolor&#123;color&#125;</span><br></pre></td></tr></table></figure><p><img src="C:\Users\uni_c\AppData\Roaming\Typora\typora-user-images\image-20200212143319726.png" alt="image-20200212143319726"></p><p>自定义颜色</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">\definecolor&#123;rgb&#125;&#123;r, g, b&#125;</span><br></pre></td></tr></table></figure><p><a href="http://latexcolor.com/" target="_blank" rel="noopener">http://latexcolor.com/</a></p><h2 id="Page"><a href="#Page" class="headerlink" title="Page"></a>Page</h2><p>导入包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">\usepackage&#123;fancyhdr&#125;</span><br><span class="line">\usepackage&#123;lastpage&#125;</span><br></pre></td></tr></table></figure><p>页面格式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">% 页眉页脚</span><br><span class="line">\pagestyle&#123;fancy&#125;</span><br><span class="line"></span><br><span class="line">% 无页眉页脚等</span><br><span class="line">\pagestyle&#123;empty&#125;</span><br></pre></td></tr></table></figure><p>页眉、页脚</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">% 页眉左上角</span><br><span class="line">\lhead&#123;left head note&#125;</span><br><span class="line"></span><br><span class="line">% 页眉右上角</span><br><span class="line">\rhead&#123;Page \thepage\ of\ \pageref&#123;LastPage&#125;&#125;</span><br><span class="line"></span><br><span class="line">% 页脚中部</span><br><span class="line">\cnote&#123;&#125;</span><br></pre></td></tr></table></figure><h2 id="Table-Of-Content"><a href="#Table-Of-Content" class="headerlink" title="Table Of Content"></a>Table Of Content</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">\setcounter&#123;tocdepth&#125;&#123;3&#125; % the depth of toc</span><br><span class="line">% 标题居中</span><br><span class="line">\begin&#123;center&#125;</span><br><span class="line">    \tableofcontents</span><br><span class="line">\end&#123;center&#125;</span><br></pre></td></tr></table></figure><h2 id="Table"><a href="#Table" class="headerlink" title="Table"></a>Table</h2><p>导入包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">\usepackage&#123;textcomp,booktabs&#125;</span><br><span class="line">\usepackage[usenames,dvipsnames]&#123;color&#125;</span><br><span class="line">\usepackage&#123;colortbl&#125;</span><br><span class="line">\definecolor&#123;mygray&#125;&#123;gray&#125;&#123;.9&#125;</span><br></pre></td></tr></table></figure><p>三线表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">% 位置参数h:here,t:top,b:bottom,p:</span><br><span class="line">\begin&#123;table*&#125;[htp]</span><br><span class="line">\centering</span><br><span class="line">    \begin&#123;tabular&#125;&#123;cc&#125;</span><br><span class="line"></span><br><span class="line">    \toprule[1.1pt] %表头直线</span><br><span class="line">    \bf&#123;Notation&#125; &amp; \bf&#123;Specification&#125;\\</span><br><span class="line">    \midrule[1.1pt] </span><br><span class="line"></span><br><span class="line">    ...&amp; ...\\</span><br><span class="line">    \rowcolor&#123;grey&#125;&#123;0.9&#125; % 填充颜色</span><br><span class="line">    ...&amp; ...\\</span><br><span class="line">    ...&amp; ...\\</span><br><span class="line"></span><br><span class="line">    \bottomrule[1.1pt] %表底直线</span><br><span class="line">    \end&#123;tabular&#125;</span><br><span class="line">\caption&#123;notations&#125;% 表格标题</span><br><span class="line">\label&#123;tab:my_label&#125; % 表格标签（引用需要）</span><br><span class="line">\end&#123;table*&#125;</span><br></pre></td></tr></table></figure><h2 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a>Graph</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">\usepackage[pdftex]&#123;graphicx&#125;</span><br><span class="line"></span><br><span class="line">\begin&#123;figure&#125;[htbp]</span><br><span class="line">\centering</span><br><span class="line">\includegraphics[height=6.0cm,width=9.5cm]&#123;fig/universe.jpg&#125;</span><br><span class="line">\caption&#123;Campus environment detection system&#125; % 图片标题</span><br><span class="line">\label&#123;fig:my_fig&#125;% 图片标签（引用需要）</span><br><span class="line">\end&#123;figure&#125;</span><br></pre></td></tr></table></figure><h2 id="Enumerate-and-Itemsize"><a href="#Enumerate-and-Itemsize" class="headerlink" title="Enumerate and Itemsize"></a>Enumerate and Itemsize</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;enumerate&#125;</span><br><span class="line">    \item .</span><br><span class="line">    \item .</span><br><span class="line">\end&#123;enumerate&#125;</span><br></pre></td></tr></table></figure><ol><li></li><li></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;itemize&#125;</span><br><span class="line">    \item .</span><br><span class="line">    \item .</span><br><span class="line">\end&#123;itemize&#125;</span><br></pre></td></tr></table></figure><ul><li></li><li></li></ul><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>导入包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">\usepackage&#123;cite&#125;</span><br><span class="line">% references format</span><br><span class="line">\bibliographystyle&#123;acm&#125;</span><br></pre></td></tr></table></figure><p>引用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">% 文献引用</span><br><span class="line">\cite&#123;&#125;</span><br><span class="line"></span><br><span class="line">% 表格、图片等引用</span><br><span class="line">\ref&#123;tab:my_label&#125;</span><br><span class="line">\ref&#123;fig:my_label&#125;</span><br><span class="line"></span><br><span class="line">% 脚注</span><br><span class="line">\footnote&#123;&#125;</span><br></pre></td></tr></table></figure><p>章节位置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">% References</span><br><span class="line">\bibliography&#123;references&#125;</span><br><span class="line">% add to toc</span><br><span class="line">\addcontentsline&#123;toc&#125;&#123;section&#125;&#123;References&#125;</span><br></pre></td></tr></table></figure><p>BiTex类型</p><p><a href="https://wenku.baidu.com/view/0f2096643968011ca300916d.htmlMath" target="_blank" rel="noopener">https://wenku.baidu.com/view/0f2096643968011ca300916d.htmlMath</a></p><h3 id="导入包"><a href="#导入包" class="headerlink" title="导入包"></a>导入包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">\usepackage&#123;amsmath,amssymb,amsthm&#125;</span><br><span class="line">\usepackage&#123;newtxmath&#125; % must come after amsXXX</span><br></pre></td></tr></table></figure><h3 id="数学公式"><a href="#数学公式" class="headerlink" title="数学公式"></a>数学公式</h3><p>分段函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;equation&#125;</span><br><span class="line">f(n) =</span><br><span class="line">\begin&#123;cases&#125;</span><br><span class="line">n/2,  &amp; \text&#123;if $n$ is even&#125; \\</span><br><span class="line">3n+1, &amp; \text&#123;if $n$ is odd&#125;</span><br><span class="line">\end&#123;cases&#125;</span><br><span class="line">\end&#123;equation&#125;</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{equation}f(n) =\begin{cases}n/2,  & \text{if $n$ is even} \\3n+1, & \text{if $n$ is odd}\end{cases}\end{equation}</script><p>方程组</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;equation*&#125;</span><br><span class="line">\left\&#123; </span><br><span class="line">\begin&#123;array&#125;&#123;c&#125;</span><br><span class="line">a_1x+b_1y+c_1z=d_1 \\ </span><br><span class="line">a_2x+b_2y+c_2z=d_2 \\ </span><br><span class="line">a_3x+b_3y+c_3z=d_3</span><br><span class="line">\end&#123;array&#125;</span><br><span class="line">\right.  </span><br><span class="line">\end&#123;equation*&#125;</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{equation*}\left\{ \begin{array}{c}a_1x+b_1y+c_1z=d_1 \\ a_2x+b_2y+c_2z=d_2 \\ a_3x+b_3y+c_3z=d_3\end{array}\right.  \end{equation*}</script><p>公式推导</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;equation&#125;</span><br><span class="line">\begin&#123;split&#125;</span><br><span class="line">\cos 2x &amp;= \cos^2 x - \sin^2 x\\</span><br><span class="line">&amp;= 2\cos^2 x - 1</span><br><span class="line">\end&#123;split&#125;</span><br><span class="line">\end&#123;equation&#125;</span><br></pre></td></tr></table></figure><script type="math/tex; mode=display">\begin{equation}\begin{split}\cos 2x &= \cos^2 x - \sin^2 x\\&= 2\cos^2 x - 1\end{split}\end{equation}</script>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Latex-Note&quot;&gt;&lt;a href=&quot;#Latex-Note&quot; class=&quot;headerlink&quot; title=&quot;Latex Note&quot;&gt;&lt;/a&gt;Latex Note&lt;/h1&gt;&lt;p&gt;​    一些简单的latex笔记，发现记录一下还是很有必要的，以免写论文的时候又忘了，到处去找，费时费神。&lt;/p&gt;
    
    </summary>
    
    
      <category term="latex" scheme="http://yoursite.com/categories/latex/"/>
    
    
      <category term="latex" scheme="http://yoursite.com/tags/latex/"/>
    
  </entry>
  
  <entry>
    <title>Python collections 简单上手</title>
    <link href="http://yoursite.com/2020/04/24/python/collections/"/>
    <id>http://yoursite.com/2020/04/24/python/collections/</id>
    <published>2020-04-24T04:52:25.133Z</published>
    <updated>2020-04-24T05:08:33.348Z</updated>
    
    <content type="html"><![CDATA[<h1 id="collections-库简单上手"><a href="#collections-库简单上手" class="headerlink" title="collections 库简单上手"></a>collections 库简单上手</h1><h2 id="Counter"><a href="#Counter" class="headerlink" title="Counter"></a>Counter</h2><p>   Counter 是 dictionary 对象的子类。collections 模块中的 Counter() 函数会接收一个诸如 list 或 tuple 的迭代器，然后返回一个 Counter dictionary。这个 dictionary 的键是该迭代器中的唯一元素，每个键的值是迭代器元素的计数。</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lst = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">counter = Counter(lst)</span><br><span class="line">print(counter)</span><br></pre></td></tr></table></figure><pre><code>Counter({1: 7, 2: 5, 3: 3})</code></pre><p>most_common()函数会以元组列表的形式，返回出现频率最高的n个元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">counter.most_common(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><pre><code>[(1, 7), (2, 5)]</code></pre><h2 id="defaultdict"><a href="#defaultdict" class="headerlink" title="defaultdict"></a>defaultdict</h2><p>defaultdict 的工作方式和平常的 python dictionary 完全相同，只是当你试图访问一个不存在的键时，它不会报错，而是会使用默认值初始化这个键。默认值是根据在创建 defaultdict 对象时作为参数输入的数据类型自动设置的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br></pre></td></tr></table></figure><p>如上所示，在访问不存在的键”Sara”时，程序并不会像一般字典一样报错，而是为其设置一个int型的默认值。<br>同理，当我们设置默认值类型为list时，看看会发生什么。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">names_dict = defaultdict(int)</span><br><span class="line">names_dict[<span class="string">"Bob"</span>] = <span class="number">1</span></span><br><span class="line">names_dict[<span class="string">"Katie"</span>] = <span class="number">2</span></span><br><span class="line">sara_number = names_dict[<span class="string">"Sara"</span>]</span><br><span class="line">print(names_dict)</span><br></pre></td></tr></table></figure><pre><code>defaultdict(&lt;class &#39;int&#39;&gt;, {&#39;Bob&#39;: 1, &#39;Katie&#39;: 2, &#39;Sara&#39;: 0})</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">names_dict1 = defaultdict(list)</span><br><span class="line">names_dict1[<span class="string">"Bob"</span>] = <span class="number">1</span></span><br><span class="line">names_dict1[<span class="string">"Katie"</span>] = <span class="number">2</span></span><br><span class="line">sara_number = names_dict1[<span class="string">"Sara"</span>]</span><br><span class="line">print(names_dict1)</span><br></pre></td></tr></table></figure><pre><code>defaultdict(&lt;class &#39;list&#39;&gt;, {&#39;Bob&#39;: 1, &#39;Katie&#39;: 2, &#39;Sara&#39;: []})</code></pre><p>可以看到，默认值变成了一个空列表</p><h2 id="OrderedDict"><a href="#OrderedDict" class="headerlink" title="OrderedDict"></a>OrderedDict</h2><p>返回一个字典的子类，具有可以将字典顺序重新排列的方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d = OrderedDict.fromkeys(<span class="string">'abcde'</span>)</span><br><span class="line">d.move_to_end(<span class="string">'b'</span>)</span><br><span class="line"><span class="string">''</span>.join(d.keys())</span><br></pre></td></tr></table></figure><pre><code>&#39;acdeb&#39;</code></pre><p>last 参数默认为True，即将元素挪到最右边，False则将元素挪到最左边</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d.move_to_end(<span class="string">'b'</span>, last=<span class="keyword">False</span>) </span><br><span class="line"><span class="string">''</span>.join(d.keys())</span><br><span class="line"><span class="string">'bacde'</span></span><br></pre></td></tr></table></figure><pre><code>&#39;bacde&#39;</code></pre><p>popitem()方法将最后的元素弹出，参数last默认为True，即弹出最右边的元素，False则为最左边。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"poped item: &#123;&#125;"</span>.format(d.popitem()))</span><br><span class="line">print(d)</span><br></pre></td></tr></table></figure><pre><code>poped item: (&#39;e&#39;, None)OrderedDict([(&#39;b&#39;, None), (&#39;a&#39;, None), (&#39;c&#39;, None), (&#39;d&#39;, None)])</code></pre><h2 id="deque"><a href="#deque" class="headerlink" title="deque"></a>deque</h2><p>   collections.deque是python中使用队列的一种非常好的方法，这个方法的一个关键特性是保持队列长度一直不变，也就是说，如果你将 queue 的最大大小设置为 10，那么 deque 将根据 FIFO 原则添加和删除元素，以保持 queue 的最大大小为 10。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">my_queue = deque(maxlen=<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    my_queue.append(i+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(my_queue)</span><br></pre></td></tr></table></figure><pre><code>deque([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], maxlen=10)</code></pre><p>   接下来，我们尝试继续往里面添加元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">11</span>,<span class="number">16</span>):</span><br><span class="line">    my_queue.append(i)</span><br><span class="line"></span><br><span class="line">print(my_queue)</span><br></pre></td></tr></table></figure><pre><code>deque([6, 7, 8, 9, 10, 11, 12, 13, 14, 15], maxlen=10)</code></pre><p>可以看到，最前面的5个元素被弹出，而新元素被追加到队列的后面</p><h2 id="nametuple"><a href="#nametuple" class="headerlink" title="nametuple"></a>nametuple</h2><p>nametuple是tuple的子类，意味着它可以像tuple一样通过索引下标访问，同时，它还具备了通过字段访问元素的功能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Point = namedtuple(<span class="string">'Point'</span>, [<span class="string">'x'</span>, <span class="string">'y'</span>])</span><br><span class="line">p = Point(<span class="number">11</span>, y=<span class="number">22</span>) </span><br><span class="line"><span class="comment"># 像普通元组一样访问元素</span></span><br><span class="line">print(p[<span class="number">0</span>] + p[<span class="number">1</span>]) </span><br><span class="line"><span class="comment"># 通过字段访问元素</span></span><br><span class="line">print(p.x + p.y)</span><br></pre></td></tr></table></figure><pre><code>3333</code></pre><p>为了防止与字段名冲突，nametuple的类方法和属性前都加了下划线_。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># _make方法将列表转化为nametuple</span></span><br><span class="line">t = [<span class="number">11</span>, <span class="number">22</span>]</span><br><span class="line">Point._make(t)</span><br></pre></td></tr></table></figure><pre><code>Point(x=11, y=22)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># _asdict()将nametuple转为字典</span></span><br><span class="line">p = Point(x=<span class="number">11</span>, y=<span class="number">22</span>)</span><br><span class="line">p._asdict()</span><br></pre></td></tr></table></figure><pre><code>OrderedDict([(&#39;x&#39;, 11), (&#39;y&#39;, 22)])</code></pre><p>需要注意，由于nametuple本质上还是一个tuple，因此直接更改其中的元素是会报错的。正确的做法是使用_replace函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># p.x = 12 报错</span></span><br><span class="line">p._replace(x=<span class="number">12</span>)</span><br></pre></td></tr></table></figure><pre><code>Point(x=12, y=22)</code></pre><p>可以通过_fields属性访问nametuple的字段。一个很常见的用法是从现有的nametuple创建一个新的nametuple。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p._fields</span><br></pre></td></tr></table></figure><pre><code>(&#39;x&#39;, &#39;y&#39;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Color = namedtuple(<span class="string">'Color'</span>, <span class="string">'red green blue'</span>)</span><br><span class="line">Pixel = namedtuple(<span class="string">'Pixel'</span>, Point._fields + Color._fields)</span><br><span class="line">Pixel(<span class="number">11</span>, <span class="number">22</span>, <span class="number">128</span>, <span class="number">255</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><pre><code>Pixel(x=11, y=22, red=128, green=255, blue=0)</code></pre><p> 如果希望将字典转换为nametuple，可以用如下方式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d = &#123;<span class="string">'x'</span>: <span class="number">11</span>, <span class="string">'y'</span>: <span class="number">22</span>&#125;</span><br><span class="line">Point(**d)</span><br></pre></td></tr></table></figure><pre><code>Point(x=11, y=22)</code></pre><h2 id="ChainMap"><a href="#ChainMap" class="headerlink" title="ChainMap"></a>ChainMap</h2><p>可以快速链接许多映射，因此可以将它们视为一个单元，通常比创建新字典并运行多个update()要快得多。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> ChainMap</span><br></pre></td></tr></table></figure><p>注意，ChainMap的迭代顺序是通过扫描从前往后的映射决定的（与官方文档说的从后往前相反，研究了甚久，有可能是版本问题）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">baseline = &#123;<span class="string">'music'</span>: <span class="string">'bach'</span>, <span class="string">'art'</span>: <span class="string">'rembrandt'</span>&#125;</span><br><span class="line">adjustments = &#123;<span class="string">'art'</span>: <span class="string">'van gogh'</span>, <span class="string">'opera'</span>: <span class="string">'carmen'</span>&#125;</span><br><span class="line">print(list(ChainMap(adjustments, baseline)))</span><br><span class="line">print(dict(ChainMap(adjustments, baseline)))</span><br></pre></td></tr></table></figure><pre><code>[&#39;art&#39;, &#39;opera&#39;, &#39;music&#39;]{&#39;art&#39;: &#39;van gogh&#39;, &#39;opera&#39;: &#39;carmen&#39;, &#39;music&#39;: &#39;bach&#39;}</code></pre><p>这与从最后一个映射开始的一系列dict.update()调用的顺序相反:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">combined = baseline.copy()</span><br><span class="line">combined.update(adjustments)</span><br><span class="line">print(list(combined))</span><br><span class="line">print(dict(combined))</span><br></pre></td></tr></table></figure><pre><code>[&#39;music&#39;, &#39;art&#39;, &#39;opera&#39;]{&#39;music&#39;: &#39;bach&#39;, &#39;art&#39;: &#39;van gogh&#39;, &#39;opera&#39;: &#39;carmen&#39;}</code></pre><p>ChainMap最常用的例子是在程序传入参数的优先级设置上，例如：参数可以通过命令行传入，可以通过环境变量传入，还可以有默认参数，我们可以设置优先级：命令行&gt;环境变量&gt;默认值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jupyter notebook中无法使用argparse</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> ChainMap</span><br><span class="line"><span class="keyword">import</span> os, argparse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造缺省参数:</span></span><br><span class="line">defaults = &#123;</span><br><span class="line">    <span class="string">'color'</span>: <span class="string">'red'</span>,</span><br><span class="line">    <span class="string">'user'</span>: <span class="string">'guest'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造命令行参数:</span></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-u'</span>, <span class="string">'--user'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'-c'</span>, <span class="string">'--color'</span>)</span><br><span class="line">namespace = parser.parse_args()</span><br><span class="line">command_line_args = &#123; k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> vars(namespace).items() <span class="keyword">if</span> v &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 组合成ChainMap:</span></span><br><span class="line">combined = ChainMap(command_line_args, os.environ, defaults)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印参数:</span></span><br><span class="line">print(<span class="string">'color=%s'</span> % combined[<span class="string">'color'</span>])</span><br><span class="line">print(<span class="string">'user=%s'</span> % combined[<span class="string">'user'</span>])</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;collections-库简单上手&quot;&gt;&lt;a href=&quot;#collections-库简单上手&quot; class=&quot;headerlink&quot; title=&quot;collections 库简单上手&quot;&gt;&lt;/a&gt;collections 库简单上手&lt;/h1&gt;&lt;h2 id=&quot;Counter&quot;&gt;&lt;a href=&quot;#Counter&quot; class=&quot;headerlink&quot; title=&quot;Counter&quot;&gt;&lt;/a&gt;Counter&lt;/h2&gt;&lt;p&gt;   Counter 是 dictionary 对象的子类。collections 模块中的 Counter() 函数会接收一个诸如 list 或 tuple 的迭代器，然后返回一个 Counter dictionary。这个 dictionary 的键是该迭代器中的唯一元素，每个键的值是迭代器元素的计数。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="collections" scheme="http://yoursite.com/tags/collections/"/>
    
  </entry>
  
  <entry>
    <title>Byte Pair Encoding</title>
    <link href="http://yoursite.com/2020/04/23/natural-language-processing/Byte-Pair-Encoding/"/>
    <id>http://yoursite.com/2020/04/23/natural-language-processing/Byte-Pair-Encoding/</id>
    <published>2020-04-23T03:15:00.587Z</published>
    <updated>2020-04-24T08:20:28.181Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Byte-Pair-Encoding"><a href="#Byte-Pair-Encoding" class="headerlink" title="Byte Pair Encoding"></a>Byte Pair Encoding</h1><p>​    <strong>Byte Pari Encoding（BPE）</strong>）是一种简单的数据压缩技巧，最初在1994年被提出，而如今广泛应用于各种现代的NLP模型（如BERT、GPT-2、XLM…）。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>​    如今NLP的发展，由刚开始的基于频率的稀疏词向量（如词袋、N-gram），到前几年通过预训练（word2vec，glove）生成语义表示的稠密词向量，再到划时代意义的BERT，NLP界越来越重视通过预训练学习语义，从而得到好的词向量表示。</p><p>​    然而这其中有个问题需要解决，基本上所有的NLP任务都需要构造一个词表，往往是选取出现频率最高的N个词加入词表。这其中有什么问题呢，举个例子，BERT的预训练语料是维基百科，如此庞大的语料库，词量肯定也是惊人的高。如果我们构造的词表太小，很多频率较低的词语就会被表示为一个掩码（又或是被去除），在模型中就根本“看不到”，这不仅会让这些低频词无法被学习，还会影响上下文中的高频词的学习。但如果词表过大，就会带来效率问题。</p><p>​    BPE就是一种很好的方法，在不扩大词表的情况下，能够让更多的词语“被模型看到”。</p><a id="more"></a><h2 id="原始BPE"><a href="#原始BPE" class="headerlink" title="原始BPE"></a>原始BPE</h2><p>​    BPE最早在1994年由Philip Gage提出（<a href="https://www.drdobbs.com/a-new-algorithm-for-data-compression/184402829" target="_blank" rel="noopener">A New Algorithm for Data Compression</a>），是一种数据压缩的技术，通过使用数据中未出现的字节代替常见的连续字节对。</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/bpe1.png" alt></p><h2 id="子词标记（Subword-Tokenization）的BPE"><a href="#子词标记（Subword-Tokenization）的BPE" class="headerlink" title="子词标记（Subword Tokenization）的BPE"></a>子词标记（Subword Tokenization）的BPE</h2><p>​    方法最早在<a href="https://arxiv.org/pdf/1508.07909.pdf" target="_blank" rel="noopener">Neural Machine Translation of Rare Words with Subword Units</a>提出，最初是为了解决神经网络翻译（NMT）中的OOV（Out of vocabulary）问题，即处理的词表是定长的，而翻译却往往会在词表之外。同样的，这个问题各种NLP任务也会遇到。</p><p>​    为了进行子词标记，BPE简单地做了一些调整，经常出现的子字对合并在一起，而不是被另一个字节代替。这样一来，低频的词就会被分割成几个高频的子词，例如athazagoraphobia可能就会被分割成[‘▁ath’, ‘az’, ‘agor’, ‘aphobia’]。</p><ul><li>第0步，初始化词表；</li><li>第1步，把每个词表示为字符的组合，为了防止与其他词混淆，在词的后面再加入一个特殊的标识”&lt;\w&gt;”。例如hello，就会被分割成：”h e l l o <_ w>“ 。</_></li><li>第2步，遍历词表，统计所有的字符对（其实就是2-gram），如：(h, e), (e, l), (l, l), (l, o), (o, <_ w>)。</_></li><li>第3步，合并频率最高的字符对，将合成的新子词加入词表（或词频+1）。例如，(h, e)出现频率最高，那么就将他们合并，变成新子词he，再将这个新词加入词表，同时原来的单词就变成“he l l o <_ w>”。</_></li><li>第4步，重复2、3步，直到达到迭代次数或达到所需的词汇量。（超参数）</li></ul><h2 id="编码与解码"><a href="#编码与解码" class="headerlink" title="编码与解码"></a>编码与解码</h2><p>​    得到子词的词表后，我们先需要对子词表进行长度从大到小的排序。编码时，我们对每个单词，遍历子词表，寻找是否有子词是该单词的子串，然后将该单词分割成若干子词。若某一子串没有找到对应的子词，那么将它替换为一个特殊的标记如“<_ unk>”。</_></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 给定单词序列</span><br><span class="line">[“the&lt;/w&gt;”, “biggest&lt;/w&gt;”, “stone&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"># 假设已有排好序的subword词表</span><br><span class="line">[“er&lt;/w&gt;”, “tain&lt;/w&gt;”, “tone”, “est&lt;/w&gt;”, “big”, “the&lt;/w&gt;”, “one&lt;/w&gt;”, &quot;s&quot;, &quot;g&quot;]</span><br><span class="line"></span><br><span class="line"># 迭代结果</span><br><span class="line">&quot;the&lt;/w&gt;&quot; -&gt; [&quot;the&lt;/w&gt;&quot;]</span><br><span class="line">&quot;biggest&lt;/w&gt;&quot; -&gt; [&quot;big&quot;, &quot;g&quot;,&quot;est&lt;/w&gt;&quot;]</span><br><span class="line">&quot;stone&lt;/w&gt;&quot; -&gt; [&quot;st&quot;, &quot;tone&lt;/w&gt;&quot;]</span><br></pre></td></tr></table></figure><p>​    这种方式的好处是，可以很好地适应新语料，缺点是，如果语料很大，这一编码方式效率较慢，因此一般是在训练模型之前，预编码好语料。还有一种方式是，在构建子词表的时候，其实就可以对构建子词表的语料进行编码了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 给定单词序列</span><br><span class="line">[..., “the&lt;/w&gt;”, “biggest&lt;/w&gt;”, “stone&lt;/w&gt;”, ...] # 假设语料库中还有其他词语</span><br><span class="line"></span><br><span class="line"># 第一步</span><br><span class="line"># [..., &quot;t h e &lt;/w&gt;&quot;, &quot;b i g g e s t &lt;/w&gt;&quot;, &quot;s t o n e &lt;/w&gt;&quot;, ...]</span><br><span class="line"></span><br><span class="line"># 第二步</span><br><span class="line"># ..., (t, h), (h, e), (e, &lt;/w&gt;), (b, i), ...</span><br><span class="line"></span><br><span class="line"># 第三步，假设he频率最高</span><br><span class="line"># [..., &quot;t he &lt;/w&gt;&quot;, &quot;b i g g e s t &lt;/w&gt;&quot;, &quot;s t o n e &lt;/w&gt;&quot;, ...]</span><br><span class="line"></span><br><span class="line"># 继续第二步</span><br><span class="line"># ..., (t , he), (he, &lt;/w&gt;), (e, &lt;/w&gt;), (b, i), ...</span><br><span class="line"></span><br><span class="line"># ... 第三步</span><br><span class="line"># ... 迭代，直到次数满足，或词表大小满足</span><br><span class="line"></span><br><span class="line"># 最终结果</span><br><span class="line"># [..., &quot;the&lt;/w&gt;&quot;, &quot;big g est&lt;/w&gt;&quot;, &quot;s tone&lt;/w&gt;&quot;]</span><br></pre></td></tr></table></figure><p>​    可以看到，这个时候，我们直接将改变后的每个单词，按照空格符分割即可。</p><p>​    解码的过程就简单多了，以”<_ w>“作为分割符，将子词合并为单词即可</_></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 假设模型预测后得到</span><br><span class="line">[“the&lt;/w&gt;”, &quot;big&quot;, &quot;g&quot;, &quot;est&lt;/w&gt;&quot;, &quot;s&quot;, &quot;tone&lt;/w&gt;&quot;]</span><br><span class="line"></span><br><span class="line"># 解码</span><br><span class="line">the biggest stone</span><br></pre></td></tr></table></figure><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter, defaultdict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(corpus: str)</span> -&gt; dict:</span></span><br><span class="line">    <span class="string">"""Step 1. 建立词表"""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把每个字符分割出来，并在词语最后加上&lt;/w&gt;，如:"hello"-&gt;"h e l l o &lt;/w&gt;"</span></span><br><span class="line">    tokens = [<span class="string">" "</span>.join(word) + <span class="string">" &lt;/w&gt;"</span> <span class="keyword">for</span> word <span class="keyword">in</span> corpus.split()]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 统计语料中的词频</span></span><br><span class="line">    vocab = Counter(tokens)  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vocab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_stats</span><span class="params">(vocab: dict)</span> -&gt; dict:</span></span><br><span class="line">    <span class="string">"""Step 2. 分割字符串，并统计字符对的频率"""</span></span><br><span class="line"></span><br><span class="line">    pairs = defaultdict(int)</span><br><span class="line">    <span class="keyword">for</span> word, frequency <span class="keyword">in</span> vocab.items():</span><br><span class="line">        symbols = word.split() <span class="comment"># 以空格分割</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 统计2-gram频率</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(symbols) - <span class="number">1</span>):</span><br><span class="line">            pairs[symbols[i], symbols[i + <span class="number">1</span>]] += frequency</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_vocab</span><span class="params">(pair: tuple, v_in: dict)</span> -&gt; dict:</span></span><br><span class="line">    <span class="string">"""合并频率最高的对"""</span></span><br><span class="line">    </span><br><span class="line">    v_out = &#123;&#125;</span><br><span class="line">    bigram = re.escape(<span class="string">' '</span>.join(pair))</span><br><span class="line">    p = re.compile(<span class="string">r'(?&lt;!\S)'</span> + bigram + <span class="string">r'(?!\S)'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> v_in:</span><br><span class="line">        <span class="comment"># 将词表中所有的最高词频对替换，如："h e l l o" -&gt; "he l l o"</span></span><br><span class="line">        w_out = p.sub(<span class="string">''</span>.join(pair), word)</span><br><span class="line">        v_out[w_out] = v_in[word]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> v_out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">corpus = <span class="string">"I had seen the biggest stone."</span></span><br><span class="line">vocab = build_vocab(corpus)  <span class="comment"># Step 1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">num_merges = <span class="number">50</span> <span class="comment"># 迭代次数，超参数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_merges):</span><br><span class="line"></span><br><span class="line">    pairs = get_stats(vocab)  <span class="comment"># Step 2</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> pairs:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># step 3</span></span><br><span class="line">    best = max(pairs, key=pairs.get)</span><br><span class="line">    vocab = merge_vocab(best, vocab)</span><br></pre></td></tr></table></figure><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>​     [1] Sennrich, Rico, Barry Haddow, and Alexandra Birch. “Neural machine translation of rare words with subword units.”<em>arXiv preprint arXiv:1508.07909</em>(2015).</p><p>​    [2] <a href="https://towardsdatascience.com/@JaswalAkash?source=post_page-----eb36c7df4f10----------------------" target="_blank" rel="noopener">Akashdeep Singh Jaswal</a>, <a href="https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10" target="_blank" rel="noopener">Byte Pair Encoding — The Dark Horse of Modern NLP</a>, 2019.</p><p>​    [3] <a href="mailto:plmsmile@126.com" target="_blank" rel="noopener">PLM</a>, <a href="https://plmsmile.github.io/2017/10/19/subword-units/" target="_blank" rel="noopener">subword-units</a>, 2017.</p><p>​    [4] <a href="https://www.zhihu.com/people/luke-china" target="_blank" rel="noopener">Luke</a>, 深入理解NLP Subword算法：BPE、WordPiece、ULM, 2020.</p><p>​    [5] Byte pair encoding - Wikipedia - <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/B</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Byte-Pair-Encoding&quot;&gt;&lt;a href=&quot;#Byte-Pair-Encoding&quot; class=&quot;headerlink&quot; title=&quot;Byte Pair Encoding&quot;&gt;&lt;/a&gt;Byte Pair Encoding&lt;/h1&gt;&lt;p&gt;​    &lt;strong&gt;Byte Pari Encoding（BPE）&lt;/strong&gt;）是一种简单的数据压缩技巧，最初在1994年被提出，而如今广泛应用于各种现代的NLP模型（如BERT、GPT-2、XLM…）。&lt;/p&gt;
&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;​    如今NLP的发展，由刚开始的基于频率的稀疏词向量（如词袋、N-gram），到前几年通过预训练（word2vec，glove）生成语义表示的稠密词向量，再到划时代意义的BERT，NLP界越来越重视通过预训练学习语义，从而得到好的词向量表示。&lt;/p&gt;
&lt;p&gt;​    然而这其中有个问题需要解决，基本上所有的NLP任务都需要构造一个词表，往往是选取出现频率最高的N个词加入词表。这其中有什么问题呢，举个例子，BERT的预训练语料是维基百科，如此庞大的语料库，词量肯定也是惊人的高。如果我们构造的词表太小，很多频率较低的词语就会被表示为一个掩码（又或是被去除），在模型中就根本“看不到”，这不仅会让这些低频词无法被学习，还会影响上下文中的高频词的学习。但如果词表过大，就会带来效率问题。&lt;/p&gt;
&lt;p&gt;​    BPE就是一种很好的方法，在不扩大词表的情况下，能够让更多的词语“被模型看到”。&lt;/p&gt;
    
    </summary>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="BERT" scheme="http://yoursite.com/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>ELU and GELU</title>
    <link href="http://yoursite.com/2019/01/11/deep-learning/ELU&amp;GELU/"/>
    <id>http://yoursite.com/2019/01/11/deep-learning/ELU&amp;GELU/</id>
    <published>2019-01-11T08:47:03.905Z</published>
    <updated>2020-04-23T12:14:01.239Z</updated>
    
    <content type="html"><![CDATA[<h2 id="ELU"><a href="#ELU" class="headerlink" title="ELU"></a>ELU</h2><p>&emsp; <strong>ELU</strong>(<em>Exponential Linear Units</em>)，是对ReLU负半区为0的改进，使得激活单元均值更趋向于0.</p><h3 id="图像和公式"><a href="#图像和公式" class="headerlink" title="图像和公式"></a>图像和公式</h3><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/ELU1.png?raw=true" alt="GELU1"></p><blockquote><p>其中，LReLU($\alpha=0.1$)，ELU($\alpha=1$) </p></blockquote><a id="more"></a><p>公式如下</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/ELU2.png?raw=true" alt="GELU1"></p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/ELU3.png?raw=true" alt="GELU1"></p><blockquote><p>在MINST数据集上的实验结果，左边是激活值的平均值，右边是交叉熵损失，可以看到ELU的激活值均值要更接近0，且交叉熵损失下降得更快.</p></blockquote><h2 id="GELU"><a href="#GELU" class="headerlink" title="GELU"></a>GELU</h2><p>&emsp; <strong>GELU</strong>(<em>Gaussian Error Linear Unit</em>)，是ReLU的一种改进，改进了ReLU缺乏概率解释的缺陷，在一些任务上表现得要更好. </p><h3 id="图像和公式-1"><a href="#图像和公式-1" class="headerlink" title="图像和公式"></a>图像和公式</h3><p>&emsp;<strong>GELU</strong>的曲线如下</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/GELU1.png?raw=true" alt="GELU1"></p><blockquote><p>其中，GELU($\mu =0,\sigma =1$)，ELU($\alpha = 1$)</p></blockquote><p>公式如下：</p><script type="math/tex; mode=display">GELU(x) = xP(X<x) = x\Phi(x)</script><p>并且可以用如下公式估计</p><script type="math/tex; mode=display">0.5x(1+tanh[\sqrt{2/\pi}(x+0.044715x^3)])</script><p>或是</p><script type="math/tex; mode=display">x \sigma(1.702x)</script><h3 id="实验结果-1"><a href="#实验结果-1" class="headerlink" title="实验结果"></a>实验结果</h3><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/GELU2.png?raw=true" alt="GELU1"></p><blockquote><p>在MINST数据集上的结果，左边是未使用Dropout，右边是使用了Dropout.</p></blockquote><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/GELU3.png?raw=true" alt="GELU1"></p><blockquote><p>在MINST数据集上的鲁棒性结果，可以看到，随着噪声的增加，GELU要表现得更稳定.</p></blockquote><p>&lt;/br&gt;</p><p>&lt;/br&gt;</p><blockquote><p>参考资料</p><p><a href="https://blog.csdn.net/mao_xiao_feng/article/details/53242235" target="_blank" rel="noopener">https://blog.csdn.net/mao_xiao_feng/article/details/53242235</a></p><p><a href="https://arxiv.org/abs/1511.07289" target="_blank" rel="noopener">https://arxiv.org/abs/1511.07289</a></p><p><a href="https://blog.csdn.net/dgyuanshaofeng/article/details/80209816" target="_blank" rel="noopener">https://blog.csdn.net/dgyuanshaofeng/article/details/80209816</a></p><p><a href="https://arxiv.org/abs/1606.08415---" target="_blank" rel="noopener">https://arxiv.org/abs/1606.08415---</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;ELU&quot;&gt;&lt;a href=&quot;#ELU&quot; class=&quot;headerlink&quot; title=&quot;ELU&quot;&gt;&lt;/a&gt;ELU&lt;/h2&gt;&lt;p&gt;&amp;emsp; &lt;strong&gt;ELU&lt;/strong&gt;(&lt;em&gt;Exponential Linear Units&lt;/em&gt;)，是对ReLU负半区为0的改进，使得激活单元均值更趋向于0.&lt;/p&gt;
&lt;h3 id=&quot;图像和公式&quot;&gt;&lt;a href=&quot;#图像和公式&quot; class=&quot;headerlink&quot; title=&quot;图像和公式&quot;&gt;&lt;/a&gt;图像和公式&lt;/h3&gt;&lt;p&gt;&lt;img src=&quot;https://blog-fig.oss-cn-shenzhen.aliyuncs.com/ELU1.png?raw=true&quot; alt=&quot;GELU1&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;其中，LReLU($\alpha=0.1$)，ELU($\alpha=1$) &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="激活函数" scheme="http://yoursite.com/tags/%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>BERT</title>
    <link href="http://yoursite.com/2019/01/11/natural-language-processing/BERT/"/>
    <id>http://yoursite.com/2019/01/11/natural-language-processing/BERT/</id>
    <published>2019-01-11T02:28:04.569Z</published>
    <updated>2020-04-24T08:21:16.115Z</updated>
    
    <content type="html"><![CDATA[<h1 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h1><p>&emsp; <strong>BERT</strong>全称是<strong>Bidirectional Encoder Representations from Transformers</strong>，从字面意思可以知道，这是一个基于Transformer的双向的编码器表征模型.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/BERT1.png?raw=trueF:\blog-fig\BERT1.png" alt="BERT1"></p><blockquote><p>上面是BERT和GPT、ELMo的结构对比.</p></blockquote><a id="more"></a><h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/BERT3.png?raw=true" alt></p><p>&emsp; <strong>BERT</strong>的Embedding是3个Embedding的相加，即Token Embeddings、Segment Embeddings、Position Embeddings.</p><ul><li>Token Embeddings：词向量，其中第一个单词是分类任务的标记，可用于后面的分类任务</li><li>Segment Embeddings：用来区别上句和下句</li><li>Position Embeddings：位置信息向量，和Transformer中使用三角函数不一样，这里的是通过学习得到的</li></ul><h2 id="预训练任务"><a href="#预训练任务" class="headerlink" title="预训练任务"></a>预训练任务</h2><p>&emsp; BERT有两个无监督预训练任务，第一步是随机遮盖一些词，然后依赖上下文来预测这些词，得到初步的预训练模型；第二步在第一步的基础上，是随机改变一半的句子对中的第二句，预测一个句子对的第二句是否是第一句的下一句。</p><h3 id="Task-1"><a href="#Task-1" class="headerlink" title="Task 1"></a>Task 1</h3><p>&emsp; 第一个任务称为Mask-LM，常规的LanguageModel是这样的</p><script type="math/tex; mode=display">P(X_i|X_{i-1},...X_1)</script><p>或是这样的</p><script type="math/tex; mode=display">P(X_i|X_{i+1},...,X_{n})</script><p>而Mask-LM充分利用上下文信息，具体地，随机Mask掉15%的词，然后只去预测那些被Mask的词，即</p><script type="math/tex; mode=display">P(masked|X_{else})</script><p>&emsp; 实际上，BERT并不总是Mask那15%的词，而是</p><ul><li>80%的概率Mask</li><li>10%的概率用一个随机的词代替</li><li>10%的概率不变</li></ul><blockquote><p>需注意的是模型本身并不知道它将要被要求预测哪些单词，或哪些单词被替换.</p></blockquote><h3 id="Task-2"><a href="#Task-2" class="headerlink" title="Task 2"></a>Task 2</h3><p>&emsp; 第二个任务则是预测句子的下一句，即给定句子对$<s_1,s_2>$，预测$S_2$是否是$S_1$的下一句。具体地，BERT随机替换50%的句子对中的$S_2$，并自动给定标签，然后去对所有句子对做预测. 例如：</s_1,s_2></p><blockquote><p>Input = [CLS] the man went to [MASK] store [SEP]</p><p>he bought a gallon [MASK] milk [SEP]</p><p>Label = IsNext</p><p>Input = [CLS] the man [MASK] to the store [SEP]</p><p>penguin [MASK] are flight ##less birds [SEP]</p><p>Label = NotNext</p></blockquote><h2 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine-tuning"></a>Fine-tuning</h2><p>&emsp; 预训练好的BERT模型，再去做一些简单调整，再在新的数据集上做训练，可以在许多自然语言处理的任务上有很好的表现. 例如，文本分类、问答、命名实体识别、上下文预测、对话等.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/BERT2.png?raw=true" alt></p><p>&lt;/br&gt;</p><p>&lt;/br&gt;</p><blockquote><p>参考资料</p><p><a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">https://arxiv.org/abs/1810.04805</a></p><p><a href="https://zhuanlan.zhihu.com/p/46652512---" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46652512---</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;BERT&quot;&gt;&lt;a href=&quot;#BERT&quot; class=&quot;headerlink&quot; title=&quot;BERT&quot;&gt;&lt;/a&gt;BERT&lt;/h1&gt;&lt;p&gt;&amp;emsp; &lt;strong&gt;BERT&lt;/strong&gt;全称是&lt;strong&gt;Bidirectional Encoder Representations from Transformers&lt;/strong&gt;，从字面意思可以知道，这是一个基于Transformer的双向的编码器表征模型.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog-fig.oss-cn-shenzhen.aliyuncs.com/BERT1.png?raw=trueF:\blog-fig\BERT1.png&quot; alt=&quot;BERT1&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;上面是BERT和GPT、ELMo的结构对比.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="BERT" scheme="http://yoursite.com/tags/BERT/"/>
    
      <category term="Pre-training" scheme="http://yoursite.com/tags/Pre-training/"/>
    
  </entry>
  
  <entry>
    <title>Attention</title>
    <link href="http://yoursite.com/2019/01/11/deep-learning/attention/"/>
    <id>http://yoursite.com/2019/01/11/deep-learning/attention/</id>
    <published>2019-01-11T01:35:08.450Z</published>
    <updated>2020-04-23T11:03:05.115Z</updated>
    
    <content type="html"><![CDATA[<h1 id="注意力模型"><a href="#注意力模型" class="headerlink" title="注意力模型"></a>注意力模型</h1><p>&emsp; <strong>注意力模型</strong>(<em>Attention Model</em>)被广泛使用在自然语言处理、图像识别及语音识别等各种不同类型的深度学习任务中，它的基本思想是仿照人类在做阅读文本、观察物体和听声音等工作时，会对特定的词语、物体、声音给予更多的关注，因而能更好地完成对语言、图像和声音的识别以及理解.</p><a id="more"></a><h2 id="Encoder-Decoder模型"><a href="#Encoder-Decoder模型" class="headerlink" title="Encoder-Decoder模型"></a>Encoder-Decoder模型</h2><p>&emsp; 注意力模型的一个广泛的应用是在机器翻译和对话生成上，而这两个应用最广泛使用的即是Encoder-Decoder模型，AM可以对Encoder-Decoder模型进行改进.</p><p>&emsp; 如图所示，这是Encoder-Decoder模型的基本架构.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/attention1.jpg" alt></p><p>具体地，Encoder负责将输入的序列$(x_1,x_2,…,x_m)$，转化为语义编码$C$，而Decoder则负责将$C$生成$(y_1,y_2,…,y_n)$. </p><script type="math/tex; mode=display">\begin{align}C &= F(x_1,x_2,...,x_m)    \\y_i &= G(C,y_1,y_2,...,y_{i-1})\end{align}</script><p>一般在文本处理领域，Encoder和Decoder都是RNN模型.</p><p>&emsp; 但普通的Encoder-Decoder模型存在一个问题，即Encoder生成的语义编码$C$都是一样的，这样一来，无论是生成哪个单词，输入的每个单词对它的影响力都是一样的.  </p><script type="math/tex; mode=display">\begin{align}y_1 &= f(C)    \\y_2 &= f(C,y_1)        \\y_3 &= f(C,y_1,y_2)\end{align}</script><p>这样一来，无法体现输入的不同单词对于目标单词的影响，于是需要引入注意力机制.</p><h2 id="引入注意力"><a href="#引入注意力" class="headerlink" title="引入注意力"></a>引入注意力</h2><p>&emsp; 引入注意力机制的Encoder-Decoder模型的结构如下，</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/attention2.jpg" alt></p><p>相比于普通的Encoder-Decoder模型，引入注意力机制的模型在生成语义编码上面更加多样化，对于每个生成的词语$y_i$，都有语义编码$C_i$与之对应.  于是目标单词的生成变成了如下的形式：</p><script type="math/tex; mode=display">\begin{align}y_1 &= f(C_1)    \\y_2 &= f(C_2,y_1)        \\y_3 &= f(C_3,y_1,y_2)\end{align}</script><p>那么，如何计算不同的目标单词对应的语义编码$C_i$呢？</p><p>&emsp; 为了体现不同输入单词对目标单词的影响，而又尽可能不要损失信息，一般采用加权求和的方式计算$C_i$.</p><script type="math/tex; mode=display">C_i = \sum_{j=1}^{L_x}a_{ij}h_j</script><p>其中，$L_x$代表输入句子的长度，$a_{ij}$代表输出第$i$个单词的注意力分配系数，而$h_j$是输入句子中第$j$个单词的语义编码.</p><p>&emsp; 于是问题就转变成如何求得注意力分配系数$a_{ij}$.  我们把引入注意力机制的Encoder-Decode模型展开，如下所示.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/attention4.jpg" alt></p><p>(图中有误，$H_n$应该为$H_{n-1}$)那么我们可以采用如下方式计算注意力分配系数.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/attention5.jpg" alt></p><p>如图所示(图中有误，$H_{i-1}$应该是$H_{i-2}$，$H_{i}$应该是$H_{i-1 }$)，使用函数F，使得$h_j$与Decoder中的$y_i$之前的隐藏状态$h_i$对齐，最后以Softmax函数输出概率，即可得到目标单词$y_i$所对应的输入单词$x_j$的注意力分配系数$a_{ij}$.</p><h2 id="注意力机制"><a href="#注意力机制" class="headerlink" title="注意力机制"></a>注意力机制</h2><p>​    我们可以总结出一个通用的注意力机制模式，即如下所示.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/attention6.jpg" alt></p><p>将Source中的元素看成是<key,value>的键值对，然后计算Query和各个key之间的相似度或相关度，就能得到每个key对应value的权重系数，然后加权求和就能得到最终的AttentionValue.</key,value></p><script type="math/tex; mode=display">Attention(Query,Source)=\sum_{i=1}^{L_x}Similarity(Query,Key_i)*Value_i</script><p>&emsp; 详细一点，我们可以将注意力机制分为3个阶段，第1个阶段，计算Query和Key的相似度或相关度；第2个阶段，进行归一化处理；第3个阶段，根据权重对Value进行加权求和得到AttentionValue.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/atttention7.jpg" alt></p><p>&lt;/br&gt;</p><p>&lt;/br&gt;</p><blockquote><p>参考资料：</p><p>[1]. 张俊林, 深度学习中的注意力机制, <a href="https://blog.csdn.net/tg229dvt5i93mxaq5a6u/article/details/78422216" target="_blank" rel="noopener">https://blog.csdn.net/tg229dvt5i93mxaq5a6u/article/details/78422216</a>. 2017-11-02.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;注意力模型&quot;&gt;&lt;a href=&quot;#注意力模型&quot; class=&quot;headerlink&quot; title=&quot;注意力模型&quot;&gt;&lt;/a&gt;注意力模型&lt;/h1&gt;&lt;p&gt;&amp;emsp; &lt;strong&gt;注意力模型&lt;/strong&gt;(&lt;em&gt;Attention Model&lt;/em&gt;)被广泛使用在自然语言处理、图像识别及语音识别等各种不同类型的深度学习任务中，它的基本思想是仿照人类在做阅读文本、观察物体和听声音等工作时，会对特定的词语、物体、声音给予更多的关注，因而能更好地完成对语言、图像和声音的识别以及理解.&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Transformer</title>
    <link href="http://yoursite.com/2019/01/10/deep-learning/Transformer/"/>
    <id>http://yoursite.com/2019/01/10/deep-learning/Transformer/</id>
    <published>2019-01-10T10:33:16.532Z</published>
    <updated>2020-04-23T12:16:06.794Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>&emsp; 在Google的“Attention is all you need”论文中，在Sequence-to-Sequence的任务下，打破了以往Encoder-Decoder模型以RNN、CNN为代表的神经网络架构，对序列的建模摒弃了RNN的时序观点和CNN的结构观点，而是直接使用self-attention机制，提出了一种新的神经网络结构——Transformer.</p><a id="more"></a> <h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p>&emsp; Transformer的结构如下</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/transformer1.png?raw=true" alt="transformer1"></p><h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self-Attention"></a>Self-Attention</h2><p>&emsp; Self-Attention的好处是可以建立句子内部的联系，可以直接跨越多个词捕获长期依赖知识，而无需像RNN那样按照时序建模.</p><p>​    RNN对序列建模的结构如下：</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/transformer3.png?raw=true" alt="transformer3"></p><p>&emsp; Self-Attention对序列的建模如下：</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/transformer4.png?raw=true" alt="transformer4"></p><p>&emsp;在Attention机制中，我们对Source和Target进行Attention建模，如果把Source看作是存储器存储的内容，元素由地址Key和值Value组成，当前有个Key=Query的查询，目的是取出存储器中对应的Value值，即Attetion值. 即把计算Attention值看作是一种软寻址，即根据Query和Key的相似性来决定取出对应Value的可能性.</p><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(QK^T)V</script><p>&emsp; 而Self-Attention则可看作是词自身为Query，整句都是<key,value>的软寻址. </key,value></p><h2 id="Self-Attention的变种"><a href="#Self-Attention的变种" class="headerlink" title="Self-Attention的变种"></a>Self-Attention的变种</h2><p>&emsp; Transformer里面的Self-Attention机制是一种新的变种，具体来说有两个方面，一个是加入了<strong>缩放因子</strong>(<em>Scaling Factor</em>)，另一方面，引入了<strong>多头注意力机制</strong>(<em>muti-head attention</em>).</p><p>&emsp; 具体地，Attention的计算公式中的分母多了一个向量的维度的平方根，为的是防止维度过大导致点乘结果过大，而进入SoftMax的饱和区，引起梯度饱和. 公式如下:</p><script type="math/tex; mode=display">Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}})V</script><p>&emsp; 而多头机制指的是，引入多组的参数矩阵来分别做Self-Attention，最后将所有结果拼接起来，如下图所示</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/transformer2.png?raw=true" alt="transformer2"></p><p>左边是单层的Attention，而右边是多头机制的Attention.</p><p>公式如下：</p><script type="math/tex; mode=display">\begin{align}MultiHead(Q,K,V) &= Concat(head_1,...,head_h)W^O    \\Where \ head_i &= Attention(QW_i^Q,KW_i^Q,VW_i^Q)\end{align}</script><p>这样一来，模型有多套比较独立的Attention参数，有一定集成的效果，理论上可以增强模型的能力.</p><h2 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h2><p>&emsp; 由于Self-Attetion机制是一种扁平化的词袋的结构，导致不论距离多远的词，他们在Self-Attention中的距离都是1，这样的话，会丢失词之间的相对距离关系。导致像“牛 吃 草”、“草 吃 牛”在Selft-Attention下可能是一个意思.  因此，词向量中应尽可能地包含词的位置信息.</p><p>&emsp; Transformer的做法是，将词在句子中所处的位置映射为向量，补充到其Embedding当中.  具体来说，Transformer使用了一种非常新颖的时序建模方式——利用三角函数的周期性，来构建词之间的相对位置关系.</p><script type="math/tex; mode=display">\begin{align}PE(pos,2i) = sin(pos/10000^{2i/d_{model}})    \\PE(pos, 2i+1) = cos(pos/10000^{2i/d_{model}})\end{align}</script><p>其中$pos$是词的绝对位置，而$i$是词的维度下标，$d_{model}$是Embedding的维度，由上述公式可以看出，Transformer将绝对位置，做为三角函数中的变量，利用三角函数的周期性去表示词的相对位置.</p><h2 id="Position-wise-Feed-Forward-Networks"><a href="#Position-wise-Feed-Forward-Networks" class="headerlink" title="Position-wise Feed-Forward Networks"></a>Position-wise Feed-Forward Networks</h2><p>&emsp; 除了在Embedding时引入词的位置信息外，Transformer还采用一种”位置智能的前馈神经网络“，实际上是进行两次线性变换，一次ReLU激活，公式如下：</p><script type="math/tex; mode=display">FFN(x) = max(0,xW_1+b_1)W_2+b_2</script><p>这样一来，如果在不同位置的词具有相同的线性变换，再经过ReLU非线性激活和再一次的线性变换后，在更新参数时能得到不同的参数.</p><p>&lt;/br&gt;</p><p>&lt;/br&gt;</p><blockquote><p>参考资料</p><p>[1] 腾讯云+社区, “变形金刚”为何强大：从模型到代码全面解析Google Tensor2Tensor系统, <a href="https://segmentfault.com/a/1190000015575985#articleHeader6" target="_blank" rel="noopener">https://segmentfault.com/a/1190000015575985#articleHeader6</a>, 2018-07-09.</p><p>[2] Ashish Vaswani,Noam Shazeer,Niki Parmar,Jakob Uszkorei,Llion Jones,Aidan N. Gomez,Łukasz Kaise and Illia Polosukhin. Attention Is All You need. <em>arXiv preprint</em> <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noopener">arXiv:1706.03762</a> , 2017.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Transformer&quot;&gt;&lt;a href=&quot;#Transformer&quot; class=&quot;headerlink&quot; title=&quot;Transformer&quot;&gt;&lt;/a&gt;Transformer&lt;/h1&gt;&lt;p&gt;&amp;emsp; 在Google的“Attention is all you need”论文中，在Sequence-to-Sequence的任务下，打破了以往Encoder-Decoder模型以RNN、CNN为代表的神经网络架构，对序列的建模摒弃了RNN的时序观点和CNN的结构观点，而是直接使用self-attention机制，提出了一种新的神经网络结构——Transformer.&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="注意力机制" scheme="http://yoursite.com/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/"/>
    
  </entry>
  
  <entry>
    <title>LSTM</title>
    <link href="http://yoursite.com/2018/11/13/deep-learning/LSTM/"/>
    <id>http://yoursite.com/2018/11/13/deep-learning/LSTM/</id>
    <published>2018-11-13T13:49:00.576Z</published>
    <updated>2020-04-23T04:23:30.139Z</updated>
    
    <content type="html"><![CDATA[<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><p>&emsp; <strong>LSTM</strong>(<em>Long Short Term Memory</em>)，是<strong>RNN</strong>的一种，通过门控机制，有选择地遗忘先前的状态和输出当前状态，可以很好地解决<strong>长期依赖</strong>(<em>Long-Term Dependencies</em>)的问题.</p><a id="more"></a><h2 id="长期依赖"><a href="#长期依赖" class="headerlink" title="长期依赖"></a>长期依赖</h2><p>&emsp; RNN的一个重要特性，就是可以对具有时序关系的数据进行很好地学习，但普通的RRN存在一个问题，在输入序列较长的情况下，处在靠后的位置往往很难得到靠前的输入的信息，如下图所示.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lstm-1.png" alt></p><h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><p>&emsp; 首先先看普通的RNN，我们把从输入到输出中间的结构看作是一个“细胞”，那么普通的RNN的细胞中只有一个tanh函数，用来处理旧信息$c_t$和当前输入$X_t$.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lstm-2.png" alt></p><p>那么LSTM的结构是这样的，相比普通RNN，多了几个门的结构.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lstm-3png.png" alt></p><p>下面分块来解释各个门及其应用.</p><h3 id="遗忘门"><a href="#遗忘门" class="headerlink" title="遗忘门"></a>遗忘门</h3><p>&emsp; 首先我们需要遗忘门来控制我们需要丢弃哪些先前的信息，具体来说，遗忘门是一个Sigmoid函数，输出一个0~1之间的数$f_t$，“1”表示完全保留，“0”表示完全舍弃.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lstm-4.png" alt></p><h3 id="输入门"><a href="#输入门" class="headerlink" title="输入门"></a>输入门</h3><p>&emsp; 接着我们用输入门控制哪些信息被更新，并且作为新信息输入到下一个时间点. 具体地，和遗忘门一样，用一个Sigmoid函数输出保留信息的比率$i_t$.  另外，和普通的RRN一样，使用一个tanh函数计算生成的状态信息$\hat{C_t}$ .</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lstm-5.png" alt></p><p>这里我们把新信息和旧信息结合，成为最终的状态输出到下个时间点. 这里就用到了前面计算得出的两个保留信息的比率$f_t$和$i_t$，让它们分别乘上新旧信息$C_{t-1}$和$\hat{C_t}$，最后相加得到最终的状态信息$C_t$，并输出到下一时间点$t+1$.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lstm-6.png" alt></p><h3 id="输出门"><a href="#输出门" class="headerlink" title="输出门"></a>输出门</h3><p>&emsp; 最后我们需要输出门来控制最终输出的值，跟前面的思路基本一样，用一个Sigmoid函数输出0~1的值，然后用这个值乘上tanh函数包裹的状态$C_t$，便得到了此时间点$t$的输出$h_t$，并输出到下一时间点$t+1$.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lstm-7.png" alt></p><h2 id="Peephole-Connection"><a href="#Peephole-Connection" class="headerlink" title="Peephole Connection"></a>Peephole Connection</h2><p>&emsp; LSTM的一种比较流行的变体是增加了一种叫“Peephole Connection”的连接，即将细胞状态也输入到门中.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lstm-9.png" alt></p><h1 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h1><p>&emsp;<strong>GRU</strong>(<em>Gated Recurrent Unit</em>)是比较常见的LSTM的变体，其最大的改动是将遗忘门和输入门合成了单一的更新门，同时将原来的细胞状态信息$c_t$与$h_t$混合，成为单一的$h_t$，并且做了其他改动.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lstm-8.png" alt></p><p>&lt;/br&gt;</p><p>&lt;/br&gt;</p><blockquote><p>参考资料：</p><p>[1]. wangduo, <a href="https://www.cnblogs.com/wangduo/p/6773601.html" target="_blank" rel="noopener">[译] 理解 LSTM(Long Short-Term Memory, LSTM) 网络</a> , 2017-04-27.</p><p>[2]. colah, Understanding LSTM Networks, <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/" target="_blank" rel="noopener">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>  , 2015-08-27.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;LSTM&quot;&gt;&lt;a href=&quot;#LSTM&quot; class=&quot;headerlink&quot; title=&quot;LSTM&quot;&gt;&lt;/a&gt;LSTM&lt;/h1&gt;&lt;p&gt;&amp;emsp; &lt;strong&gt;LSTM&lt;/strong&gt;(&lt;em&gt;Long Short Term Memory&lt;/em&gt;)，是&lt;strong&gt;RNN&lt;/strong&gt;的一种，通过门控机制，有选择地遗忘先前的状态和输出当前状态，可以很好地解决&lt;strong&gt;长期依赖&lt;/strong&gt;(&lt;em&gt;Long-Term Dependencies&lt;/em&gt;)的问题.&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Dropout</title>
    <link href="http://yoursite.com/2018/11/13/deep-learning/dropout/"/>
    <id>http://yoursite.com/2018/11/13/deep-learning/dropout/</id>
    <published>2018-11-13T03:19:12.049Z</published>
    <updated>2020-04-23T11:05:35.602Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h1><p>&emsp; Dropout是指在深度学习网络的训练过程中，按照一定概率暂时将神经元从网络中丢弃，这样一来，每一批的训练数据都在训练不同的网络.  实验证明，Dropout具有提高神经网络训练速度和防止过拟合的效果.</p><a id="more"></a><h2 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h2><p>&emsp; 假设我们要训练如下的一个神经网络</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/dropout-1.png" alt></p><p>使用Dropout时，我们临时随机删除一些神经元，得到如下的子网络：</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/dropout-2.png" alt></p><p>使用反向传播算法，训练此子网络的参数，然后再重新从完整的神经网络，随机删除一些神经元，不断重复这一过程.</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>首先我们要设置一个参数p，代表的是神经网络中被随机删除的神经元比例.</p><h3 id="训练阶段"><a href="#训练阶段" class="headerlink" title="训练阶段"></a>训练阶段</h3><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/dropout-3.png" alt></p><ul><li><strong>Standard network</strong><script type="math/tex; mode=display">\begin{align}z_i^{(l+1)} &= w_i^{(l+1)}y^l+b_i^{(l+1)}    \\y_i^{(l+1)} &= f(z_i^{l+1})\end{align}</script></li></ul><ul><li><strong>Drop network</strong><script type="math/tex; mode=display">\begin{align}r_j^{(l)} &\sim Bernoulli(p)    \\\hat{y} &= r^{(l)}*y^{(l)}    \\z_i^{(l+1)} &= w_i^{(l+1)} \hat{y}^l + b_i^{(l+1)}    \\y_i^{(l+1)} &= f(z_i^{(l+1)})\end{align}</script></li></ul><h3 id="测试阶段"><a href="#测试阶段" class="headerlink" title="测试阶段"></a>测试阶段</h3><p>&emsp; 测试时，每一个单元的参数都要乘以p</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/dropout-4.png" alt></p><p>即</p><script type="math/tex; mode=display">w_{test}^{(l)} = pW^{(l)}</script><h2 id="缓解过拟合的原因"><a href="#缓解过拟合的原因" class="headerlink" title="缓解过拟合的原因"></a>缓解过拟合的原因</h2><p>&emsp; Dropout的原理很简单直接，但为什么Dropout能够缓解神经网络中过拟合的问题呢？</p><ul><li><p>集成(Ensemble)的思想</p><p>&emsp; 如果把Dropout后的每个子网络看成是一个学习器，那么就可以把Dropout看作是Bagging的一种近似，且消耗更少的内存和运算力.  </p></li><li><p>神经元共享的集成</p><p>&emsp; Dropout不仅仅是训练一个Bagging的集成模型，而且是共享神经元的集成模型.  这意味着，无论其他神经元是否在模型中，每个神经元都必须表现良好.  这样一来，每一个神经元在很多情况下都能表现良好，也就提高了神经网络的泛化能力.</p></li><li><p>引入噪声</p><p>&emsp; 我们可以认为Dropout实际上是对神经元施加了噪声扰动，增强了神经网络的学习能力和鲁棒性. 传统地，我们为了在神经网络中引入噪声，会对输入的原始值进行破坏，而Dropout直接作用于神经元，相当于是屏蔽掉一部分的输入，这比原始的方法更加智能和高效.</p></li></ul><p>&lt;/br&gt;</p><p>&lt;/br&gt;</p><blockquote><p>参考资料：</p><p>[1]  Micorstrong0305.  深度学习中Dropout原理解析. <a href="https://blog.csdn.net/program_developer/article/details/80737724" target="_blank" rel="noopener">https://blog.csdn.net/program_developer/article/details/80737724</a></p><p>[2]  Srivastava N, Hinton G, Krizhevsky A, et al. Dropout: A simple way to prevent neural networks from overfitting[J]. The Journal of Machine Learning Research, 2014, 15(1): 1929-1958.</p><p>[3] Dropout as data augmentation. <a href="http://arxiv.org/abs/1506.08700" target="_blank" rel="noopener">http://arxiv.org/abs/1506.08700</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Dropout&quot;&gt;&lt;a href=&quot;#Dropout&quot; class=&quot;headerlink&quot; title=&quot;Dropout&quot;&gt;&lt;/a&gt;Dropout&lt;/h1&gt;&lt;p&gt;&amp;emsp; Dropout是指在深度学习网络的训练过程中，按照一定概率暂时将神经元从网络中丢弃，这样一来，每一批的训练数据都在训练不同的网络.  实验证明，Dropout具有提高神经网络训练速度和防止过拟合的效果.&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>批标准化</title>
    <link href="http://yoursite.com/2018/11/13/deep-learning/batch-normalization/"/>
    <id>http://yoursite.com/2018/11/13/deep-learning/batch-normalization/</id>
    <published>2018-11-13T00:14:30.646Z</published>
    <updated>2020-04-23T04:33:29.766Z</updated>
    
    <content type="html"><![CDATA[<h1 id="批标准化-Batch-Normalization"><a href="#批标准化-Batch-Normalization" class="headerlink" title="批标准化(Batch Normalization)"></a>批标准化(Batch Normalization)</h1><p>&emsp; <strong>批标准化</strong>(<em>Batch Normalization</em>)，是解决随着网络深度加深，训练困难的问题的一种方法，一般用在激活函数之前，其基本思想是把每层的输入值的分布拉回到标准正态分布当中，使得每一层中，每一次的输入独立同分布，使得神经网络可以很好地学习数据；针对于非线性激活函数，还能将处于饱和区的值，拉回到梯度较大的区域，一定程度上解决梯度消失的问题.</p><a id="more"></a><h2 id="独立同分布"><a href="#独立同分布" class="headerlink" title="独立同分布"></a>独立同分布</h2><p>&emsp; 机器学习有一个重要的假设：<strong>独立同分布</strong>(<em>Independently and Identically Distributed, IID</em>)。即假设训练数据和测试数据是满足相同分布的，它是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障. </p><p>&emsp; 如果将神经网络的每一层都看作是一次的输入和输出，由于隐藏层之间存在非线性映射的关系，那么显然难以保证每一层中每一次的输入独立同分布，并且随着网络的加深，这种情况会越来越糟糕.</p><p>&emsp; 而批标准化可以很好地解决这一问题，具体来说，批标准化将输入变换到均值为0、方差为1的正态分布中，这样一来就实现了输入的同分布.</p><h2 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h2><p>&emsp; 深度神经网络中，非线性激活函数往往会遇到一个问题，梯度消失.  以Logistic函数为例，</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/logistic.jpg" alt="boy"></p><p>如图，当输入值太大或太小的时候，函数值容易进入饱和区，这个时候斜率几乎为0，这样在反向传播的时候，梯度显然是无法继续传递的，也就导致了前面几层的参数无法更新.</p><p>&emsp; 批标准化将输入标准化为均值为0、方差为1的正态分布，如下图所示.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/01.png" alt></p><p>可以看到，在此正态分布中，数据落在[-1,1]的概率为68%，落在[-2,2]的概率为95%，结合Logisitc函数的图像来看，批标准化后的数据大部分都会落在斜率较大的区间内，这样一来，梯度消失的问题就得到了很好的解决.</p><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2><p>&emsp; 在神经网络中，往往是在激活函数之前加入对数据的批标准化操作，如下图所示.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/bn.png" alt></p><p>BN算法的核心即将数据进行正态分布的变换，即</p><script type="math/tex; mode=display">\hat{x}^{(k)} = \frac{x^{(k)-E[x^{(k)}]}}{\sqrt{Var[x^{(k)}]}}</script><p>但这也会导致网络的表达能力变差，于是引入两个调节参数，可以通过训练来学习，对变换后的值进行修正.</p><script type="math/tex; mode=display">y^{(k)} = \gamma^{(k)}\hat{x}^{(k)}+\beta^{(k)}</script><p>具体算法如下所示</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/bn-1.png" alt></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>深度神经网络使用批标准化有如下好处：</p><ol><li>使得隐藏层的每一次输入同分布，模型能更好地从数据中学习；</li><li>提升了训练速度，收敛过程加快；</li><li>一定程度上缓解了过拟合问题；</li><li>使得神经网络对初始化的要求降低，可以使用较大的学习率等；</li></ol><p>&lt;/br&gt;</p><p>&lt;/br&gt;</p><blockquote><p>参考资料</p><p>[1] 郭耀华 <a href="https://www.cnblogs.com/guoyaohua/p/8724433.html" target="_blank" rel="noopener">深入理解Batch Normalization批标准化</a> , 2018-04-05.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;批标准化-Batch-Normalization&quot;&gt;&lt;a href=&quot;#批标准化-Batch-Normalization&quot; class=&quot;headerlink&quot; title=&quot;批标准化(Batch Normalization)&quot;&gt;&lt;/a&gt;批标准化(Batch Normalization)&lt;/h1&gt;&lt;p&gt;&amp;emsp; &lt;strong&gt;批标准化&lt;/strong&gt;(&lt;em&gt;Batch Normalization&lt;/em&gt;)，是解决随着网络深度加深，训练困难的问题的一种方法，一般用在激活函数之前，其基本思想是把每层的输入值的分布拉回到标准正态分布当中，使得每一层中，每一次的输入独立同分布，使得神经网络可以很好地学习数据；针对于非线性激活函数，还能将处于饱和区的值，拉回到梯度较大的区域，一定程度上解决梯度消失的问题.&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>循环神经网络</title>
    <link href="http://yoursite.com/2018/10/08/deep-learning/recurrent-neural-network/"/>
    <id>http://yoursite.com/2018/10/08/deep-learning/recurrent-neural-network/</id>
    <published>2018-10-08T14:47:11.163Z</published>
    <updated>2018-12-03T16:04:28.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h1><h2 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h2><h2 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;循环神经网络&quot;&gt;&lt;a href=&quot;#循环神经网络&quot; class=&quot;headerlink&quot; title=&quot;循环神经网络&quot;&gt;&lt;/a&gt;循环神经网络&lt;/h1&gt;&lt;h2 id=&quot;结构&quot;&gt;&lt;a href=&quot;#结构&quot; class=&quot;headerlink&quot; title=&quot;结构&quot;&gt;&lt;
      
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络</title>
    <link href="http://yoursite.com/2018/10/08/deep-learning/convolutional-neural-network/"/>
    <id>http://yoursite.com/2018/10/08/deep-learning/convolutional-neural-network/</id>
    <published>2018-10-08T14:45:42.623Z</published>
    <updated>2018-12-03T16:04:28.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h1><h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><h2 id="池化"><a href="#池化" class="headerlink" title="池化"></a>池化</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;卷积神经网络&quot;&gt;&lt;a href=&quot;#卷积神经网络&quot; class=&quot;headerlink&quot; title=&quot;卷积神经网络&quot;&gt;&lt;/a&gt;卷积神经网络&lt;/h1&gt;&lt;h2 id=&quot;卷积&quot;&gt;&lt;a href=&quot;#卷积&quot; class=&quot;headerlink&quot; title=&quot;卷积&quot;&gt;&lt;
      
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
      <category term="计算机视觉" scheme="http://yoursite.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/"/>
    
  </entry>
  
  <entry>
    <title>神经网络</title>
    <link href="http://yoursite.com/2018/10/08/deep-learning/neural-network/"/>
    <id>http://yoursite.com/2018/10/08/deep-learning/neural-network/</id>
    <published>2018-10-08T10:35:46.825Z</published>
    <updated>2020-04-23T04:24:34.855Z</updated>
    
    <content type="html"><![CDATA[<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><p>&emsp; <strong>神经网络</strong>(<em>neural network</em>)，是<strong>深度学习</strong>(<em>deep learning</em>)领域的基础和核心工具，是一种模仿生物神经网络的结构和功能的机器学习模型。神经网络由大量的人工神经元(neron)组成，基本思想是神经元超过一个<strong>阈值</strong>(<em>threshold</em>)后被激活，输出值传到下一神经元，一层层传播，最终输出结果。</p><a id="more"></a><h2 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h2><p>&emsp; 神经网络的基本结构如下图所示，由输入层、隐藏层和输出层组成。</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com//neural-network1.png" alt="图片来源维基百科"></p><h2 id="隐藏单元"><a href="#隐藏单元" class="headerlink" title="隐藏单元"></a>隐藏单元</h2><p>&emsp; 在神经网络中，我们把隐藏层的神经元称为隐藏单元，其作用是对输入的特征进行非线性变换，将原有的特征空间拓展到更高维度，使原来线性不可分的样本点线性可分。</p><p>&emsp; 由于隐藏单元要对输入值做非线性运算，因此隐藏单元中的激活函数必须是非线性的。常见的激活函数有：Sigmoid、Tanh和ReLU。</p><h3 id="Logistic-Sigmoid"><a href="#Logistic-Sigmoid" class="headerlink" title="Logistic Sigmoid"></a>Logistic Sigmoid</h3><p>函数式如下：</p><script type="math/tex; mode=display">f(z) = \frac{1}{1+exp{(-z)}}</script><p>使用Logistic Sigmoid函数作为神经网络的激活函数，会有如下几个问题：</p><ul><li>计算量大，反向传播求误差梯度时，求导涉及除法；</li><li>函数值易饱和，反向传播时，容易出现<strong>梯度消失</strong>的情况；</li></ul><h3 id="Tanh"><a href="#Tanh" class="headerlink" title="Tanh"></a>Tanh</h3><p>函数式如下：</p><script type="math/tex; mode=display">f(z) = \frac{e^z-e^{-z}}{e^z+e^{-z}}</script><p>与Logistic Sigmoid函数相比，Tanh函数有如下特点：</p><ul><li>同样存在计算量大的问题；</li><li>同样容易出现梯度消失的情况，但延迟了函数值的饱和期；</li><li>在0附近与单位函数类似，在激活保持地很小的情况下，类似于训练一个线性模型，使得训练神经网络更容易；</li></ul><h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><p>ReLU的函数式很简单：</p><script type="math/tex; mode=display">f(z) = max(0,z)</script><ul><li>没有函数值饱和导致的梯度消失的问题；</li><li>只需要设置阈值就可以得到激活值，运算量小；</li><li>可以较好地模拟生物神经网络中，部分神经元未处于激活状态的情景；</li><li>可能出现神经元”大面积死亡”的情况；</li></ul><p><strong>现如今的深度神经网络多使用ReLU作为隐藏单元。</strong></p><h2 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h2><p>&emsp; 跟其他机器学习算法一样，神经网络同样涉及到模型训练的问题。训练一个神经网络，最常用的算法是<strong>反向传播算法</strong>(<em>back propagation algorithm</em>)，经常简称为<strong>backprop</strong>或<strong>BP算法</strong>。</p><p>&emsp; 其基本原理是复合函数求导的<strong>链式法则</strong>，具体地，我们首先对神经网络进行<strong>前向传播</strong>，即从输入值传播到输出值，然后计算误差，再将误差梯度<strong>后向传播</strong>，更新参数。</p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>&emsp; 在机器学习中，我们使用许多策略来减少模型在测试集上的误差，即使这些策略可能会增大训练误差，这些策略统称为<strong>正则化</strong>(<em>normalization</em>)。实际上，正则化就是希望显著地减少<strong>方差</strong>(<em>varience</em>)而不过度增加<strong>偏差</strong>(<em>bias</em>)的策略。</p><h3 id="参数范数惩罚"><a href="#参数范数惩罚" class="headerlink" title="参数范数惩罚"></a>参数范数惩罚</h3><h4 id="L2参数正则化"><a href="#L2参数正则化" class="headerlink" title="L2参数正则化"></a>L2参数正则化</h4><p>&emsp; L2正则化策略，通过向目标函数添加一个正则项 $\Omega(\theta) = \frac{1}{2}||w||_2^2$ ，使权值更加接近原点。与线性回归中添加L2正则项一样，神经网络使用L2正则化，目的是通过允许一定的训练误差，来防止过拟合。</p><h4 id="L1参数正则化"><a href="#L1参数正则化" class="headerlink" title="L1参数正则化"></a>L1参数正则化</h4><p>&emsp; 与L2正则化类似，L1正则化也是通过向目标函数添加一个正则项 $\Omega(\theta)=||w||_1=\sum_i |w_i|$ ，来使得权重进行一定程度的衰减。不同的是，L1正则化回产生更<strong>稀疏</strong>(<em>sparse</em>)的解，即最终会得到一些权重为0的神经网络，因而对<strong>特征选择</strong>(<em>feature selection</em>)有一定的作用。</p><h3 id="数据集增强"><a href="#数据集增强" class="headerlink" title="数据集增强"></a>数据集增强</h3><p>&emsp; 与其他的机器学习算法一样，提高神经网络的泛化能力的最好办法是使用更多的数据进行训练，然而，在实际情况中，我们所拥有的数据量是有限的。一种解决方案是创造一些数据添加到训练集中。</p><p>&emsp; 在计算机视觉中，图像是高维的并且包含着各种巨大的变化因素，我们可以模拟其中的一些，像是通过放大缩小，移动像素，来模拟现实中同一物体在不同位置的情景。</p><p>&emsp; 同时，为了改善神经网络对噪声的健壮性，可以<strong>将随机噪声添加到输入</strong>中进行训练，使得训练出来的模型在具有一定噪声的情况下也能很好地完成任务。</p><p>&emsp; 对于某些模型而言，向输入添加方差极小的噪声等价于对权重施加范数惩罚，而一般情况下，注入噪声远比简单地收缩参数要更强大。于是将<strong>噪声添加到权重</strong>中也是一种正则化的手段，这项技术主要用于<strong>循环神经网络(RNN)</strong>。实际上，这种做法是对权重添加了随机扰动，可以反映贝叶斯学习过程的权重不确定性。</p><p>&emsp; 在现实的应用场景中，大多数的数据集的标签都存在一定错误，错误的标签是不利于最大化似然的，于是，我们可以通过显式地对标签上的噪声进行建模。例如假设一个小常数 $\epsilon $ ，训练集标签正确的概率是 $1-\epsilon$ ，其他的可能标签也可能是正确的，这样就不用显式地将噪声样本抽取出来。</p><h3 id="提前终止"><a href="#提前终止" class="headerlink" title="提前终止"></a>提前终止</h3><p>&emsp; 当我们训练一个有足够表示能力的模型时，往往可以观察到，训练误差会随着时间的推移逐渐降低，但验证集的误差会再次上升。这意味着，我们应当在验证集误差有所改善时，存储训练好的模型，而当验证集误差在一定循环次数内没有改善时，我们应当终止训练。这种策略被称为<strong>提前终止</strong>(<em>early stopping</em>)。</p><p>&lt;/br&gt;</p><blockquote><p>参考资料：</p><p>[1] 深度学习[M], 北京: 人民邮电出版社, 2017: 105-156.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;神经网络&quot;&gt;&lt;a href=&quot;#神经网络&quot; class=&quot;headerlink&quot; title=&quot;神经网络&quot;&gt;&lt;/a&gt;神经网络&lt;/h1&gt;&lt;p&gt;&amp;emsp; &lt;strong&gt;神经网络&lt;/strong&gt;(&lt;em&gt;neural network&lt;/em&gt;)，是&lt;strong&gt;深度学习&lt;/strong&gt;(&lt;em&gt;deep learning&lt;/em&gt;)领域的基础和核心工具，是一种模仿生物神经网络的结构和功能的机器学习模型。神经网络由大量的人工神经元(neron)组成，基本思想是神经元超过一个&lt;strong&gt;阈值&lt;/strong&gt;(&lt;em&gt;threshold&lt;/em&gt;)后被激活，输出值传到下一神经元，一层层传播，最终输出结果。&lt;/p&gt;
    
    </summary>
    
    
      <category term="深度学习" scheme="http://yoursite.com/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="神经网络" scheme="http://yoursite.com/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    
  </entry>
  
  <entry>
    <title>Word2Vec</title>
    <link href="http://yoursite.com/2018/09/09/natural-language-processing/Word2Vec/"/>
    <id>http://yoursite.com/2018/09/09/natural-language-processing/Word2Vec/</id>
    <published>2018-09-09T11:19:59.257Z</published>
    <updated>2020-04-23T04:55:11.492Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><p>&emsp; Word2Vec是一种以<strong>无监督</strong>的方式来得到词向量的模型，其核心思想是通过上下文信息表达一个词语，将词语从原先的空间映射到新的空间中(本质上是一种降维)。这样得到的词向量相对于<strong>独热编码</strong>具有维度较低、词语之间相似度易衡量等优点.</p><p>&emsp; 具体地，Word2Vec首先将词语进行独热编码，然后将编码后的词向量输入神经网络，通过最小化误差来更新权值矩阵，最后将训练好的权值矩阵作为处理后的词向量矩阵. </p><a id="more"></a><p>&emsp; Word2Vec主要有CBOW、Skip-gram两种模型.  CBOW是根据上下文词语预测当前词；而Skip-Gram正好相反，根据当前词预测上下文. 如下图所示:   </p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/word2vec1.jpg" alt="CBOW and Skip-Gram"></p><p>对于CBOW，我们的目标函数为：</p><script type="math/tex; mode=display">L = \sum_{w\in C} \log p(w|context(w))</script><p>而对于Skip-gram，目标函数为：</p><script type="math/tex; mode=display">L = \sum_{w\in C}\log p(context(w)|w)</script><p>&emsp; Word2Vec有两个trick，分别是Hierarchical Softmax 和 Nagative Sampling，他们是Word2Vec提高训练效率和效果的两种技巧(trick).</p><h2 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h2><h2 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h2><h2 id="Nagative-Sampling"><a href="#Nagative-Sampling" class="headerlink" title="Nagative Sampling"></a>Nagative Sampling</h2>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Word2Vec&quot;&gt;&lt;a href=&quot;#Word2Vec&quot; class=&quot;headerlink&quot; title=&quot;Word2Vec&quot;&gt;&lt;/a&gt;Word2Vec&lt;/h1&gt;&lt;p&gt;&amp;emsp; Word2Vec是一种以&lt;strong&gt;无监督&lt;/strong&gt;的方式来得到词向量的模型，其核心思想是通过上下文信息表达一个词语，将词语从原先的空间映射到新的空间中(本质上是一种降维)。这样得到的词向量相对于&lt;strong&gt;独热编码&lt;/strong&gt;具有维度较低、词语之间相似度易衡量等优点.&lt;/p&gt;
&lt;p&gt;&amp;emsp; 具体地，Word2Vec首先将词语进行独热编码，然后将编码后的词向量输入神经网络，通过最小化误差来更新权值矩阵，最后将训练好的权值矩阵作为处理后的词向量矩阵. &lt;/p&gt;
    
    </summary>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>特征选择</title>
    <link href="http://yoursite.com/2018/08/10/feature-selection/"/>
    <id>http://yoursite.com/2018/08/10/feature-selection/</id>
    <published>2018-08-10T12:31:18.430Z</published>
    <updated>2018-12-03T16:04:28.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h1><p>&emsp; 特征选择是数据预处理中非常重要的技术，具体来说，在一般的场景下，数据的特征往往很多，会出现数据样本稀疏、距离计算困难等问题，称为<strong>维数灾难</strong>(<em>curse of dimensionality</em>).  另外，去除那些不那么重要的特征，能使得重要的特征对结果的预测有更好的作用.  因此，特征的选择就变得非常重要.  具体的，在特征选择中，我们的目标是去除那些与预测结果相关性小的特征，而保留那些相关性大的特征.</p><a id="more"></a><h2 id="过滤式"><a href="#过滤式" class="headerlink" title="过滤式"></a>过滤式</h2><h2 id="包裹式"><a href="#包裹式" class="headerlink" title="包裹式"></a>包裹式</h2><h2 id="嵌入式"><a href="#嵌入式" class="headerlink" title="嵌入式"></a>嵌入式</h2>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;特征选择&quot;&gt;&lt;a href=&quot;#特征选择&quot; class=&quot;headerlink&quot; title=&quot;特征选择&quot;&gt;&lt;/a&gt;特征选择&lt;/h1&gt;&lt;p&gt;&amp;emsp; 特征选择是数据预处理中非常重要的技术，具体来说，在一般的场景下，数据的特征往往很多，会出现数据样本稀疏、距离计算困难等问题，称为&lt;strong&gt;维数灾难&lt;/strong&gt;(&lt;em&gt;curse of dimensionality&lt;/em&gt;).  另外，去除那些不那么重要的特征，能使得重要的特征对结果的预测有更好的作用.  因此，特征的选择就变得非常重要.  具体的，在特征选择中，我们的目标是去除那些与预测结果相关性小的特征，而保留那些相关性大的特征.&lt;/p&gt;
    
    </summary>
    
    
      <category term="特征工程" scheme="http://yoursite.com/categories/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
    
      <category term="特征选择" scheme="http://yoursite.com/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>线性判别分析</title>
    <link href="http://yoursite.com/2018/08/10/descend-dimension/LDA/"/>
    <id>http://yoursite.com/2018/08/10/descend-dimension/LDA/</id>
    <published>2018-08-10T01:53:45.379Z</published>
    <updated>2020-04-23T04:38:41.301Z</updated>
    
    <content type="html"><![CDATA[<h1 id="线性判别分析-LDA"><a href="#线性判别分析-LDA" class="headerlink" title="线性判别分析(LDA)"></a>线性判别分析(LDA)</h1><p>&emsp; <strong>线性判别分析</strong>(<em>Linear Discriminant Analysis</em>，简称LDA)，是一种有监督的降维方法.  其思想是希望投影到超平面上的点具有如下性质：</p><ul><li>同类的样本点尽可能地接近</li><li>不同类的样本点尽可能远离</li></ul><p>如下图所示，右图的降维效果显然要好于左图.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lda1.png" alt="图片来源：博客园刘建平"></p><a id="more"></a><blockquote><p>(图片来源于博客园的刘建平大神，文末分享链接)</p></blockquote><h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><h3 id="二分类情况"><a href="#二分类情况" class="headerlink" title="二分类情况"></a>二分类情况</h3><p>&emsp; 首先，我们从二分类的情况来解释原理。给定 $n$ 维数据集 $D= \{(x_i,y_i)\}^m_{i=1},  y_i \in \{0,1\}$, 令 $X_i、\mu_i、\Sigma_i$ 分别表示第 $i\in \{0,1\}$ 类的样本的集合、均值向量、协方差矩阵. </p><p>&emsp; 在二分类的情况下，我们将 $n$ 维数据压缩到 $1$ 维，即数据投影到直线 $w$ 上，则两类样本的中心在直线上的投影分别为：$w^T\mu_0 $ 和 $w^T\mu_1$ ; 两类样本的协方差分别为：$w^T\Sigma_0 w$ 和 $w^T \Sigma_1 w$ . 显然，在一维空间中，它们都是一个实数.</p><p>&emsp; 回到开头我们讲LDA的两个目标：</p><ul><li><strong>同类的样本点尽可能地接近，则同类样本的投影点的协方差尽可能小，即 $w^T\Sigma_0w+w^T\Sigma_1w$ 尽可能小；</strong></li><li><strong>不同类样本点尽可能远离，则不同类的中心之间的距离尽可能大，即 $||w^T\mu_0-w^T\mu_1||_2^2$ 尽可能大.</strong></li></ul><p>同时考虑二者，我们就可以得到要最大化的目标：</p><script type="math/tex; mode=display">\begin{align}J &= \frac{||w^T\mu_0-w^T\mu_1||_2^2}{w^T\Sigma_0w+w^T\Sigma_1w}    \\&= \frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T(\Sigma_0+\Sigma_1)w}\end{align}    \tag{1}</script><p>我们给出 <strong>类内散度矩阵</strong>(<em>within-class scatter matrix</em>) 的定义</p><script type="math/tex; mode=display">\begin{align}S_w &= \Sigma_0 + \Sigma_1     \\&= \sum_{x\in X_0}(x-\mu_0)(x-u_0)^T+ \sum_{x\in X_1}(x-\mu_1)(x-\mu_1)^T\end{align}    \tag{2}</script><p>以及 <strong>类间散度矩阵</strong>(<em>between-class scatter matrix</em>) 的定义</p><script type="math/tex; mode=display">S_b =(\mu_0 -\mu_1)(\mu_0-\mu_1)^T    \tag{3}</script><p>于是 式$(1)$ 可以重写为</p><script type="math/tex; mode=display">J = \frac{w^TS_bw}{w^TS_w w}        \tag{4}</script><p>这个就是 LDA 要最大化的目标函数，即 $S_b$ 与 $S_w$ 的 <strong>广义瑞利商</strong>(<em>generalized Rayleigh quotient</em>) .</p><p>&emsp; 注意到式$(4)$的分子分母都是关于 $w $ 的二次项，因此式$(4)$的解与 $w$ 的长度无关，只与其方向有关.  于是，我们可以令 $w^TS_ww=1$ ，可得</p><script type="math/tex; mode=display">\begin{align}\min_w& \quad -w^TS_bw    \\s.t.& \quad w^TS_ww =1\end{align}    \tag{5}</script><p>应用拉格朗日乘子法，上式等价于</p><script type="math/tex; mode=display">S_bw = \lambda S_ww    \tag{6}</script><p>即</p><script type="math/tex; mode=display">S_w^{-1}S_b w = \lambda w    \tag{7}</script><p>这正好是<strong>特征值分解</strong>的形式，因此我们可以对 $S_w^{-1}S_b$ 特征值分解，得到的最大的特征值对应的特征向量即为降维后的投影方向 $w$ .  同理，如果是降到 $d$ 维的情况，只需选取最大的 $d$ 个特征值对应的特征向量张成投影矩阵即可.</p><p>另外，对于二类的情况，注意到 $S_bw$ 的方向恒为 $\mu_0 -\mu_1$,不妨令</p><script type="math/tex; mode=display">S_bw = \lambda(\mu_0-\mu_1)    \tag{8}</script><p>代入式$(6)$可得</p><script type="math/tex; mode=display">w = S^{-1}(\mu_0-\mu_1)    \tag{9}</script><p>即为最佳的投影方向.</p><h3 id="多分类情况"><a href="#多分类情况" class="headerlink" title="多分类情况"></a>多分类情况</h3><p>&emsp; 设$W=(w_1,w_2,…,w_d)$，即我们将数据压缩到 $d$ 维，$X_j$ 是第 $i$ 类样本的集合，$N_j$ 是第 $ j$ 类样本的个数，将式$(4)$推广到多分类情形时，有</p><script type="math/tex; mode=display">J = \frac{W^TS_bW}{W^TS_w W}    \tag{10}</script><p>其中，$S_b = \sum^k_{j=1}N_j(\mu_j-\mu)(\mu_j-\mu)^T$，$\mu$ 为所有样本的均值向量，$S_w = \sum^k_{j=1}S_{wj}=\sum^k_{j=1}\sum_{x\in X_j}(x-\mu_j)(x-\mu_j)^T$ .</p><p>&emsp; 问题在于，式$(10)$的分子和分母都是矩阵，而不是标量，没有办法作为一个标量函数来优化.  而针对这个问题，常见的一种方法是采用优化目标</p><script type="math/tex; mode=display">\arg\max_W\frac{tr(W^TS_bW)}{tr(W^TS_w W)}    \tag{11}</script><p>其中 $tr(·)$ 表示矩阵的<strong>迹</strong>(<em>trace</em>)，即<strong>主对角线元素的乘积</strong>. 于是，我们可以通过如下问题求解</p><script type="math/tex; mode=display">S_bW = \lambda S_w W    \tag{12}</script><p>正好和二类中的式(6)形式一样，因此我们也是同样采用对 $S_W^{-1}S_b$ 求特征值的方法，<strong>选取最大的 $d$ 个特征值对应的特征向量，把它们张成的矩阵作为降维的投影矩阵 $W$.</strong></p><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><blockquote><p>输入：数据集 $D=\{(x_1,y_1),(x_2,y_2),…,(x_m,y_m)\} $，其中任意样本 $x_i$ 为 $n$ 维向量，$y_i \in \{C_1,C_2,…,C_k\}$，降维后的维度 $d$.</p><ol><li><p>计算类内散度矩阵</p><script type="math/tex; mode=display">S_w = \sum^k_{j=1}S_{wj}=\sum^k_{j=1}\sum_{x\in X_j}(x-\mu_j)(x-\mu_j)^T</script></li><li><p>计算类间散度矩阵</p></li></ol><script type="math/tex; mode=display">S_b = \sum^k_{j=1}N_j(\mu_j-\mu)(\mu_j-\mu)^T</script><ol><li><p>计算矩阵</p><script type="math/tex; mode=display">S_w^{-1}S_b</script></li><li><p>对矩阵 $S^{-1}_wS_b$ 进行特征值分解，选取特征值最大的 $d$ 个特征值对应的特征向量，张成投影矩阵 $W =(w_1,w_2,…,w_d)$</p></li><li><p><strong>for</strong> i =1,2,…m <strong>do</strong></p><script type="math/tex; mode=display">z_i = W^Tx_i</script><p><strong>end for</strong></p></li><li><p>得到输出样本集 $D^{‘} = \{(z_1,y_1),(z_2,y_2),…,(z_m,y_m)\}$ .</p></li></ol><p>输出：降维后的样本集 $D^{‘}$ .</p></blockquote><p>&lt;/br&gt;</p><blockquote><p>参考资料：</p><p>[1] 刘建平Pinard. 博客园: 线性判别分析LDA原理总结, <a href="https://www.cnblogs.com/pinard/p/6244265.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6244265.html</a> , 2017-01-03/2018-08-10 .</p><p>[2] 周志华. 机器学习[M], 北京: 清华大学出版社, 2016: 60-63. </p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;线性判别分析-LDA&quot;&gt;&lt;a href=&quot;#线性判别分析-LDA&quot; class=&quot;headerlink&quot; title=&quot;线性判别分析(LDA)&quot;&gt;&lt;/a&gt;线性判别分析(LDA)&lt;/h1&gt;&lt;p&gt;&amp;emsp; &lt;strong&gt;线性判别分析&lt;/strong&gt;(&lt;em&gt;Linear Discriminant Analysis&lt;/em&gt;，简称LDA)，是一种有监督的降维方法.  其思想是希望投影到超平面上的点具有如下性质：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;同类的样本点尽可能地接近&lt;/li&gt;
&lt;li&gt;不同类的样本点尽可能远离&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如下图所示，右图的降维效果显然要好于左图.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog-fig.oss-cn-shenzhen.aliyuncs.com/lda1.png&quot; alt=&quot;图片来源：博客园刘建平&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="特征工程" scheme="http://yoursite.com/categories/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="数据降维" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E9%99%8D%E7%BB%B4/"/>
    
      <category term="线性判别分析" scheme="http://yoursite.com/tags/%E7%BA%BF%E6%80%A7%E5%88%A4%E5%88%AB%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>梯度提升树(GBDT)</title>
    <link href="http://yoursite.com/2018/08/08/ensemble-learning/GBDT/"/>
    <id>http://yoursite.com/2018/08/08/ensemble-learning/GBDT/</id>
    <published>2018-08-08T09:12:11.045Z</published>
    <updated>2018-12-03T16:04:28.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="梯度提升树-GBDT"><a href="#梯度提升树-GBDT" class="headerlink" title="梯度提升树(GBDT)"></a>梯度提升树(GBDT)</h1><p>&emsp; 提升树(Boosting Tree)是以分类树或回归树为基学习器的 <strong>Boosting 算法</strong>. 而因为面对不同的学习任务，会有不同的损失函数，而对于一般的损失函数，往往每一步的优化不容易，于是提出了<strong>梯度提升</strong>(<em>gradient boosting</em>)算法，即<strong>梯度提升树</strong>(<em>Gradient Boosting Decision Tree</em>, 简称<strong>GBDT</strong>).  总的来说，GBDT是一种学习性能非常好的算法.</p><a id="more"></a><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>&emsp; 我们知道，提升树是由分类树或回归树为基学习器的Boosting方法，因此提升树模型可以表示为决策树的加法模型：</p><script type="math/tex; mode=display">f_M(x) = \sum^M_{m=1}T(x;\theta_m)    \tag{1}</script><p>其中，$T(x;\theta_m)$ 表示决策树；$\theta_m$ 为决策树的参数；$M$ 为树的个数.</p><h2 id="前向分步算法"><a href="#前向分步算法" class="headerlink" title="前向分步算法"></a>前向分步算法</h2><h3 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h3><p>&emsp; 首先确定初始提升树 $f_0(x)=0$，第 $m$ 步的模型是</p><script type="math/tex; mode=display">f_m(x) =f_{m-1}(X) + T(x;\theta_m)    \tag{2}</script><p>其中，$f_{m-1}(x)$ 为当前模型，通过经验风险极小化确定下一棵决策树的参数 $\theta$，</p><script type="math/tex; mode=display">\theta_m = \arg\min_{\theta_m}\sum^N_{i=1}L(y_i,f_{m-1}(x_i)+T(x_i;\theta_m))    \tag{3}</script><p>由于树的线性组合可以很好地拟合训练数据，即使数据中的输入与输出之间的关系很复杂，所以提升树是一个高功能的学习算法.</p><p>&emsp; 给定一个训练集 $D = \{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\}$ ，我们将输入的属性集 $X$ 划分为 $J$ 个互不相交的子集 $R_1,R_2,…,R_J$，并且在每个子集上确定输出的常量 $c_j$，那么树可以表示为：</p><script type="math/tex; mode=display">T(x;\theta) = \sum^J_{j=1}c_jI(x\in R_j)    \tag{4}</script><p>其中 $\theta = \{(R_1,c_1),(R_2,c_2),…,(R_J,c_J)\}$ 表示树的划分和所对应划分的输出常量；$J$ 是叶结点的个数.</p><p>&emsp; 我们使用如下的前向分步算法：</p><script type="math/tex; mode=display">\begin{align}f_0(x) &= 0    \\f_m(x) &= f_{m-1}(x) + T(x;\theta_m) , \quad m=1,2,...,M    \\f_M(x) &= \sum^M_{m=1} T(x;\theta_m)\end{align}</script><p>于是，在算法的第 $m$ 步，我们需要求解：</p><script type="math/tex; mode=display">\theta_m = \arg\min_{\theta_m} \sum^N_{i=1}L(y_i;f_{m-1}(x_i)+T(x;\theta_m))</script><p>得到 $\theta_m$ ，即第 $m$ 棵树的参数.</p><p>&emsp; 如果我们使用<strong>平方误差函数</strong>，</p><script type="math/tex; mode=display">L(y,f(x) ) = (y-f(x))^2</script><p>其损失变为</p><script type="math/tex; mode=display">\begin{align}L&(y,f_{m-1}(x)+T(x;\theta_m))    \\&= [y-f_{m-1}(x)-T(x;\theta_m)]^2    \\&= [r-T(x;\theta_m)]^2\end{align}</script><p>这里，</p><script type="math/tex; mode=display">r = y-f_{m-1}(x)    \tag{5}</script><p>是当前模型拟合数据的<strong>残差</strong>(residual) . 所以，提升树算法实际上是不断地训练出能拟合上一个树产生的残差的当前树.</p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><blockquote><p>输入：训练数据集 $D = \{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\}$</p><p>初始化：</p><script type="math/tex; mode=display">f_0(x) = 0</script><p><strong>for</strong> $m = 1, 2, …, M$ <strong>do</strong></p><p>&emsp; 计算残差:</p><script type="math/tex; mode=display">r_{mi} = y_i - f_{m-1}(x_i) , \ i =1,2,...,N</script><p>&emsp; 拟合残差 $r_{mi}$ 学习一个回归树，得到 $T(x;\theta_m)$</p><p>&emsp; 更新 $f_m(x) = f_{m-1}(x) + T(x;\theta_m)$</p><p><strong>end for</strong></p><p>得到提升树</p><script type="math/tex; mode=display">f_M(x) = \sum^M_{m=1}T(x;\theta_m)</script><p>输出：提升树 $f_M(x)$ .</p></blockquote><p> &emsp; 需注意的是，无论是面对回归任务还是分类任务，提升树的子树都是 CART 回归树.  因为这样，残差的计算才有意义.  但由于分类问题的损失函数不是平方损失，因此不能由上述的算法简单地求解.</p><h2 id="梯度提升算法"><a href="#梯度提升算法" class="headerlink" title="梯度提升算法"></a>梯度提升算法</h2><p>&emsp; 在上述的的前向分步算法中，当损失函数是平方损失函数时，每一步的优化是很简单的.  但是面对不同的任务时，会有不同的损失函数，对于一般的损失函数而言，每一步的优化往往不那么简单.  于是，<strong>Friedman</strong> 提出了<strong>梯度提升</strong>(<em>gradient boosting</em>)算法. 关键是利用损失函数的负梯度在当前模型的值，作为残差的估计，即</p><script type="math/tex; mode=display">r_{mi}=-[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}</script><blockquote><p>输入：训练数据集 $D =\{(x_1,y_1),(x_2,y_2),…,(x_N,y_N)\}$，损失函数 $L(y,f(x))$</p><p>初始化：</p><script type="math/tex; mode=display">f_0(x)= \arg\min_c \sum^N_{i=1}L(y_i,c)</script><p><strong>for</strong> $m =1,2,…,M$  <strong>do</strong></p><p>&emsp; <strong>for</strong> $i=1,2,…,N$ <strong>do</strong></p><script type="math/tex; mode=display">r_{mi}=-[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}</script><p>&emsp; <strong>end for</strong></p><p>&emsp; 对 $r_{mi}$ 拟合一个回归树，得到第 $m$ 棵树的叶结点的属性子集 $R_{mj},  j=1,2,…,J$</p><p>&emsp; <strong>for</strong> $ j =1,2,…,J $ <strong>do</strong></p><script type="math/tex; mode=display">c_{mj} =\arg\min_c \sum_{x\in R_{mj}}L(y_i,f_{m-1}(x_i)+c)</script><p>&emsp; <strong>end for</strong></p><p>&emsp; <strong>update</strong></p><script type="math/tex; mode=display">f_m(x) = f_{m-1}(x) + \sum^J_{j=1}c_{mj}I(x\in R_{mj})</script><p><strong>end for</strong></p><p>得到梯度提升树</p><script type="math/tex; mode=display">f_M(x) = \sum^M_{m=1}\sum^J_{j=1}c_{mj}I(x\in R_{mj})</script><p>输出：梯度提升树 $f_M(x)$ .</p></blockquote><p>&gt;</p><blockquote><p>参考资料：<br>[1] 李航. 统计学习方法[M], 北京: 清华大学出版社, 2012: 146-152.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;梯度提升树-GBDT&quot;&gt;&lt;a href=&quot;#梯度提升树-GBDT&quot; class=&quot;headerlink&quot; title=&quot;梯度提升树(GBDT)&quot;&gt;&lt;/a&gt;梯度提升树(GBDT)&lt;/h1&gt;&lt;p&gt;&amp;emsp; 提升树(Boosting Tree)是以分类树或回归树为基学习器的 &lt;strong&gt;Boosting 算法&lt;/strong&gt;. 而因为面对不同的学习任务，会有不同的损失函数，而对于一般的损失函数，往往每一步的优化不容易，于是提出了&lt;strong&gt;梯度提升&lt;/strong&gt;(&lt;em&gt;gradient boosting&lt;/em&gt;)算法，即&lt;strong&gt;梯度提升树&lt;/strong&gt;(&lt;em&gt;Gradient Boosting Decision Tree&lt;/em&gt;, 简称&lt;strong&gt;GBDT&lt;/strong&gt;).  总的来说，GBDT是一种学习性能非常好的算法.&lt;/p&gt;
    
    </summary>
    
    
      <category term="集成学习" scheme="http://yoursite.com/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="集成学习" scheme="http://yoursite.com/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Boosting" scheme="http://yoursite.com/tags/Boosting/"/>
    
  </entry>
  
  <entry>
    <title>主成分分析</title>
    <link href="http://yoursite.com/2018/08/07/descend-dimension/PCA/"/>
    <id>http://yoursite.com/2018/08/07/descend-dimension/PCA/</id>
    <published>2018-08-07T09:28:49.859Z</published>
    <updated>2018-12-03T16:04:28.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="主成分分析-PCA"><a href="#主成分分析-PCA" class="headerlink" title="主成分分析(PCA)"></a>主成分分析(PCA)</h1><p>&emsp;<strong>主成分分析</strong>(<em>Principal Component Analysis</em> , 简称 <em>PCA</em> ) 是最常用的一种降维方法.  其求解主要有对样本的协方差矩阵的<strong>特征值分解</strong>和对原样本矩阵的<strong>奇异值分解</strong>两种方法.</p><a id="more"></a><h2 id="特征值分解"><a href="#特征值分解" class="headerlink" title="特征值分解"></a>特征值分解</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>&emsp;对于样本空间是高维的情况，我们希望用一个超平面对进行所有样本进行恰当的表达，因此我们希望这个超平面满足如下两点性质：</p><ul><li><strong>最近重构性</strong>：样本点到这个超平面的距离都足够近 ;</li><li><strong>最大可分性</strong>：样本点在这个超平面上的投影能尽可能分开 .</li></ul><p>&emsp;我们基于上述两点性质对PCA中特征值分解的方法进行推导，首先是基于最近重构性的推导：</p><p>&emsp;我们假设样本点进行了中心化，即 $\sum_i x_i =0$; 再假定投影变换后得到的新坐标系为 $\{w_1,w_2,…,w_d\}$ ，其中 $w_i$ 是标准正交基向量，即 $||w_i||_2 =1 , w_i^Tw_j =0$ . 如果丢弃新坐标系中的部分坐标，即将维度降到 $d^{‘}&lt;d$，则样本点在新坐标系下的投影是 $z_i = (z_{i1};z_{i2};…;z_{id^{‘}})$，其中 $z_{ij}= w^T_jx_i$ 是 $x_i$ 在低维坐标系下第 $j$ 维的坐标.  若基于 $z_i$ 来重构 $x_i$，则会得到 $\hat{x_i} = \sum_{j=1}^{d^{‘}}z_{ij}w_j$ .</p><p>&emsp; 考虑整个训练集，那么原样本点和基于投影重构的样本点 $\hat{x_i}$ 之间的距离为：</p><script type="math/tex; mode=display">\sum^m_{i=1}||\sum^{d^{'}}_{j=1}z_{ij}w_j-x_i||_2^2 = \sum_{i=1}^mz_i^Tz_i-2\sum^m_{i=1}z^T_iW^Tx_i + C    \\\alpha  - (W^T(\sum^m_{i=1}x_ix_i^T)W)    \tag{1}</script><p>即PCA的目标为：</p><script type="math/tex; mode=display">\begin{align}&\min_W \ - tr(W^T(\sum^m_{i=1}x_ix_i^T)W)    \\&s.t. \ W^TW = I\end{align}    \tag{2}</script><p>&emsp; 我们从最大可分性出发，希望所有样本点的投影尽可能分开，即希望投影后的样本点方程最大化.</p><p>&emsp; 投影后样本点的方差是 $\sum_i W^Tx_ix_i^TW$ ，于是优化目标可写为：</p><script type="math/tex; mode=display">\begin{align}&\max_W \  tr(W^TXX^TW)    \\&s.t. \ W^TW = I\end{align} \tag{3}</script><p>显然，式 (2) 与 式 (3) 等价 .</p><p>于是，使用<strong>拉格朗日乘子法</strong>可得：</p><script type="math/tex; mode=display">XX^Tw_i = \lambda_iw_i    \tag{4}</script><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><blockquote><p>输入：样本集 $D = \{x_1,x_2,…,x_m\}$;  低维空间维数 $d^{‘}$.</p><ol><li>对所有样本进行中心化：$x_i := x_i - \frac{1}{m}\sum^m_{i=1}x_i$ ;</li><li>计算样本的协方差矩阵 $XX^T$ ;</li><li>对协方差矩阵 $XX^T$ 进行特征值分解 ;</li><li>取最大的 $d^{‘}$ 个特征值所对应的特征向量 $w_1,w_2,…,w_{d^{‘}}$.</li></ol><p>输出：投影矩阵 $W^\ast =(w_1,w_2,…,w_{d^{‘}})$ .</p></blockquote><h2 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h2><p>&emsp;<strong>奇异值分解</strong>(<em>Sigular Value Decomposition</em>, 简称SVD)，是机器学习广泛应用的算法，这里我们简单介绍奇异值分解的原理，及其在降维中的应用.</p><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>&emsp; 假设矩阵 $A$ 是一个 $m\times n$ 矩阵，那么我们定义 $A$ 的 $SVD$ 为：</p><script type="math/tex; mode=display">A = U\Sigma V^T</script><p>其中 $U$ 是一个 $m \times m $ 矩阵，$\Sigma$ 是一个 $m\times n $ 矩阵，除了对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，$V $ 是一个 $n\times n$ 矩阵，如下图所示</p><p><img src="http://pbug3xg5x.bkt.clouddn.com/svd1.png" alt="SVD"></p><p>其中，我们对 $AA^T$ 进行特征分解，有：</p><script type="math/tex; mode=display">(AA^T)u_i =\lambda_i u_i</script><p>于是，将 $AA^T$ 的所有特征向量张成一个 $m\times m$ 矩阵，就是 $U$ 矩阵.</p><p>同理，将 $A^TA$ 的所有特征向量张成一个 $n\times n $  矩阵，就是 $V$ 矩阵.</p><p>$\Sigma$  对角线上的每个元素(奇异值) $\sigma_i$ 和特征值 $\lambda_i$ 有如下关系</p><script type="math/tex; mode=display">\sigma_i = \sqrt{\lambda_i}</script><h3 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h3><p>&emsp; 奇异值与特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值减少得特别快，很多情况下，前10%甚至1%的奇异值的和就占了全部奇异值之和的99%以上.  所以，我们可以用最大的 k 个奇异值和对应的左右奇异向量来近似描述矩阵.</p><script type="math/tex; mode=display">A_{m\times n} = U_{m\times m} \Sigma_{m\times n } V^T_{n\times n} \approx U_{m\times k}\Sigma_{k\times k} V^T_{k\times n}</script><p>即我们可以用一小部分的$U、V、\Sigma$ 来近似描述 $A$ ，如下图所示</p><p><img src="http://pbug3xg5x.bkt.clouddn.com/svd2.png" alt="SVD2"></p><h3 id="应用于PCA"><a href="#应用于PCA" class="headerlink" title="应用于PCA"></a>应用于PCA</h3><p>&emsp; 在 PCA 中，我们进行如下处理，右乘矩阵 $V$，即</p><script type="math/tex; mode=display">X^{'}_{m\times d} = X_{m\times n} V_{n\times k}</script><p>即可得到降维后的数据.</p><p>另外，如果我们对原特征矩阵左乘 $U^T$, 即</p><script type="math/tex; mode=display">X^{'}_{d\times n} = U^T_{d\times m} X_{m\times n}</script><p>则可以对行数进行压缩.</p><p>&emsp;  有一些算法可以不用先求出协方差矩阵 $X^TX$，也能求出右奇异矩阵 $V$，因此，SVD 要比特征值分解高效.</p><blockquote><p>参考资料：<br>[1] 周志华. 机器学习[M], 北京: 清华大学出版社, 2016: 229-232.</p><p>[2] 刘建平Pinard. 奇异值分解(SVD)原理与在降维中的应用[EB/OL], <a href="http://www.cnblogs.com/pinard/p/6251584.html" target="_blank" rel="noopener">http://www.cnblogs.com/pinard/p/6251584.html</a>, 2017-01-05/2018-08-07 .</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;主成分分析-PCA&quot;&gt;&lt;a href=&quot;#主成分分析-PCA&quot; class=&quot;headerlink&quot; title=&quot;主成分分析(PCA)&quot;&gt;&lt;/a&gt;主成分分析(PCA)&lt;/h1&gt;&lt;p&gt;&amp;emsp;&lt;strong&gt;主成分分析&lt;/strong&gt;(&lt;em&gt;Principal Component Analysis&lt;/em&gt; , 简称 &lt;em&gt;PCA&lt;/em&gt; ) 是最常用的一种降维方法.  其求解主要有对样本的协方差矩阵的&lt;strong&gt;特征值分解&lt;/strong&gt;和对原样本矩阵的&lt;strong&gt;奇异值分解&lt;/strong&gt;两种方法.&lt;/p&gt;
    
    </summary>
    
    
      <category term="特征工程" scheme="http://yoursite.com/categories/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="数据降维" scheme="http://yoursite.com/tags/%E6%95%B0%E6%8D%AE%E9%99%8D%E7%BB%B4/"/>
    
      <category term="主成分分析" scheme="http://yoursite.com/tags/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>Random Forest</title>
    <link href="http://yoursite.com/2018/08/07/ensemble-learning/Random-Forest/"/>
    <id>http://yoursite.com/2018/08/07/ensemble-learning/Random-Forest/</id>
    <published>2018-08-07T01:58:03.038Z</published>
    <updated>2018-12-03T16:04:28.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Random-Forest"><a href="#Random-Forest" class="headerlink" title="Random Forest"></a>Random Forest</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>&emsp; 我们知道，Bagging 算法在样本选择的时候采用<strong>自助采样法</strong>，这样的好处在于引入了随机性，使得最终学习器不容易过拟合，而在<strong>随机森林</strong>(<em>Random Forest</em>) 中，我们进一步引入随机性：在决策树的训练过程中引入随机属性选择.  最终由多个决策树共同组成随机森林.</p><p>&emsp; 令人惊讶的是，这样一种原理简单的算法，在许多任务中都有非常优秀的性能.</p><a id="more"></a><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><blockquote><p>输入：训练样本 $D$，决策树数目 $T$，决策树深度 $H$</p><p><strong>for</strong> $t = 1, 2, …, T$  <strong>do</strong></p><p>&emsp; 自助采样法对样本进行采样，得到采样集 $D_{t}$;</p><p>&emsp; 随机从 $D_t$ 中选取包含 $k$ 个属性的子集 $D_{t,sub}$;</p><p>&emsp; 基于 $D_{t,sub}$ 训练决策树 $tree_t$ .</p><p><strong>end for</strong></p><p>$forest = (tree_1, tree_2, …, tree_T)$ .</p><p>输出：随机森林 forest .</p></blockquote><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol><li>准确率高.</li><li>随机性的引入很好地解决过拟合的问题.</li><li>随机性的引入，使随机森林具有很好的抗噪声能力.</li><li>可以在不做特征选择的情况下，处理高维度的数据.</li><li>能处理离散型数据，也能处理连续型数据，无需规范化数据.</li><li>训练速度快，容易实现并行化.</li></ol><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li>当决策树的个数很多时，需要的空间和时间比较大.</li><li>算法的可解释性不强.</li></ol><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random <span class="keyword">as</span> rd</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">from</span> cart_tree <span class="keyword">import</span> build_classify_tree, predict </span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(dir)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    导入数据</span></span><br><span class="line"><span class="string">    :param dir:数据的路径</span></span><br><span class="line"><span class="string">    :return: 放入学习器的数据</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    tmp = np.loadtxt(dir, dtype=np.str, delimiter=<span class="string">","</span>)</span><br><span class="line">    data = tmp[<span class="number">1</span>:<span class="number">500</span>, <span class="number">1</span>:<span class="number">-2</span>].astype(np.float)</span><br><span class="line">    label = tmp[<span class="number">1</span>:<span class="number">500</span>, <span class="number">-1</span>:].astype(np.float)</span><br><span class="line">    <span class="comment"># 将数据进行标准化</span></span><br><span class="line">    x_std = standard_transform(data)</span><br><span class="line">    <span class="keyword">return</span> np.hstack((x_std, label))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standard_transform</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    将数据进行标准化处理</span></span><br><span class="line"><span class="string">    :param data: 原始数据</span></span><br><span class="line"><span class="string">    :return: 处理后的数据</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> (data - np.mean(data, axis=<span class="number">0</span>))/np.std(data, axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_forest_training</span><span class="params">(data_train, trees_num)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练随机森科</span></span><br><span class="line"><span class="string">    :param data_train: 训练集</span></span><br><span class="line"><span class="string">    :param trees_num: 树的个数</span></span><br><span class="line"><span class="string">    :return: trees_result: 训练好的决策树集</span></span><br><span class="line"><span class="string">             trees_feature: 决策树集对应的特征集</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    trees_result = []</span><br><span class="line">    trees_feature = []</span><br><span class="line">    n = np.shape(data_train)[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> n &gt; <span class="number">2</span>:</span><br><span class="line">        k = int(log(n, <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        k = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(trees_num):</span><br><span class="line">        data_samples, feature = choose_samples(data_train, k)</span><br><span class="line">        <span class="comment"># print(data_samples)</span></span><br><span class="line">        tree = build_classify_tree(data_samples, min_sample=<span class="number">0</span>, min_gain=<span class="number">0</span>)</span><br><span class="line">        trees_result.append(tree)</span><br><span class="line">        trees_feature.append(feature)</span><br><span class="line">    <span class="keyword">return</span> trees_result, trees_feature</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_samples</span><span class="params">(data, k)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    随机选择样本和特征集</span></span><br><span class="line"><span class="string">    :param data: 训练集</span></span><br><span class="line"><span class="string">    :param k: 选取的特征数</span></span><br><span class="line"><span class="string">    :return: data_samples: 样本集</span></span><br><span class="line"><span class="string">             feature: 特征集</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m, n = np.shape(data)</span><br><span class="line">    feature = []</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(k):</span><br><span class="line">        feature.append(rd.randint(<span class="number">0</span>, n - <span class="number">2</span>))</span><br><span class="line">    index = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        index.append(rd.randint(<span class="number">0</span>, m<span class="number">-1</span>))</span><br><span class="line">    data_samples = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        data_tmp = []</span><br><span class="line">        <span class="keyword">for</span> fea <span class="keyword">in</span> feature:</span><br><span class="line">            data_tmp.append(data[index[i]][fea])</span><br><span class="line">        data_tmp.append(data[index[i]][<span class="number">-1</span>])</span><br><span class="line">        data_samples.append(data_tmp)</span><br><span class="line">    print(data_samples)</span><br><span class="line">    <span class="keyword">return</span> data_samples, feature</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_data</span><span class="params">(data_test, feature)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    根据树中的特征集切分样本</span></span><br><span class="line"><span class="string">    :param data_test: 样本集</span></span><br><span class="line"><span class="string">    :param feature: 特征集</span></span><br><span class="line"><span class="string">    :return: data: 切分后的样本</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = np.shape(data_test)[<span class="number">0</span>]</span><br><span class="line">    data = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        data_x_tmp = []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> feature:</span><br><span class="line">            data_x_tmp.append(data_test[i][x])</span><br><span class="line">        data_x_tmp.append(data_test[i][<span class="number">-1</span>])</span><br><span class="line">        data.append(data_x_tmp)</span><br><span class="line">    <span class="comment"># print(data)</span></span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_predict_classify</span><span class="params">(trees_result, trees_feature, data_test)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对训练好的模型进行分类预测</span></span><br><span class="line"><span class="string">    :param trees_result: 树的集合</span></span><br><span class="line"><span class="string">    :param trees_feature: 特征集合</span></span><br><span class="line"><span class="string">    :param data_test: 测试集或验证集</span></span><br><span class="line"><span class="string">    :return: final_predict: 预测结果集</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m_tree = len(trees_result)</span><br><span class="line">    m = np.shape(data_test)[<span class="number">0</span>]</span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m_tree):</span><br><span class="line">        clf = trees_result[i]</span><br><span class="line">        feature = trees_feature[i]</span><br><span class="line">        data = split_data(data_test, feature)</span><br><span class="line">        result_i = []</span><br><span class="line">        <span class="comment"># print(data)</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">            <span class="comment"># print(predict(data[i][0:-1], clf))</span></span><br><span class="line">            pre = list(predict(data[j][<span class="number">0</span>:<span class="number">-1</span>], clf).values())</span><br><span class="line">            <span class="comment"># print(pre)</span></span><br><span class="line">            <span class="comment"># print("--------------")</span></span><br><span class="line">            result_i.append(pre)</span><br><span class="line">        result.append(result_i)</span><br><span class="line"></span><br><span class="line">    final_predict = np.sum(result, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment"># print("==============")</span></span><br><span class="line">    <span class="comment"># (np.shape(final_predict))</span></span><br><span class="line">    <span class="keyword">return</span> final_predict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_correct_rate</span><span class="params">(data_test, final_predict)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算正确率</span></span><br><span class="line"><span class="string">    :param data_test: 验证集</span></span><br><span class="line"><span class="string">    :param final_predict: 预测结果集</span></span><br><span class="line"><span class="string">    :return: corr/m: 正确率</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = np.shape(final_predict)[<span class="number">0</span>]</span><br><span class="line">    corr = <span class="number">0.0</span></span><br><span class="line">    <span class="comment"># print(final_predict)</span></span><br><span class="line">    pre = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="comment"># print(final_predict[i][0])</span></span><br><span class="line">        <span class="comment"># print("--------")</span></span><br><span class="line">        <span class="comment"># print(final_predict[i][1])</span></span><br><span class="line">        <span class="keyword">if</span> final_predict[i][<span class="number">0</span>] &gt; final_predict[i][<span class="number">1</span>]:</span><br><span class="line">            <span class="keyword">if</span> data_test[i][<span class="number">-1</span>] == <span class="number">0</span>:</span><br><span class="line">                corr += <span class="number">1</span></span><br><span class="line">            pre.append(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pre.append(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> data_test[i][<span class="number">-1</span>] == <span class="number">1</span>:</span><br><span class="line">                corr += <span class="number">1</span></span><br><span class="line">    <span class="comment"># print(corr)</span></span><br><span class="line">    <span class="comment"># print(m)</span></span><br><span class="line">    <span class="keyword">return</span> corr / m</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">save_model</span><span class="params">(trees_result, trees_feature, result_flie, feature_file)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    保存模型</span></span><br><span class="line"><span class="string">    :param trees_result: 训练好的随机森林模型</span></span><br><span class="line"><span class="string">    :param trees_feature: 对应的特征集</span></span><br><span class="line"><span class="string">    :param result_flie: 保存的模型路径</span></span><br><span class="line"><span class="string">    :param feature_file: 保存的特征集路径</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = len(trees_feature)</span><br><span class="line">    f_fea = open(feature_file, <span class="string">"w"</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        fea_tmp = []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> trees_feature[i]:</span><br><span class="line">            fea_tmp.append(str(x))</span><br><span class="line">        f_fea.writelines(<span class="string">"\t"</span>.join(fea_tmp) + <span class="string">"\n"</span>)</span><br><span class="line">    f_fea.close()</span><br><span class="line">    <span class="keyword">with</span> open(result_flie, <span class="string">"wb+"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        pickle.dump(trees_result, f)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_model</span><span class="params">(result_file, feature_file)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    加载模型</span></span><br><span class="line"><span class="string">    :param result_file: 保存的随机森林的模型</span></span><br><span class="line"><span class="string">    :param feature_file: 保存的对应的特征集</span></span><br><span class="line"><span class="string">    :return: trees_result: 随机森林的模型</span></span><br><span class="line"><span class="string">             trees_feature: 随机森林对应的特征</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    trees_feature = []</span><br><span class="line">    f_fea = open(feature_file)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> f_fea.readlines():</span><br><span class="line">        lines = line.strip().split(<span class="string">"\t"</span>)</span><br><span class="line">        tmp = []</span><br><span class="line">        <span class="keyword">for</span> x <span class="keyword">in</span> lines:</span><br><span class="line">            tmp.append(int(x))</span><br><span class="line">        trees_feature.append(tmp)</span><br><span class="line">    f_fea.close()</span><br><span class="line">    <span class="keyword">with</span> open(result_file, <span class="string">"rb+"</span>) <span class="keyword">as</span> f :</span><br><span class="line">        trees_result = pickle.load(f)</span><br><span class="line">    <span class="keyword">return</span> trees_result, trees_feature</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    模型训练</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    t = time.time()</span><br><span class="line">    print(<span class="string">"--------load data-------"</span>)</span><br><span class="line">    data_train = load_data(<span class="string">"train_data.csv"</span>)</span><br><span class="line">    trees_result, trees_feature = random_forest_training(data_train, <span class="number">140</span>)</span><br><span class="line">    result = get_predict_classify(trees_result, trees_feature, data_train)</span><br><span class="line">    corr_rate = cal_correct_rate(data_train, result)</span><br><span class="line">    print(<span class="string">"&#123;:.2%&#125;"</span>.format(corr_rate))</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"--------save model-------"</span>)</span><br><span class="line">    save_model(trees_result, trees_feature, <span class="string">"result_file"</span>, <span class="string">"feature_file"</span>)</span><br><span class="line">    print(<span class="string">"cost time: "</span> + str(time.time() - t))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">test</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    模型测试</span></span><br><span class="line"><span class="string">    :return: </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    t = time.time()</span><br><span class="line">    print(<span class="string">"--------load ver-------"</span>)</span><br><span class="line">    data_test = load_data(<span class="string">"verification_data.csv"</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"--------load model-------"</span>)</span><br><span class="line">    trees_result, trees_feature = load_model(<span class="string">"result_file"</span>, <span class="string">"feature_file"</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"--------predict-------"</span>)</span><br><span class="line">    result = get_predict_classify(trees_result, trees_feature, data_test)</span><br><span class="line">    corr_rate = cal_correct_rate(data_test, result)</span><br><span class="line">    print(<span class="string">"&#123;:.2%&#125;"</span>.format(corr_rate))</span><br><span class="line">    print(<span class="string">"cost time: "</span> + str(time.time() - t))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    train()</span><br><span class="line">    test()</span><br></pre></td></tr></table></figure><p>引用的CART算法在<a href="https://uniblog.cn/2018/06/08/decision-tree/" target="_blank" rel="noopener">CART</a>中有详细代码示例.  注意到此算法随机性较强，每次运行的验证正确率都会有所浮动.</p><blockquote><p>参考资料：</p><p>[1] 周志华. 机器学习[M], 北京: 清华大学出版社, 2016:179-180.</p><p>[2] shjyoudp. CSDN: 随机森林算法学习(RandomForest),</p><p><a href="https://blog.csdn.net/qq547276542/article/details/78304454" target="_blank" rel="noopener">https://blog.csdn.net/qq547276542/article/details/78304454</a>, 2017-10-21/2018-08-08.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Random-Forest&quot;&gt;&lt;a href=&quot;#Random-Forest&quot; class=&quot;headerlink&quot; title=&quot;Random Forest&quot;&gt;&lt;/a&gt;Random Forest&lt;/h1&gt;&lt;h2 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h2&gt;&lt;p&gt;&amp;emsp; 我们知道，Bagging 算法在样本选择的时候采用&lt;strong&gt;自助采样法&lt;/strong&gt;，这样的好处在于引入了随机性，使得最终学习器不容易过拟合，而在&lt;strong&gt;随机森林&lt;/strong&gt;(&lt;em&gt;Random Forest&lt;/em&gt;) 中，我们进一步引入随机性：在决策树的训练过程中引入随机属性选择.  最终由多个决策树共同组成随机森林.&lt;/p&gt;
&lt;p&gt;&amp;emsp; 令人惊讶的是，这样一种原理简单的算法，在许多任务中都有非常优秀的性能.&lt;/p&gt;
    
    </summary>
    
    
      <category term="集成学习" scheme="http://yoursite.com/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="集成学习" scheme="http://yoursite.com/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Bagging" scheme="http://yoursite.com/tags/Bagging/"/>
    
  </entry>
  
  <entry>
    <title>AdaBoost</title>
    <link href="http://yoursite.com/2018/08/06/ensemble-learning/AdaBoost/"/>
    <id>http://yoursite.com/2018/08/06/ensemble-learning/AdaBoost/</id>
    <published>2018-08-06T10:11:38.598Z</published>
    <updated>2018-12-03T16:04:28.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>&emsp; 我们知道 AdaBoost 是一种 Boosting 算法，而 Boosting 的核心问题在于：如何在每一轮改变训练数据的权值或概率分布；如何将弱分类器组合成一个强分类器.  而 AdaBoost 的做法是，提高那些被前一轮弱分类器错误分类的样本权值，而降低那些正确分类样本的权值；采用加权多数表决的方法，加大分类误差率小的弱分类器的权值，减小分类误差率大的弱分类器的权值. </p><a id="more"></a><h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><blockquote><p>输入：训练数据集 $T={(x_1,y_1),(x_2,y_2),…,(x_N,y_N)}$，其中 $x_i \in \chi \subseteq R^n$，$y_i \in \gamma = \{-1,+1\}$；弱学习算法；</p><p>初始化训练数据的权值分布：</p><script type="math/tex; mode=display">D_1=(w_{11},...,w_{1i},...,w_{1N}), \quad w_{1i} = \frac{1}{N}, \quad i=1,2...,N</script><p><strong>for</strong> m = 1, 2, …, M <strong>do</strong></p><p>&emsp;使用具有权值分布 $D_m$ 的训练数据集学习，得到基本分类器</p><script type="math/tex; mode=display">G_m(x) :\chi \rightarrow \{-1,+1\}</script><p>&emsp;计算 $G_m(x)$ 在训练数据集上的分类误差率</p><script type="math/tex; mode=display">e_m = \sum_{i=1}^NP(G_m(x_i)\neq y_i)=\sum^N_{i=1}w_{mi}I(G_m(x_i)\neq y_i)</script><p>&emsp;计算 $G_m(x)$ 的系数</p><script type="math/tex; mode=display">\alpha_m = \frac{1}{2} \log \frac{1-e_m}{e_m}</script><p>&emsp;更新训练数据集的权值分布</p><script type="math/tex; mode=display">w_{m+1,i} = \frac{w_{mi}}{Z_m}exp (-\alpha_my_iG_m(x_i)), \quad i=1,2,...,N    \\D_{m+1} = (w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N})</script><p>这里，$Z_m$ 是规范化因子</p><script type="math/tex; mode=display">Z_m = \sum^N_{i=1}w_{mi}exp(-\alpha_my_iG_m(x_i))</script><p>它使 $D_{m+1}$ 成为一个概率分布.</p><p><strong>end for</strong></p><p>构建基本分类器的线性组合</p><script type="math/tex; mode=display">f(x) = \sum^M_{m=1}\alpha_mG_m(x)</script><p>得到最终分类器</p><script type="math/tex; mode=display">G(x) = sign(f(x)) = sign(\sum^M_{m=1}\alpha_mG_m(x))</script><p>输出：最终分类器 $G(x)$.</p></blockquote><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol><li>很好地训练弱分类器，并且将它们线性结合.</li><li>可以将不同的分类算法作为弱分类器.</li><li>具有很高的精度.</li><li>相对于 Bagging，充分考虑了每个分类器的权重.</li></ol><h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><ol><li>迭代次数，即弱分类器数目不好设定，可以通过交叉验证来确定.</li><li>数据不平衡导致分类精度下降.</li><li>训练耗时.</li></ol><blockquote><p>参考资料：</p><p>[1] 李航. 统计学习方法[M], 北京: 清华大学出版社, 2012: 138-146.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;AdaBoost&quot;&gt;&lt;a href=&quot;#AdaBoost&quot; class=&quot;headerlink&quot; title=&quot;AdaBoost&quot;&gt;&lt;/a&gt;AdaBoost&lt;/h1&gt;&lt;h2 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h2&gt;&lt;p&gt;&amp;emsp; 我们知道 AdaBoost 是一种 Boosting 算法，而 Boosting 的核心问题在于：如何在每一轮改变训练数据的权值或概率分布；如何将弱分类器组合成一个强分类器.  而 AdaBoost 的做法是，提高那些被前一轮弱分类器错误分类的样本权值，而降低那些正确分类样本的权值；采用加权多数表决的方法，加大分类误差率小的弱分类器的权值，减小分类误差率大的弱分类器的权值. &lt;/p&gt;
    
    </summary>
    
    
      <category term="集成学习" scheme="http://yoursite.com/categories/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="集成学习" scheme="http://yoursite.com/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Boosting" scheme="http://yoursite.com/tags/Boosting/"/>
    
  </entry>
  
</feed>
