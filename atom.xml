<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>One Blog</title>
  
  <subtitle>好好学习</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2021-01-06T15:09:16.961Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Yao-zz</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Python collections 简单上手</title>
    <link href="http://yoursite.com/2021/01/06/python/collections/"/>
    <id>http://yoursite.com/2021/01/06/python/collections/</id>
    <published>2021-01-06T15:09:16.961Z</published>
    <updated>2021-01-06T15:09:16.961Z</updated>
    
    <content type="html"><![CDATA[<h1 id="collections-库简单上手"><a href="#collections-库简单上手" class="headerlink" title="collections 库简单上手"></a>collections 库简单上手</h1><h2 id="Counter"><a href="#Counter" class="headerlink" title="Counter"></a>Counter</h2><p>   Counter 是 dictionary 对象的子类。collections 模块中的 Counter() 函数会接收一个诸如 list 或 tuple 的迭代器，然后返回一个 Counter dictionary。这个 dictionary 的键是该迭代器中的唯一元素，每个键的值是迭代器元素的计数。</p><a id="more"></a><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">lst = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line">counter = Counter(lst)</span><br><span class="line">print(counter)</span><br></pre></td></tr></table></figure><pre><code>Counter({1: 7, 2: 5, 3: 3})</code></pre><p>most_common()函数会以元组列表的形式，返回出现频率最高的n个元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">counter.most_common(<span class="number">2</span>)</span><br></pre></td></tr></table></figure><pre><code>[(1, 7), (2, 5)]</code></pre><h2 id="defaultdict"><a href="#defaultdict" class="headerlink" title="defaultdict"></a>defaultdict</h2><p>defaultdict 的工作方式和平常的 python dictionary 完全相同，只是当你试图访问一个不存在的键时，它不会报错，而是会使用默认值初始化这个键。默认值是根据在创建 defaultdict 对象时作为参数输入的数据类型自动设置的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br></pre></td></tr></table></figure><p>如上所示，在访问不存在的键”Sara”时，程序并不会像一般字典一样报错，而是为其设置一个int型的默认值。<br>同理，当我们设置默认值类型为list时，看看会发生什么。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">names_dict = defaultdict(int)</span><br><span class="line">names_dict[<span class="string">"Bob"</span>] = <span class="number">1</span></span><br><span class="line">names_dict[<span class="string">"Katie"</span>] = <span class="number">2</span></span><br><span class="line">sara_number = names_dict[<span class="string">"Sara"</span>]</span><br><span class="line">print(names_dict)</span><br></pre></td></tr></table></figure><pre><code>defaultdict(&lt;class &#39;int&#39;&gt;, {&#39;Bob&#39;: 1, &#39;Katie&#39;: 2, &#39;Sara&#39;: 0})</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">names_dict1 = defaultdict(list)</span><br><span class="line">names_dict1[<span class="string">"Bob"</span>] = <span class="number">1</span></span><br><span class="line">names_dict1[<span class="string">"Katie"</span>] = <span class="number">2</span></span><br><span class="line">sara_number = names_dict1[<span class="string">"Sara"</span>]</span><br><span class="line">print(names_dict1)</span><br></pre></td></tr></table></figure><pre><code>defaultdict(&lt;class &#39;list&#39;&gt;, {&#39;Bob&#39;: 1, &#39;Katie&#39;: 2, &#39;Sara&#39;: []})</code></pre><p>可以看到，默认值变成了一个空列表</p><h2 id="OrderedDict"><a href="#OrderedDict" class="headerlink" title="OrderedDict"></a>OrderedDict</h2><p>返回一个字典的子类，具有可以将字典顺序重新排列的方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> OrderedDict</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d = OrderedDict.fromkeys(<span class="string">'abcde'</span>)</span><br><span class="line">d.move_to_end(<span class="string">'b'</span>)</span><br><span class="line"><span class="string">''</span>.join(d.keys())</span><br></pre></td></tr></table></figure><pre><code>&#39;acdeb&#39;</code></pre><p>last 参数默认为True，即将元素挪到最右边，False则将元素挪到最左边</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d.move_to_end(<span class="string">'b'</span>, last=<span class="keyword">False</span>) </span><br><span class="line"><span class="string">''</span>.join(d.keys())</span><br><span class="line"><span class="string">'bacde'</span></span><br></pre></td></tr></table></figure><pre><code>&#39;bacde&#39;</code></pre><p>popitem()方法将最后的元素弹出，参数last默认为True，即弹出最右边的元素，False则为最左边。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">"poped item: &#123;&#125;"</span>.format(d.popitem()))</span><br><span class="line">print(d)</span><br></pre></td></tr></table></figure><pre><code>poped item: (&#39;e&#39;, None)OrderedDict([(&#39;b&#39;, None), (&#39;a&#39;, None), (&#39;c&#39;, None), (&#39;d&#39;, None)])</code></pre><h2 id="deque"><a href="#deque" class="headerlink" title="deque"></a>deque</h2><p>   collections.deque是python中使用队列的一种非常好的方法，这个方法的一个关键特性是保持队列长度一直不变，也就是说，如果你将 queue 的最大大小设置为 10，那么 deque 将根据 FIFO 原则添加和删除元素，以保持 queue 的最大大小为 10。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">my_queue = deque(maxlen=<span class="number">10</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    my_queue.append(i+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">print(my_queue)</span><br></pre></td></tr></table></figure><pre><code>deque([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], maxlen=10)</code></pre><p>   接下来，我们尝试继续往里面添加元素</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">11</span>,<span class="number">16</span>):</span><br><span class="line">    my_queue.append(i)</span><br><span class="line"></span><br><span class="line">print(my_queue)</span><br></pre></td></tr></table></figure><pre><code>deque([6, 7, 8, 9, 10, 11, 12, 13, 14, 15], maxlen=10)</code></pre><p>可以看到，最前面的5个元素被弹出，而新元素被追加到队列的后面</p><h2 id="nametuple"><a href="#nametuple" class="headerlink" title="nametuple"></a>nametuple</h2><p>nametuple是tuple的子类，意味着它可以像tuple一样通过索引下标访问，同时，它还具备了通过字段访问元素的功能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> namedtuple</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Point = namedtuple(<span class="string">'Point'</span>, [<span class="string">'x'</span>, <span class="string">'y'</span>])</span><br><span class="line">p = Point(<span class="number">11</span>, y=<span class="number">22</span>) </span><br><span class="line"><span class="comment"># 像普通元组一样访问元素</span></span><br><span class="line">print(p[<span class="number">0</span>] + p[<span class="number">1</span>]) </span><br><span class="line"><span class="comment"># 通过字段访问元素</span></span><br><span class="line">print(p.x + p.y)</span><br></pre></td></tr></table></figure><pre><code>3333</code></pre><p>为了防止与字段名冲突，nametuple的类方法和属性前都加了下划线_。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># _make方法将列表转化为nametuple</span></span><br><span class="line">t = [<span class="number">11</span>, <span class="number">22</span>]</span><br><span class="line">Point._make(t)</span><br></pre></td></tr></table></figure><pre><code>Point(x=11, y=22)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># _asdict()将nametuple转为字典</span></span><br><span class="line">p = Point(x=<span class="number">11</span>, y=<span class="number">22</span>)</span><br><span class="line">p._asdict()</span><br></pre></td></tr></table></figure><pre><code>OrderedDict([(&#39;x&#39;, 11), (&#39;y&#39;, 22)])</code></pre><p>需要注意，由于nametuple本质上还是一个tuple，因此直接更改其中的元素是会报错的。正确的做法是使用_replace函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># p.x = 12 报错</span></span><br><span class="line">p._replace(x=<span class="number">12</span>)</span><br></pre></td></tr></table></figure><pre><code>Point(x=12, y=22)</code></pre><p>可以通过_fields属性访问nametuple的字段。一个很常见的用法是从现有的nametuple创建一个新的nametuple。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p._fields</span><br></pre></td></tr></table></figure><pre><code>(&#39;x&#39;, &#39;y&#39;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Color = namedtuple(<span class="string">'Color'</span>, <span class="string">'red green blue'</span>)</span><br><span class="line">Pixel = namedtuple(<span class="string">'Pixel'</span>, Point._fields + Color._fields)</span><br><span class="line">Pixel(<span class="number">11</span>, <span class="number">22</span>, <span class="number">128</span>, <span class="number">255</span>, <span class="number">0</span>)</span><br></pre></td></tr></table></figure><pre><code>Pixel(x=11, y=22, red=128, green=255, blue=0)</code></pre><p> 如果希望将字典转换为nametuple，可以用如下方式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">d = &#123;<span class="string">'x'</span>: <span class="number">11</span>, <span class="string">'y'</span>: <span class="number">22</span>&#125;</span><br><span class="line">Point(**d)</span><br></pre></td></tr></table></figure><pre><code>Point(x=11, y=22)</code></pre><h2 id="ChainMap"><a href="#ChainMap" class="headerlink" title="ChainMap"></a>ChainMap</h2><p>可以快速链接许多映射，因此可以将它们视为一个单元，通常比创建新字典并运行多个update()要快得多。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> ChainMap</span><br></pre></td></tr></table></figure><p>注意，ChainMap的迭代顺序是通过扫描从前往后的映射决定的（与官方文档说的从后往前相反，研究了甚久，有可能是版本问题）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">baseline = &#123;<span class="string">'music'</span>: <span class="string">'bach'</span>, <span class="string">'art'</span>: <span class="string">'rembrandt'</span>&#125;</span><br><span class="line">adjustments = &#123;<span class="string">'art'</span>: <span class="string">'van gogh'</span>, <span class="string">'opera'</span>: <span class="string">'carmen'</span>&#125;</span><br><span class="line">print(list(ChainMap(adjustments, baseline)))</span><br><span class="line">print(dict(ChainMap(adjustments, baseline)))</span><br></pre></td></tr></table></figure><pre><code>[&#39;art&#39;, &#39;opera&#39;, &#39;music&#39;]{&#39;art&#39;: &#39;van gogh&#39;, &#39;opera&#39;: &#39;carmen&#39;, &#39;music&#39;: &#39;bach&#39;}</code></pre><p>这与从最后一个映射开始的一系列dict.update()调用的顺序相反:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">combined = baseline.copy()</span><br><span class="line">combined.update(adjustments)</span><br><span class="line">print(list(combined))</span><br><span class="line">print(dict(combined))</span><br></pre></td></tr></table></figure><pre><code>[&#39;music&#39;, &#39;art&#39;, &#39;opera&#39;]{&#39;music&#39;: &#39;bach&#39;, &#39;art&#39;: &#39;van gogh&#39;, &#39;opera&#39;: &#39;carmen&#39;}</code></pre><p>ChainMap最常用的例子是在程序传入参数的优先级设置上，例如：参数可以通过命令行传入，可以通过环境变量传入，还可以有默认参数，我们可以设置优先级：命令行&gt;环境变量&gt;默认值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># jupyter notebook中无法使用argparse</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> ChainMap</span><br><span class="line"><span class="keyword">import</span> os, argparse</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造缺省参数:</span></span><br><span class="line">defaults = &#123;</span><br><span class="line">    <span class="string">'color'</span>: <span class="string">'red'</span>,</span><br><span class="line">    <span class="string">'user'</span>: <span class="string">'guest'</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造命令行参数:</span></span><br><span class="line">parser = argparse.ArgumentParser()</span><br><span class="line">parser.add_argument(<span class="string">'-u'</span>, <span class="string">'--user'</span>)</span><br><span class="line">parser.add_argument(<span class="string">'-c'</span>, <span class="string">'--color'</span>)</span><br><span class="line">namespace = parser.parse_args()</span><br><span class="line">command_line_args = &#123; k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> vars(namespace).items() <span class="keyword">if</span> v &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 组合成ChainMap:</span></span><br><span class="line">combined = ChainMap(command_line_args, os.environ, defaults)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印参数:</span></span><br><span class="line">print(<span class="string">'color=%s'</span> % combined[<span class="string">'color'</span>])</span><br><span class="line">print(<span class="string">'user=%s'</span> % combined[<span class="string">'user'</span>])</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;collections-库简单上手&quot;&gt;&lt;a href=&quot;#collections-库简单上手&quot; class=&quot;headerlink&quot; title=&quot;collections 库简单上手&quot;&gt;&lt;/a&gt;collections 库简单上手&lt;/h1&gt;&lt;h2 id=&quot;Counter&quot;&gt;&lt;a href=&quot;#Counter&quot; class=&quot;headerlink&quot; title=&quot;Counter&quot;&gt;&lt;/a&gt;Counter&lt;/h2&gt;&lt;p&gt;   Counter 是 dictionary 对象的子类。collections 模块中的 Counter() 函数会接收一个诸如 list 或 tuple 的迭代器，然后返回一个 Counter dictionary。这个 dictionary 的键是该迭代器中的唯一元素，每个键的值是迭代器元素的计数。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Python" scheme="http://yoursite.com/categories/Python/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
      <category term="collections" scheme="http://yoursite.com/tags/collections/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2021/01/06/test/"/>
    <id>http://yoursite.com/2021/01/06/test/</id>
    <published>2021-01-06T15:09:16.961Z</published>
    <updated>2021-01-06T15:09:16.962Z</updated>
    
    <content type="html"><![CDATA[<p>this a test file</p><h2 id="Test"><a href="#Test" class="headerlink" title="Test"></a>Test</h2><h3 id="Test-1"><a href="#Test-1" class="headerlink" title="Test"></a>Test</h3><p>hello world !!!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;this a test file&lt;/p&gt;
&lt;h2 id=&quot;Test&quot;&gt;&lt;a href=&quot;#Test&quot; class=&quot;headerlink&quot; title=&quot;Test&quot;&gt;&lt;/a&gt;Test&lt;/h2&gt;&lt;h3 id=&quot;Test-1&quot;&gt;&lt;a href=&quot;#Test-1&quot; class=&quot;
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>概率论与数理统计(三)</title>
    <link href="http://yoursite.com/2021/01/06/probability/probability-3/"/>
    <id>http://yoursite.com/2021/01/06/probability/probability-3/</id>
    <published>2021-01-06T15:09:16.961Z</published>
    <updated>2021-01-06T15:09:16.961Z</updated>
    
    <content type="html"><![CDATA[<p>  本章的主要内容是随机变量的<strong>数字特征</strong>，其中包括<strong>数学期望、方差、协方差</strong>和<strong>相关系数</strong>。</p><h2 id="随机变量的数字特征"><a href="#随机变量的数字特征" class="headerlink" title="随机变量的数字特征"></a>随机变量的数字特征</h2><h3 id="数学期望"><a href="#数学期望" class="headerlink" title="数学期望"></a>数学期望</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><h5 id="离散型"><a href="#离散型" class="headerlink" title="离散型"></a>离散型</h5><p>  设离散型随机变量$X$的概率函数为</p><script type="math/tex; mode=display">P(X =a_i) = p_i \ , \quad i=1, 2, ...</script><p>当级数$\sum_ia_ip_i$ 绝对收敛时，称$\sum_ia_ip_i $为随机变量$X$的数学期望(或期望、均值)，记作$E(X).$</p><a id="more"></a><h5 id="连续型"><a href="#连续型" class="headerlink" title="连续型"></a>连续型</h5><p>  设连续型随机变量$X$的概率密度函数为$f(x)$，当积分$\int<em>{-\infty}^{+\infty}xf(x)dx$绝对收敛时， 称$\int ^{+\infty}</em>{-\infty}xf(x)dx$ 为随机变量X的数学期望，记作E(X)，即</p><script type="math/tex; mode=display">E(X) =\int_{-\infty}^{+\infty}xf(x)dx</script><hr><h4 id="常用离散型随机变量的数学期望"><a href="#常用离散型随机变量的数学期望" class="headerlink" title="常用离散型随机变量的数学期望"></a>常用离散型随机变量的数学期望</h4><h5 id="离散型-1"><a href="#离散型-1" class="headerlink" title="离散型"></a>离散型</h5><ul><li>$0-1 分布 B(1,P) : E(X) = p $</li><li>$二项分布 B(n,p) : E(X) = np$</li><li>$泊松分布P(\lambda) : E(X) = \lambda $</li></ul><h5 id="连续型-1"><a href="#连续型-1" class="headerlink" title="连续型"></a>连续型</h5><ul><li>$均匀分布 R(a,b) : E(X) = \frac{a+b}{2} $</li><li>$指数分布 E(\lambda): E(X) = \frac{1}{\lambda}$</li><li>$正态分布N(\mu, \sigma^2): E(X) = \mu$</li></ul><hr><h4 id="随机变量函数的期望"><a href="#随机变量函数的期望" class="headerlink" title="随机变量函数的期望"></a>随机变量函数的期望</h4><h5 id="定理4-1"><a href="#定理4-1" class="headerlink" title="定理4.1"></a>定理4.1</h5><ul><li>设$X$为离散型随机变量，概率函数为</li></ul><script type="math/tex; mode=display">P(X =a_i) = p_i \ , \quad i=1,2,...</script><p>$Y = g(X)$ 是随机变量X的函数，当$\sum_i|g(a_i)|p_i $收敛时</p><script type="math/tex; mode=display">E(Y) = \sum_i g(a_i) p_i</script><ul><li>若$X$为连续型随机变量，$f(x)$为其相应的密度函数，当$\int^{+\infty}_{-\infty}|g(x)|f(x) dx $收敛时，</li></ul><script type="math/tex; mode=display">E(Y) =\int^{+\infty}_{-\infty}g(x)f(x) dx</script><ul><li>设$(X,Y) $是二维离散型随机变量，其概率函数为</li></ul><script type="math/tex; mode=display">P(X = a, Y = b_j) = p_{ij} \qquad i,j = 1,2,...</script><p>$Z = g(X,Y) $是 X,Y 的函数，当$\sum<em>i \sum_j g(a_i,b_j) p</em>{ij}$ 绝对收敛时</p><script type="math/tex; mode=display">E(Z) = \sum_i \sum_j g(a_i,b_j) p_{ij}</script><ul><li><p>若$(X,Y)$是二维连续型随机变量，$f(x,y)$为联合密度函数，$Z = g(X,Y) $是 $(X,Y)$ 的函数，</p><p>当$\int_{-\infty}^{+\infty}g(x,y)f(x,y)dxdy$ 绝对收敛时</p></li></ul><script type="math/tex; mode=display">E(Z) =\int_{-\infty}^{+\infty}g(x,y)f(x,y)dxdy</script><ul><li>$(X,Y)$为离散型随机变量，当$g(X,Y) = X$ 或$(Y)$时</li></ul><script type="math/tex; mode=display">E(X) = \sum_i\sum_ja_ip_{ij} = \sum_i a_i p_{i·}或</script><p>  或</p><script type="math/tex; mode=display">E(Y) = \sum_i\sum_jb_jp_{ij} = \sum_j b_j p_{·j}</script><ul><li>$(X,Y)$为连续型随机变量，当$g(X,Y) = X$(或$Y$) 时</li></ul><script type="math/tex; mode=display">\ \ \ E(X) = \int^\infty_{-\infty} \int^\infty_{-\infty} xf(x,y)dxdy \\ =\int^\infty_{-\infty} xf_X(x)dx</script><p>或</p><script type="math/tex; mode=display">\quad E(Y) = \int^\infty_{-\infty} \int^\infty_{-\infty} yf(x,y)dxdy \\ =\int^\infty_{-\infty} yf_Y(y)dy</script><h4 id="数学期望的性质"><a href="#数学期望的性质" class="headerlink" title="数学期望的性质"></a>数学期望的性质</h4><h5 id="退化分布"><a href="#退化分布" class="headerlink" title="退化分布"></a>退化分布</h5><p>  把常数$c$看做是概率函数为$P(X = c) = 1$ 的随机变量$X$，并称$X$服从参数为$c$的退化分布.</p><h5 id="定理4-2"><a href="#定理4-2" class="headerlink" title="定理4.2"></a>定理4.2</h5><p>  $设k,l,c都是常数，则有$</p><ol><li>$E(c) = c $</li><li>$E(kX+c) = kE(X) + c$</li><li>$E(kX + lY) = kE(X) + lE(Y)$</li><li>$当X与Y相互独立时，有E(XY) = E(X)E(Y)$</li></ol><h3 id="方差和协方差"><a href="#方差和协方差" class="headerlink" title="方差和协方差"></a>方差和协方差</h3><h4 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h4><p>  $设X是一个随机变量，称$</p><script type="math/tex; mode=display">D(X) \doteq E\{[X-E(X)]^2\}</script><p>$为X的方差，而称\sqrt{D(X)}为X的标准差.$</p><h4 id="计算公式"><a href="#计算公式" class="headerlink" title="计算公式"></a>计算公式</h4><script type="math/tex; mode=display">D(X) = E(X^2) - E^2(X) \ .</script><h4 id="常见离散型分布的方差"><a href="#常见离散型分布的方差" class="headerlink" title="常见离散型分布的方差"></a>常见离散型分布的方差</h4><h5 id="0-1分布"><a href="#0-1分布" class="headerlink" title="0-1分布"></a>0-1分布</h5><script type="math/tex; mode=display">D(X) = p(1-p)</script><h5 id="二项分布"><a href="#二项分布" class="headerlink" title="二项分布"></a>二项分布</h5><script type="math/tex; mode=display">D(X) = np(1-p)</script><h5 id="泊松分布"><a href="#泊松分布" class="headerlink" title="泊松分布"></a>泊松分布</h5><script type="math/tex; mode=display">D(X) = \lambda</script><h4 id="常见连续型分布的方差"><a href="#常见连续型分布的方差" class="headerlink" title="常见连续型分布的方差"></a>常见连续型分布的方差</h4><h5 id="均匀分布"><a href="#均匀分布" class="headerlink" title="均匀分布"></a>均匀分布</h5><script type="math/tex; mode=display">D(X) = \frac{(b-a)^2}{12}</script><h5 id="指数分布"><a href="#指数分布" class="headerlink" title="指数分布"></a>指数分布</h5><script type="math/tex; mode=display">D(X) = \frac{1}{\lambda^2}</script><h5 id="正态分布"><a href="#正态分布" class="headerlink" title="正态分布"></a>正态分布</h5><script type="math/tex; mode=display">D(X) = \sigma^2</script><h4 id="方差的性质"><a href="#方差的性质" class="headerlink" title="方差的性质"></a>方差的性质</h4><h5 id="定理4-3"><a href="#定理4-3" class="headerlink" title="定理4.3"></a>定理4.3</h5><p>  $设k,l及c都是常数，则$</p><ol><li>$D(c) = 0$</li><li>$D(kX+c) = k^2D(x)$</li><li>$D(X\pm Y) = D(X) + D(Y) \pm 2E{[X-E(X)][Y-E(Y)]} $</li><li>$当X与Y相互独立时，D(X \pm Y ) = D(X) + D(Y)$</li></ol><h4 id="协方差"><a href="#协方差" class="headerlink" title="协方差"></a>协方差</h4><h5 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h5><p>  $设(X,Y)是二维随机变量，称$</p><script type="math/tex; mode=display">E\{[X-E(x)][Y-E(Y)]\}</script><p>$为X与Y的协方差，记为cov(X,Y) .$</p><p>  协方差反映的是$X$和$Y$之间<strong>协同</strong>发展的趋势</p><h5 id="计算公式-1"><a href="#计算公式-1" class="headerlink" title="计算公式"></a>计算公式</h5><ol><li><script type="math/tex; mode=display">cov(X,Y) = E(XY) - E(X)E(Y)</script></li><li></li></ol><script type="math/tex; mode=display">cov(X,X) = D(X)</script><ol><li><script type="math/tex; mode=display">D(X \pm Y) =D(X) +D(Y) \pm 2cov(X,Y)    \\D(aX \pm bY) = a^2D(X) + b^2D(Y) \pm 2abcov(X,Y)</script></li><li><p>$当X与Y相互独立时，有$</p></li></ol><script type="math/tex; mode=display">cov(X,Y) = 0</script><h5 id="性质-定理4-4"><a href="#性质-定理4-4" class="headerlink" title="性质(定理4.4)"></a>性质(定理4.4)</h5><p>  $设k,l,c都是常数，则(X,Y)的协方差满足$</p><ol><li>$cov(X,Y) = cov(Y,X)$</li><li>$cov(X,c) = 0$</li><li>$cov(kX,lY) = klcov(X,Y)$</li><li>$cov(\sum^m<em>{i=1}X_i, \sum^n</em>{j=1}Y<em>j) = \sum^m</em>{i=1}\sum^n_{j=1}cov(X_i,Yj)$</li></ol><h3 id="相关系数"><a href="#相关系数" class="headerlink" title="相关系数"></a>相关系数</h3><h4 id="定义-3"><a href="#定义-3" class="headerlink" title="定义"></a>定义</h4><p>  设$(X,Y)$是随机变量，当$D(X)&gt;0,D(Y)&gt;0$时，称</p><script type="math/tex; mode=display">\rho(X,Y) = E[\frac{X-E(X)}{\sqrt{D(X)}}·\frac{Y-E(Y)}{\sqrt{D(Y)}}]</script><p>为$X$和$Y$的<strong>相关系数</strong>，标准化方差.</p><p>  <strong>注：</strong></p><ol><li><script type="math/tex; mode=display">\rho(X,Y) = \frac{cov(X,Y)}{\sqrt{D(X)}\sqrt{D(Y)}}</script></li><li></li></ol><script type="math/tex; mode=display">D(X \pm Y) = D(X) + D(Y) \pm 2\rho(X,Y)\sqrt{D(X)D(Y)}</script><ol><li>二维正态随机变量$(X,Y) \sim N(\mu_1,\mu_2,\sigma^2_1,\sigma_2^2,\rho)$中$X$与$Y$的相关系数</li></ol><script type="math/tex; mode=display">\rho(X,Y) = \rho</script><h4 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h4><h5 id="定理4-5"><a href="#定理4-5" class="headerlink" title="定理4.5"></a>定理4.5</h5><p>  当$D(X) &gt;0, D(Y) &gt;0$ 时</p><ol><li>$\rho(X,Y) = \rho(Y,X)$</li><li>$|\rho(X,Y)|\le 1$</li><li>$|\rho(X,Y)|= 1 $的<strong>充分必要条件</strong>是：存在不为零的常数$k$与常数$c$，使得$P(Y=kX+c) = 1.$</li></ol><h4 id="线性关系"><a href="#线性关系" class="headerlink" title="线性关系"></a>线性关系</h4><p>  当$rho(X,Y) = + 1 $时，称$X$与$Y$<strong>正线性相关</strong></p><p>  当$rho(X,Y) = - 1$ 时，称$X$与$Y$<strong>负线性相关</strong>    </p><p>$X$与$Y$之间的线性联系程度随着$|\rho(X,Y)|$ 的减小而减弱，特别地有下面定义</p><p>  当$\rho(X,Y) =0$ 时，称X与Y<strong>(线性)不相关</strong>.</p><h4 id="不相关与相互独立的关系"><a href="#不相关与相互独立的关系" class="headerlink" title="不相关与相互独立的关系"></a>不相关与相互独立的关系</h4><h5 id="定理4-6"><a href="#定理4-6" class="headerlink" title="定理4.6"></a>定理4.6</h5><p>  <strong>如果$X$与$Y$相互独立，那么$X$与$Y$一定不相关，反之不然；如果$X$与$Y$相关，则它们一定不独立，反正不然。</strong></p><p><strong>注：独立意味着随机变量之间没有任何关系，不相关仅意味着无线性关系，不能排除具有非线性关系。</strong></p><h5 id="定理4-7"><a href="#定理4-7" class="headerlink" title="定理4.7"></a>定理4.7</h5><p>  $如果(X,Y)服从二维正态分布，则X与Y相互独立等价于X与Y不相关.$</p><p>  $如果(X,Y)服从二维正态分布，则X与Y相互独立等价于\rho =0.$</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;  本章的主要内容是随机变量的&lt;strong&gt;数字特征&lt;/strong&gt;，其中包括&lt;strong&gt;数学期望、方差、协方差&lt;/strong&gt;和&lt;strong&gt;相关系数&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&quot;随机变量的数字特征&quot;&gt;&lt;a href=&quot;#随机变量的数字特征&quot; class=&quot;headerlink&quot; title=&quot;随机变量的数字特征&quot;&gt;&lt;/a&gt;随机变量的数字特征&lt;/h2&gt;&lt;h3 id=&quot;数学期望&quot;&gt;&lt;a href=&quot;#数学期望&quot; class=&quot;headerlink&quot; title=&quot;数学期望&quot;&gt;&lt;/a&gt;数学期望&lt;/h3&gt;&lt;h4 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h4&gt;&lt;h5 id=&quot;离散型&quot;&gt;&lt;a href=&quot;#离散型&quot; class=&quot;headerlink&quot; title=&quot;离散型&quot;&gt;&lt;/a&gt;离散型&lt;/h5&gt;&lt;p&gt;  设离散型随机变量$X$的概率函数为&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
P(X =a_i) = p_i \ , \quad i=1, 2, ...&lt;/script&gt;&lt;p&gt;当级数$\sum_ia_ip_i$ 绝对收敛时，称$\sum_ia_ip_i $为随机变量$X$的数学期望(或期望、均值)，记作$E(X).$&lt;/p&gt;
    
    </summary>
    
    
      <category term="概率论与数理统计" scheme="http://yoursite.com/categories/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"/>
    
    
      <category term="数学" scheme="http://yoursite.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
      <category term="概率论" scheme="http://yoursite.com/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>概率论与数理统计(二)</title>
    <link href="http://yoursite.com/2021/01/06/probability/probability-2/"/>
    <id>http://yoursite.com/2021/01/06/probability/probability-2/</id>
    <published>2021-01-06T15:09:16.960Z</published>
    <updated>2021-01-06T15:09:16.960Z</updated>
    
    <content type="html"><![CDATA[<p>  本章主要内容是<strong>连续型随机变量及其分布</strong>。</p><h2 id="连续型随机变量及其分布"><a href="#连续型随机变量及其分布" class="headerlink" title="连续型随机变量及其分布"></a>连续型随机变量及其分布</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>  给定一个随机变量$X$，称定义域为$(-\infty, +\infty)$ 的实值函数 </p><script type="math/tex; mode=display">F(x) = P (X \le x)，-\infty < x < + \infty</script><p>为随机变量$X$的<strong>分布函数</strong>.</p><p>对任意实数 $x \in R , {X \le x}$ 是一个随机事件，它的概率$P(X \le x)$ 就定义为点$x$处分布函数的函数值.</p><ul><li>注：对任意满足条件 $-\infty &lt; a +\infty$ 的实数 $a,b,$ 有<script type="math/tex; mode=display">P(a < X \le b) = F(b) - F(a).</script></li></ul><hr><a id="more"></a><p>  <strong>定理3.1 (分布函数的性质)</strong>    设$F(x) 是随机变量$$X$的分布函数，则有：</p><ol><li>$0 \le F(x) \le 1 ;$</li><li>分布函数单调不减；</li><li>对任意$x \in (- \infty, +\infty)，$分布函数右连续；</li><li>$\lim<em>{x \rightarrow -\infty} F(x) = 0, \  \lim</em>{x \rightarrow + \infty} F(X)= 1.$</li></ol><hr><p>  <strong>定理3.2 对于任意的随机变量$X$，其分布函数$F(X)$ 在 $x=x_o$点处连续的充分必要条件是$P(X = x_o) = 0.$</strong></p><hr><h3 id="概率密度函数"><a href="#概率密度函数" class="headerlink" title="概率密度函数"></a>概率密度函数</h3><p>  给定一个连续型的随机变量$X$，如果存在一个定义域为$(-\infty, +\infty)$的非负实值函数$f(x)$，使得$X$的分布函数$F(x)$可以表示为</p><script type="math/tex; mode=display">F(x ) = \int^x_{-\infty} f(t) dt \qquad -\infty < x < +\infty</script><p>那么就称$f(x)$为连续型随机变量$X$的<strong>概率密度函数.</strong></p><hr><p>  概率密度函数满足下面两个条件：</p><p>(1) $f(x) \ge 0, \quad -\infty&lt;x&lt;+\infty$</p><p>(2)  $\int^{+\infty}_{-\infty} f(x)dx = F(+\infty) = 1.$</p><p>  对照一下，设离散型随机变量的概率函数为</p><script type="math/tex; mode=display">P(X = a_i) = p_i , \quad i =1, 2, ...</script><p>则有(1) $p_i \ge 0$     (2) $\sum_i p_i =1$</p><hr><p><strong>连续型随机变量的性质</strong>   设$X$ 是任意连续型的随机变量，且$F(X)$与$f(x)$分别是它的分布函数与概率密度函数，则有：</p><p>  （1）$F(x) $是连续函数，且在$f(x)$的连续点处，有</p><script type="math/tex; mode=display">F'(x) = f(x) ;</script><p>  （2）对任意常数 $c(-\infty &lt;c &lt; +\infty),$ 有 $P(X = c) = 0;$</p><p>  （3） 对任意的两个常数$a,b,-\infty&lt;a&lt;b&lt;+\infty，$有</p><script type="math/tex; mode=display">P(a < X \le b) = F(b) - F(a)  = \int^b_{-\infty} f(x) dx - \int^a_{-\infty}f(x)dx  \\=\int^b_af(x)dx</script><p>注意到</p><script type="math/tex; mode=display">P(x < X \le x+ \Delta x) = F(x + \Delta x) - F(x)\\\approx F'(x) \Delta x \\\approx f(x)\Delta x.</script><p>进一步，对实数轴上任意一个集合$S,$</p><script type="math/tex; mode=display">P(X > a) = \int^{+\infty}_a f(x) dx = 1- F(a)   \\P(X \le b) = \int^b_{-\infty} f(x)dx = F(b).</script><hr><h3 id="常见一维连续型分布"><a href="#常见一维连续型分布" class="headerlink" title="常见一维连续型分布"></a>常见一维连续型分布</h3><h4 id="均匀分布"><a href="#均匀分布" class="headerlink" title="均匀分布"></a>均匀分布</h4><p>  设随机变量$X$的概率密度函数为</p><script type="math/tex; mode=display">\begin{equation}f(x) =\left \{ \begin{aligned}c   \qquad a<x<b  \\0    \qquad 其余\end{aligned}\right .\end{equation}</script><p>则称$X$服从区间$(a, b)$上的<strong>均匀分布</strong>，记作$X \sim R(a,b).$</p><p>  由概率密度函数性质可知$c = \frac{1}{b-a}$，并计算可得分布函数为</p><script type="math/tex; mode=display">\begin{equation}F(x) = \left \{\begin{aligned}0 \qquad x<a    \\\frac{x-a}{b-a}  \qquad   a \le x \le b    \\1   \qquad    x \ge b \end{aligned}\right .\end{equation}</script><hr><h4 id="指数分布"><a href="#指数分布" class="headerlink" title="指数分布"></a>指数分布</h4><p>  如果随机变量$X$的概率密度函数为</p><script type="math/tex; mode=display">f(x) =\left \{\begin{align}\lambda e^{-\lambda x}    \qquad    x>0    \\0    \qquad    x\le 0\end{align}\right .</script><p>则称$X$服从参数为$\lambda$的<strong>指数分布</strong>，记为$X \sim E(\lambda), (\lambda &gt;0).$</p><p>  其分布函数为</p><script type="math/tex; mode=display">F(X) =\left \{\begin{align}1 - e^{-\lambda x}    \qquad    x\ge 0    \\0    \qquad    x<0\end{align}\right .</script><hr><h4 id="正态分布"><a href="#正态分布" class="headerlink" title="正态分布"></a>正态分布</h4><h4 id="正态随机变量和概率密度函数"><a href="#正态随机变量和概率密度函数" class="headerlink" title="正态随机变量和概率密度函数"></a>正态随机变量和概率密度函数</h4><p>  设随机变量$X$的概率密度函数为</p><script type="math/tex; mode=display">f(x) = \frac{1}{\sqrt{2 \pi} \delta} e^{-\frac{(x-u)^2}{2 \delta^2}},    \qquad    -\infty < x < +\infty</script><p>其中参数$\mu \in (-\infty,+\infty)$，而$\delta &gt; 0.$</p><p>则称$X$服从参数为$\mu, \delta^2$的<strong>正态分布</strong>，记为$X \sim N(\mu, \delta^2).$</p><p>服从正态分布的随机变量称为<strong>正态随机变量</strong>.</p><hr><p>  正态分布概率密度函数的曲线特征：</p><p>(1) 密度函数$f(x)$的图形关于$x = \mu$对称；</p><p>(2) $f(x)$在$x = \mu$处取得最大值$f(\mu) = \frac{1}{\sqrt{2\pi} \delta};$</p><p>(3) 当$|x| \rightarrow \infty$时，$f(x) \rightarrow 0 ;$</p><p>(4) 当$\delta^2$较大时曲线比较平坦，当$\delta^2$较小时曲线比较陡峭.</p><hr><h4 id="标准正态分布"><a href="#标准正态分布" class="headerlink" title="标准正态分布"></a>标准正态分布</h4><p>   $\mu = 0, \delta^2 = 1$时的正态分布称为<strong>标准正态分布</strong>，其概率密度函数和分布函数分别为</p><script type="math/tex; mode=display">\varphi(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}    \qquad    -\infty < x < +\infty    \\\phi(x)    = \int^x_{-\infty} \phi(t)dt = P(X\le x)    \qquad    -\infty < x < +\infty</script><p>  关于标准正态分布有以下结果：</p><ul><li>当$x&gt;0$时，$\phi(x)$的值可以查概率函数数值表得到，且</li></ul><script type="math/tex; mode=display">P(a < x \le b) = \phi(b) - \phi(a)</script><ul><li>当$x &lt; 0$时，由密度函数对称性可得$\phi(x) = 1 - \phi(-x)$，</li></ul><p>特别地，有$\phi(0) = \frac{1}{2}.$</p><ul><li><p>若$X \sim N(\mu, \delta^2)$，则$P(a&lt;x \le b) = \phi(\frac{b-\mu}{\delta}) - \phi(\frac{a-\mu}{\delta})$</p><p>特别地 $P(X \le b) = P(-\infty &lt; X \le b) = \phi(\frac{b-\mu}{\delta})$</p><p>​         $P(X &gt;a) = P(a &lt; x &lt;+\infty) = 1 - \phi( \frac{a-\mu}{\delta})$</p></li></ul><ul><li><p>标准正态分布的分位数概念：</p><p>给定常数$0&lt;p&lt;1$，由概率函数数值表可查得数值$\mu_p$，使</p><script type="math/tex; mode=display">\phi(u_p) = \int_{-\infty}^{u_p} \varphi(x)dx = P(X \le u_p) = p</script><p>称$u_p$为随机变量$X$的<strong>p-分位数</strong>.</p></li></ul><hr><h3 id="二维连续型随机变量"><a href="#二维连续型随机变量" class="headerlink" title="二维连续型随机变量"></a>二维连续型随机变量</h3><h4 id="联合分布函数"><a href="#联合分布函数" class="headerlink" title="联合分布函数"></a>联合分布函数</h4><h5 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h5><p>  给定随机向量$(X, Y)$，称定义域为整个平面的二元实值函数</p><script type="math/tex; mode=display">F(x ,y) \doteq P(X \le x, Y \le y) , \qquad    -\infty <x , y< + \infty</script><p>为随机变量$(X, Y)$的<strong>分布函数</strong>，或称为$X$与$Y$的<strong>联合分布函数.</strong></p><hr><h5 id="性质"><a href="#性质" class="headerlink" title="性质"></a>性质</h5><ol><li><p>$0 \le F(x,y) \le 1$；</p></li><li><p>$F(x,y)$关于$x$或$y$单调不减；</p></li><li><p>$F(x,y)$关于$x$或$y$是右连续的；</p></li><li><script type="math/tex; mode=display">\lim_{x \rightarrow -\infty} F(x,y) = 0,  \qquad   \lim_{y\rightarrow - \infty}F(x,y) = 0;    \\\lim_{x,y \rightarrow -\infty} F(x,y) = 0,  \qquad   \lim_{x,y\rightarrow - \infty}F(x,y) = 1.</script></li><li><p>对任意实数$x_1 &lt; x_2, y_1 &lt;y_2,$</p><script type="math/tex; mode=display">P(x_1 < X \le x_2, y1< Y \le y_2)    \\=F(x_2,y_2) - F(x_1,y_2) - F(x_2,y_1) + F(x_1,y_1).</script></li></ol><hr><h4 id="二维连续型随机变量-1"><a href="#二维连续型随机变量-1" class="headerlink" title="二维连续型随机变量"></a>二维连续型随机变量</h4><p>  如果一个二维随机变量的值域是平面上的一个区域，那么称它为<strong>二维连续型随机变量(或向量)</strong>，类似地有$n$维连续型随机变量(向量).</p><hr><h4 id="联合概率密度函数"><a href="#联合概率密度函数" class="headerlink" title="联合概率密度函数"></a>联合概率密度函数</h4><h5 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h5><p>  给定一个二维连续型随机变量$(X, Y)$，如果存在一个定义域为整个平面的二元非负实值函数 $f(x, y)$ ，使得 $(X, Y)$ 的联合分布函数 $F(x,y)$ 可以表示为</p><script type="math/tex; mode=display">F(x, y) = \int^x_{-\infty} \int^y_{-\infty} f(u,v) dudv \ , \quad -\infty < x,y < + \infty</script><p>那么称$f(x, y)$是随机向量$(X, Y )$的<strong>联合概率密度函数</strong>.</p><p>$f(x, y)$必须满足下列两个条件：</p><p>  (1) $f(x,y) \ge 0, \ -\infty &lt; x,y &lt; +\infty$ ;</p><p>  (2)  $\int^{+\infty}<em>{-\infty} \int^{+\infty}</em>{-\infty} f(x,y) dxdy = F(+\infty, +\infty) =1$.</p><hr><h5 id="二维连续型随机向量的性质"><a href="#二维连续型随机向量的性质" class="headerlink" title="二维连续型随机向量的性质"></a>二维连续型随机向量的性质</h5><p>(1) $F(x,y)$ 连续，且在$f(x ,y )$的连续点处有</p><script type="math/tex; mode=display">\frac{\partial^2 F(x, y)}{\partial x \partial y} = f(x, y) ;</script><p>(2) 对平面内任意一条曲线$L$，有$P((X,Y) \in L) = 0 ;$</p><p>(3)  对任意一个平面上的集合$D$，有</p><script type="math/tex; mode=display">P((X,Y) \in D) = \int\int_Df(x,y)dxdy</script><hr><h4 id="两个常见分布"><a href="#两个常见分布" class="headerlink" title="两个常见分布"></a>两个常见分布</h4><h5 id="均匀分布-1"><a href="#均匀分布-1" class="headerlink" title="均匀分布"></a>均匀分布</h5><p>平面区域上的均匀分布：设$(X, Y)$的联合密度函数为</p><script type="math/tex; mode=display">f(x,y) = \left \{    \begin{align}\frac{1}{|G|}   \quad  (x,y ) \in G    \\0   \quad  \qquad \quad  其余\end{align}\right .</script><p>其中$G$是平面上的某个有界区域，$|G|$表示区域面积，则称随机变量$(X, Y)$服从区域$G$的<strong>均匀分布</strong>.</p><hr><h5 id="n维正态分布"><a href="#n维正态分布" class="headerlink" title="n维正态分布"></a>n维正态分布</h5><p>我们将正态分布推广到$n$维，有对<strong>n维向量组$X:x_1,x_2,…x_n$</strong>：</p><script type="math/tex; mode=display">f(X) = \frac{1}{(\sqrt{2\pi})^n\sqrt{|\Sigma|}}exp\{-\frac{1}{2}(X-\mu)^T \Sigma^{-1}(X-\mu)\}</script><p> 其中，$\mu$ 是高斯分布的<strong>均值向量</strong>，$\Sigma$ 是<strong>协方差矩阵</strong>.</p><p>特别地，当$n=2$时有：</p><p>设$(X,Y)$的联合密度函数为</p><script type="math/tex; mode=display">f(x,y) = \frac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}} exp({- \frac{1}{2(1-\rho)^2}[\frac{(x-\mu_1)^2}{\sigma_1^2}-\frac{2\rho(x-\mu_1)(y-\mu_2)}{\sigma_1\sigma_2}+\frac{(y-\mu_2)^2}{\sigma_2^2}]})</script><p>其中</p><script type="math/tex; mode=display">-\infty < \mu_1 , \mu_2 < +\infty , \quad \sigma_1,  \sigma_2 >0 , \quad |\rho| < 1,</script><p>则称$(X, Y)$服从参数为$\mu_1,\mu_2,\sigma_1^2,\sigma_2^2, \rho$ 的<strong>二维正态分布</strong>，</p><p>并记作</p><script type="math/tex; mode=display">(X, Y) \sim N(\mu_1,\mu_2, \sigma_1^2, \sigma_2^2, \rho) .</script><h4 id="边缘概率密度函数"><a href="#边缘概率密度函数" class="headerlink" title="边缘概率密度函数"></a>边缘概率密度函数</h4><h5 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h5><p>  对于二维连续型随机变量$(X,Y)$，它的两个分量$X$或$Y$本身就是两个一维连续型随机变量，因此它们各自的分布就称为$(X,Y)$的关于$X$或关于$Y$的<strong>边缘分布（边缘概率密度函数）</strong>.</p><h5 id="边缘分布函数"><a href="#边缘分布函数" class="headerlink" title="边缘分布函数"></a>边缘分布函数</h5><script type="math/tex; mode=display">F_X(x) = P(X \le x)    \\\qquad \qquad \qquad \ \ \ \ = P(X \le x, Y < +\infty)    \\\qquad \qquad \qquad \qquad \ =\lim_{y\rightarrow+\infty} P(X \le x, Y \le y)     \\\qquad \qquad= \lim_{y \rightarrow +\infty} F(x, y)    \\\qquad \quad\doteq F(x, +\infty)</script><p>称$F_X(x) \doteq = F(x, +\infty)$ 为随机变量$X$的<strong>边缘分布函数</strong>.</p><p><em>随机变量Y的边缘分布函数同理</em></p><h5 id="边缘概率密度函数-1"><a href="#边缘概率密度函数-1" class="headerlink" title="边缘概率密度函数"></a>边缘概率密度函数</h5><script type="math/tex; mode=display">F_X(x) = P(X \le x, Y < +\infty) = \int^x_{-\infty} \{ \int^{\infty}_{\infty}f(x,y)dy \}dx</script><p>按一维密度函数定义，称</p><script type="math/tex; mode=display">f_X(x) = \int^{+\infty}_{-\infty} f(x,y)dy , \quad -\infty <x<+\infty</script><p>为随机变量 $X$ 的<strong>边缘概率密度函数</strong>（或边缘分布）；</p><p>同理，</p><script type="math/tex; mode=display">F_Y(y) = P(X < +\infty, Y \le y) = \int^y_{-\infty} \{ \int^{\infty}_{\infty}f(x,y)dx \}dy</script><p>称</p><script type="math/tex; mode=display">f_Y(y) = \int^{+\infty}_{-\infty} f(x,y)dx , \quad -\infty <y<+\infty</script><p>为随机变量 $Y$ 的<strong>边缘概率密度函数</strong>（或边缘分布）. </p><h4 id="随机变量的相互独立性"><a href="#随机变量的相互独立性" class="headerlink" title="随机变量的相互独立性"></a>随机变量的相互独立性</h4><h5 id="定义-3"><a href="#定义-3" class="headerlink" title="定义"></a>定义</h5><p>  如果等式$F(x,y) = F_X(x)F_Y(y)$ 对所有的 $-\infty &lt; x,y &lt; +\infty$ 成立，那么称随机变量 $X$ 与 $Y$ <strong>相互独立</strong>.</p><p>  对于连续型随机变量而言，等价于等式：$f(x,y) = f_X(x)f_Y(y)$ 在 $f(x,y) , f_X(x), f_Y(y)$ 的一切公共连续点上成立.</p><h5 id="定理3-7"><a href="#定理3-7" class="headerlink" title="定理3.7"></a>定理3.7</h5><p>  设$(X,Y) \sim N(\mu_1, \mu_2, \sigma_1^2,\sigma_2^2, \rho)$ ，那么 $X$ 与 $Y$ 相互独立的充分必要条件是 $\rho =0$. </p><h4 id="条件密度函数"><a href="#条件密度函数" class="headerlink" title="条件密度函数"></a>条件密度函数</h4><h5 id="定义-4"><a href="#定义-4" class="headerlink" title="定义"></a>定义</h5><p>  设$(X,Y)$ 的密度函数为 $f(x,y)$ ，对于任意固定的$y, -\infty<y < +\infty$ ，当$f_y(y)>0$时，称</y></p><script type="math/tex; mode=display">f_{x|y} = \frac{f(x,y)}{f_Y(y)} , \quad  -\infty < x < +\infty</script><p>为已知事件${Y =y }$ 发生的条件下 $X$ 的<strong>条件概率密度函数</strong>或条件分布；$y$ 同理.</p><p>  相应的条件分布函数定义为</p><script type="math/tex; mode=display">F_{X|Y}(x|y) = \int^x_{-\infty} f_{X|Y} (u|y)du = \int^x_{-\infty} \frac{f(u,y)}{f_Y(y)}du    \ , \\-\infty < x < +\infty</script><h3 id="随机变量函数的分布"><a href="#随机变量函数的分布" class="headerlink" title="随机变量函数的分布"></a>随机变量函数的分布</h3><h4 id="一维随机变量函数"><a href="#一维随机变量函数" class="headerlink" title="一维随机变量函数"></a>一维随机变量函数</h4><h5 id="定理3-8"><a href="#定理3-8" class="headerlink" title="定理3.8"></a>定理3.8</h5><p>  当$X \sim N(\mu, \sigma^2)$ 时，$Y = kX+c \sim N(ku+c, k^2\sigma^2)$</p><p>其中$k,c$ 为常数，且$k \neq 0$；特别地，有$\frac{X -\mu}{\sigma} \sim N(0,1)$.</p><h4 id="二维随机变量函数"><a href="#二维随机变量函数" class="headerlink" title="二维随机变量函数"></a>二维随机变量函数</h4><h5 id="随机变量和的分布"><a href="#随机变量和的分布" class="headerlink" title="随机变量和的分布"></a>随机变量和的分布</h5><h6 id="卷积公式"><a href="#卷积公式" class="headerlink" title="卷积公式"></a>卷积公式</h6><p>  一般地，设随机变量 $(X,Y )$ 的联合密度函数为$f(x,y)$，则 $Z =X+ Y$ 的密度函数为</p><script type="math/tex; mode=display">f_Z(z) = \int^{+ \infty}_{- \infty} f(x, z-x) dx</script><p>当 $X$ 与 $Y$ 独立时，上式成为</p><script type="math/tex; mode=display">f_Z(z) = \int^{+ \infty}_{- \infty} f_X(x) f_Y(z-x) dx</script><p>该公式称为 <strong>卷积公式</strong> .</p><p>将 $X$ 与 $Y $ 对调，得到卷积公式的另一形式：</p><script type="math/tex; mode=display">f_Z(z) = \int^{+ \infty}_{- \infty} f_X(z-y) f_Y(y) dy</script>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;  本章主要内容是&lt;strong&gt;连续型随机变量及其分布&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&quot;连续型随机变量及其分布&quot;&gt;&lt;a href=&quot;#连续型随机变量及其分布&quot; class=&quot;headerlink&quot; title=&quot;连续型随机变量及其分布&quot;&gt;&lt;/a&gt;连续型随机变量及其分布&lt;/h2&gt;&lt;h3 id=&quot;基本概念&quot;&gt;&lt;a href=&quot;#基本概念&quot; class=&quot;headerlink&quot; title=&quot;基本概念&quot;&gt;&lt;/a&gt;基本概念&lt;/h3&gt;&lt;p&gt;  给定一个随机变量$X$，称定义域为$(-\infty, +\infty)$ 的实值函数 &lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
F(x) = P (X \le x)，-\infty &lt; x &lt; + \infty&lt;/script&gt;&lt;p&gt;为随机变量$X$的&lt;strong&gt;分布函数&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;对任意实数 $x \in R , {X \le x}$ 是一个随机事件，它的概率$P(X \le x)$ 就定义为点$x$处分布函数的函数值.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;注：对任意满足条件 $-\infty &amp;lt; a +\infty$ 的实数 $a,b,$ 有&lt;script type=&quot;math/tex; mode=display&quot;&gt;
P(a &lt; X \le b) = F(b) - F(a).&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
    
    </summary>
    
    
      <category term="概率论与数理统计" scheme="http://yoursite.com/categories/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"/>
    
    
      <category term="数学" scheme="http://yoursite.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
      <category term="概率论" scheme="http://yoursite.com/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>概率论与数理统计(一)</title>
    <link href="http://yoursite.com/2021/01/06/probability/probability-1/"/>
    <id>http://yoursite.com/2021/01/06/probability/probability-1/</id>
    <published>2021-01-06T15:09:16.960Z</published>
    <updated>2021-01-06T15:09:16.960Z</updated>
    
    <content type="html"><![CDATA[<p>  概率论是研究概率和随机现象的数学分支，是研究随机性或不确定性等现的象数学。概率论是统计学的数学基础，也是机器学习的重要数学基础之一。本章主要内容是<strong>离散型随机变量及其分布</strong>。</p><h1 id="概率论与数理统计-一"><a href="#概率论与数理统计-一" class="headerlink" title="概率论与数理统计(一)"></a>概率论与数理统计(一)</h1><h2 id="离散型随机变量及其分布"><a href="#离散型随机变量及其分布" class="headerlink" title="离散型随机变量及其分布"></a>离散型随机变量及其分布</h2><h3 id="随机变量及概率函数"><a href="#随机变量及概率函数" class="headerlink" title="随机变量及概率函数"></a>随机变量及概率函数</h3><h4 id="随机变量"><a href="#随机变量" class="headerlink" title="随机变量"></a>随机变量</h4><p><strong>定义：</strong> 给定一个随机试验，$\Omega$是样本空间，如果对$\Omega$中的每个样本点$\omega$，都有一个实数X($\omega$)与之对应，那么就把这个定义域为 $\Omega$ 的单值实值函数：X = X($\omega$) 称为是（一维）随机变量.</p><ul><li>一般用大写字母 X, Y, Z, … 等表示随机变量</li><li>把随机变量 X 的值域记做 $\Omega _x $，则 $\Omega _x$ $\subseteq$ (- $\infty$, + $\infty$)</li><li>随机变量的取值规律反映了随机现象的统计规律性，描述这种规律性的各种形式称为<strong>分布</strong>。</li></ul><hr><a id="more"></a> <h4 id="概率函数"><a href="#概率函数" class="headerlink" title="概率函数"></a>概率函数</h4><p><strong>定义：</strong> 如果一个随机变量只可能取有限个值或可列无限个值，那么称这个随机变量为（一维）<strong>离散型随机变量</strong>.</p><p>  离散型随机变量的分布的表现形式称为<strong>概率函数</strong>.</p><hr><h3 id="常见一维离散型分布"><a href="#常见一维离散型分布" class="headerlink" title="常见一维离散型分布"></a>常见一维离散型分布</h3><h4 id="0-1分布"><a href="#0-1分布" class="headerlink" title="0-1分布"></a>0-1分布</h4><p>如果随机变量X 的概率函数为</p><p>$ P(X=0) = 1-p ,   P(X=1) = p  (0&lt; p &lt;1) $</p><p>那么称X服从参数为p的<strong>0-1分布</strong>，记为<strong>X~B(1, p).</strong></p><p>也可以用下面的式子表示为</p><p>$ P(X = k) = p^k(1-p)^k , k = 0, 1 $</p><hr><h4 id="二项分布"><a href="#二项分布" class="headerlink" title="二项分布"></a>二项分布</h4><p>如果随机变量X的概率函数为</p><script type="math/tex; mode=display">P(X = k) = C^k_n p^k (1-p)^{n-k} , k = 0, 1, 2, ..., n</script><p>则称X服从参数为n, p的二项分布，记为<strong>X~B(n, p).</strong></p><p>其中0 &lt; p &lt; 1.</p><p><strong>注：</strong></p><ol><li>$ \sum^n_{k=0} C^k_n p^k (1-p)^{n-k} = (p + (1-p)) = 1$</li><li>0-1分布可以看作是二项分布在n=1时的特例</li></ol><hr><h4 id="泊松分布"><a href="#泊松分布" class="headerlink" title="泊松分布"></a>泊松分布</h4><p>若随机变量X的概率函数为</p><p>$P(X=k) = \frac{\lambda ^ k}{k!} e^{-\lambda}, k =0, 1, 2, …, n$</p><p> 则称X服从参数为$\lambda$的泊松分布，记作<strong>X ~ P($\lambda$).</strong></p><p>由无穷级数的知识可以验证 $\sum^\infty_{k=0} \frac{\lambda ^ k}{k!} e^{-\lambda} = 1$</p><hr><h5 id="泊松定理"><a href="#泊松定理" class="headerlink" title="- 泊松定理"></a>- 泊松定理</h5><p>设 $\lambda = n p_n &gt; 0, 0 &lt; p_n &lt;1$，对任意一个非负整数k，</p><script type="math/tex; mode=display">\lim_{n\to \infty} C^k_n p^k_n (1 - p_n) ^{n-k} = \frac{\lambda ^k}{k!} e^{-\lambda}</script><p>泊松定理告诉我们：二项概率可以用服从泊松分布的概率值来近似，且当</p><p>$ n \ge 10, p \le 0.1$时近似效果比较理想.</p><hr><h3 id="二维随机变量和概率函数"><a href="#二维随机变量和概率函数" class="headerlink" title="二维随机变量和概率函数"></a>二维随机变量和概率函数</h3><h4 id="二维随机变量"><a href="#二维随机变量" class="headerlink" title="二维随机变量"></a>二维随机变量</h4><p><strong>定义：</strong> 给定一个随机试验，$\Omega$是它的样本空间，如果对$\Omega$中的每一个样本点$\omega$，都有一对有序实数$(X, Y)$与之对应，则称$(X, Y)$ 是<strong>二维随机变量.</strong></p><p>  如果一个二维随机变量只可能取有限个或可列无限个数值，则称其为<strong>二维离散型随机变量.</strong></p><hr><h4 id="联合概率函数"><a href="#联合概率函数" class="headerlink" title="联合概率函数"></a>联合概率函数</h4><p>  设$(X,Y)$的值域为$\Omega_{(X, Y)} = {(a_i, b_i): i, j = 1, 2, …}$</p><p>称表达式：</p><script type="math/tex; mode=display">P(X = a_i, Y = b_j) \doteq P(\{X = a_i \} \cap \{Y = b_j\}) = p_{ij}  \\i = 1, 2, ...; j = 1, 2, ...</script><p>为二维随机变量$(X, Y )$的<strong>联合概率函数</strong>或<strong>联合分布律.</strong></p><p>其中$p_{ij}$须满足下列条件：</p><ol><li>$p_{ij} \ge 0 ;$</li><li>$\sum<em>i \sum_j p </em>{ij} = 1.$</li></ol><p>事实上利用<strong>联合概率函数</strong>，可以求任意事件的概率：</p><script type="math/tex; mode=display">P((X,Y) \in D) = \sum_{(a_i, b_j) \in D} P (X = a_i, Y = b_j) = \sum_{(a_i, b_j) \in D} p_{ij}</script><hr><h4 id="边缘概率函数"><a href="#边缘概率函数" class="headerlink" title="边缘概率函数"></a>边缘概率函数</h4><p>  对于随机变量$(X, Y)$，分量$X$或$Y$本身就说一个一维随机变量，它们各自的概率函数就称为$(X, Y)$的关于$X$或关于$Y$的<strong>边缘概率函数</strong>或<strong>边缘分布律.</strong></p><p>  设随机变量$(X,Y)$的<strong>联合概率函数</strong>为</p><script type="math/tex; mode=display">P(X = a_i, Y = b_j) = p_{ij},  \quad  i, j =1, 2, ...</script><p>则随机变量$X$的值域为$\Omega_x = {a_1, a_2, …}$， 而$X$的边缘概率函数或边缘分布律定义为</p><script type="math/tex; mode=display">P = (X = a_i) = \sum_j p_{ij} \doteq p_{i·}, \quad i = 1, 2 , ..</script><p> 随机变量$Y$的值域为$\Omega_y = {b_1, b_2, …}$， 而$Y$的边缘概率函数或边缘分布律定义为</p><script type="math/tex; mode=display">P (Y = b_j) = \sum_i p_{ij} \doteq p_{·j} , \quad j = 1, 2, ...</script><hr><h3 id="随机变量的独立性和条件分布"><a href="#随机变量的独立性和条件分布" class="headerlink" title="随机变量的独立性和条件分布"></a>随机变量的独立性和条件分布</h3><h4 id="随机变量的独立性"><a href="#随机变量的独立性" class="headerlink" title="随机变量的独立性"></a>随机变量的独立性</h4><p>  设随机变量$X$与$Y$的联合概率函数为</p><script type="math/tex; mode=display">P = (X = a_i, Y = b_j) = p_{ij} , \quad i, j = 1, 2, ...</script><p>如果等式$P (X = a_i, Y = b_j) = P (X = a_i) \times P(Y = b_j)$</p><p>也即$p<em>{ij} = p</em>{i·} p<em>{·j}$对所有的i, j = 1, 2, … 都成立，那么就称随机变量$X$和$Y$是相互<em>_独立</em></em>的.</p><hr><p>​    </p><p>  如果随机变量$X<em>1, X_2, …, X_n$的联合概率函数恰为n个边缘概率函数的乘积，即对$a_i \in \Omega</em>{x_i}(i = 1, …, n)$，</p><script type="math/tex; mode=display">P(X_1 = a_1, X_2 = a_2, ..., X_n = a_n) = \prod^n_{i = 1} P(X_i = a_i)</script><p>则称这n个随机变量$X_1, X_2, …, X_n$相互独立.</p><hr><p>  随机变量$X$与$Y$相互独立的充分必要条件是：</p><p>对于实数轴上的任意两个集合$S_1$与$S_2$，总有</p><script type="math/tex; mode=display">P(X \in S_1, Y \in S_2) = P(X \in S_1) \times P(Y \in S_2).</script><p>定理可以推广，故当$X_1, X_2, …, X_n$相互独立时，这n个随机变量中的任意k个也是相互独立的 ($2 \le k \le n-1$).</p><p>  进一步n个随机变量相互独立保证它们两两独立.</p><hr><h4 id="条件概率函数"><a href="#条件概率函数" class="headerlink" title="条件概率函数"></a>条件概率函数</h4><p>  设随机变量$(X,Y)$的联合分布为</p><script type="math/tex; mode=display">P(X = a_i, Y = b_j) = p_{ij} , \quad i, j = 1, 2, ...</script><p>若对任意一个固定的$j$，$P(Y = b_j) &gt;0, \ (j=1, 2, …)$</p><p>则称$ P(X = a<em>i | Y = b_j) = \frac{p</em>{ij}}{p_{·j}}, \quad i= 1, 2, …$</p><p>为已知事件${Y = b_j}$发生的条件下随机变量$X$的条件概率函数.</p><p>  类似地，对任意一个固定的$i, P(X = a_i) &gt; 0, \ (i =1, 2, …),$</p><p>称</p><script type="math/tex; mode=display">P (Y = b_j | X = a_i) = \frac{p_{ij}}{p_{i·}} \quad j = 1, 2, ...</script><p>为已知事件${X = a_i}$发生的条件下随机变量$Y$的条件概率函数.</p><p>  条件分布也是分布，易知$ \frac{p<em>{ij}}{p</em>{i·}}$或$\frac{p<em>{ij}}{p</em>{·j}}$满足</p><ol><li>$ \frac{p<em>{ij}}{p</em>{i·}} \ge 0, \quad  \frac{p<em>{ij}}{p</em>{·j}} \ge0$</li><li>$\sum<em>j  \frac{p</em>{ij}}{p<em>{i·}} = 1, \quad \sum_i \frac{p</em>{ij}}{p_{·j}} = 1$</li></ol><hr><h3 id="随机变量函数的分布"><a href="#随机变量函数的分布" class="headerlink" title="随机变量函数的分布"></a>随机变量函数的分布</h3><h4 id="离散型随机变量函数的分布"><a href="#离散型随机变量函数的分布" class="headerlink" title="离散型随机变量函数的分布"></a>离散型随机变量函数的分布</h4><p>  在随机变量$X$上定义函数$g(X)$，表示一个随机变量$Y$ ，且当$X = x$时， $Y = y = g(x)$，并记作$Y = g(X)$.</p><p>那么$Y$就称为是随机变量$X$的函数.</p><hr><h4 id="离散型随机变量的分布可加性"><a href="#离散型随机变量的分布可加性" class="headerlink" title="离散型随机变量的分布可加性"></a>离散型随机变量的分布可加性</h4><ul><li><p><strong>定理1</strong> 设$X_1, X_2, …, X_n$是独立同分布的随机变量，且</p><script type="math/tex; mode=display">X_i \sim B(1, p), \quad i = 1, 2, ..., n,</script><p>记$Y = \sum^n_{i =1} $，则$Y \sim B (n, p).$</p><p><strong>思路</strong>   </p><script type="math/tex; mode=display">\begin{align*}& \ P (Y = k) \quad (k=0, 1, 2, ..., n)     \\& =P(\{X_1, ..., X_n 中恰有k个取值是1，n-k个取值是0\})    \\& =C^k_n p^k (1-p)^{n-k}\end{align*}</script><hr></li></ul><ul><li><p><strong>定理2（分布的可加性）</strong> 设$X$与$Y$相互独立，那么</p><ol><li><p>当 $X \sim B(m,p), Y \sim B(n,p) $ 时，有</p><script type="math/tex; mode=display">X + Y \sim B(m+n, p);</script></li><li><p>当$X \sim P(\lambda_1), Y \sim P(\lambda_2)$ 时，有</p><script type="math/tex; mode=display">X + Y \sim P(\lambda_1 + \lambda_2).</script><p><em>定理2可推广到n个相互独立的随机变量的和.</em></p></li></ol><hr></li></ul><ul><li><p><strong>定理3</strong>    设$X<em>1, X_2, …, X_n$ 是相互独立的随机变量，对于任意一个正整数$m(1\le m \le n-1)$，下面两个随机变量$g(X_1,X_2, …, X_m)$与$h(X</em>{m+1}, …, X_n)$ 相互独立，其中$g$与$h$都是单值函数.</p><p>由定理可知，若$X,Y$相互独立，则$g(X), h(Y)$ 也相互独立.</p></li></ul><hr>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;  概率论是研究概率和随机现象的数学分支，是研究随机性或不确定性等现的象数学。概率论是统计学的数学基础，也是机器学习的重要数学基础之一。本章主要内容是&lt;strong&gt;离散型随机变量及其分布&lt;/strong&gt;。&lt;/p&gt;
&lt;h1 id=&quot;概率论与数理统计-一&quot;&gt;&lt;a href=&quot;#概率论与数理统计-一&quot; class=&quot;headerlink&quot; title=&quot;概率论与数理统计(一)&quot;&gt;&lt;/a&gt;概率论与数理统计(一)&lt;/h1&gt;&lt;h2 id=&quot;离散型随机变量及其分布&quot;&gt;&lt;a href=&quot;#离散型随机变量及其分布&quot; class=&quot;headerlink&quot; title=&quot;离散型随机变量及其分布&quot;&gt;&lt;/a&gt;离散型随机变量及其分布&lt;/h2&gt;&lt;h3 id=&quot;随机变量及概率函数&quot;&gt;&lt;a href=&quot;#随机变量及概率函数&quot; class=&quot;headerlink&quot; title=&quot;随机变量及概率函数&quot;&gt;&lt;/a&gt;随机变量及概率函数&lt;/h3&gt;&lt;h4 id=&quot;随机变量&quot;&gt;&lt;a href=&quot;#随机变量&quot; class=&quot;headerlink&quot; title=&quot;随机变量&quot;&gt;&lt;/a&gt;随机变量&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;定义：&lt;/strong&gt; 给定一个随机试验，$\Omega$是样本空间，如果对$\Omega$中的每个样本点$\omega$，都有一个实数X($\omega$)与之对应，那么就把这个定义域为 $\Omega$ 的单值实值函数：X = X($\omega$) 称为是（一维）随机变量.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一般用大写字母 X, Y, Z, … 等表示随机变量&lt;/li&gt;
&lt;li&gt;把随机变量 X 的值域记做 $\Omega _x $，则 $\Omega _x$ $\subseteq$ (- $\infty$, + $\infty$)&lt;/li&gt;
&lt;li&gt;随机变量的取值规律反映了随机现象的统计规律性，描述这种规律性的各种形式称为&lt;strong&gt;分布&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
    
    </summary>
    
    
      <category term="概率论与数理统计" scheme="http://yoursite.com/categories/%E6%A6%82%E7%8E%87%E8%AE%BA%E4%B8%8E%E6%95%B0%E7%90%86%E7%BB%9F%E8%AE%A1/"/>
    
    
      <category term="数学" scheme="http://yoursite.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
      <category term="概率论" scheme="http://yoursite.com/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>条件随机场</title>
    <link href="http://yoursite.com/2021/01/06/probabilistic-graphical-model/CRF/"/>
    <id>http://yoursite.com/2021/01/06/probabilistic-graphical-model/CRF/</id>
    <published>2021-01-06T15:09:16.959Z</published>
    <updated>2021-01-06T15:09:16.959Z</updated>
    
    <content type="html"><![CDATA[<h1 id="条件随机场"><a href="#条件随机场" class="headerlink" title="条件随机场"></a>条件随机场</h1><p> 首先，<strong>概率图模型</strong>(<em>Probabilistic graphical model</em>)的体系结构如下：</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/PGM1.jpg" alt="PGM"></p><p> 可见，条件随机场是<strong>无向图模型</strong>(<em>Unidirected graphical models</em>)中<strong>马尔可夫网络</strong>(<em>Markov networks</em>)的一种.</p><p> 具体地，<strong>条件随机场</strong>(<em>Conditional random field</em>) 是给定随机变量 $X$ 的条件下，随机变量 $Y$ 的马尔可夫随机场.  这里主要介绍定义在线性链上的特殊的条件随机场，称为<strong>线性链条件随机场</strong> (<em>Linear chain conditional random field</em>). 线性链条件随机场可以用于标注等问题.</p><a id="more"></a><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p> 首先给出一般的条件随机场的定义:</p><p><strong>(条件随机场)</strong> 设 $X$ 与 $Y$ 是随机变量，$P(Y|X)$ 是在给定 $X$ 的条件下 $Y$ 的条件概率分布.  若随机变量 $Y$ 构成一个由无向图 $G=(V,E)$ 表示的<strong>马尔可夫随机场</strong>，即</p><script type="math/tex; mode=display">P(Y_v|X,Y_w,w\neq v) = P(Y_v|X,Y_w,w\sim v)    \tag{1}</script><p>对任意结点 $v$ 成立，则称条件概率分布 $P(Y|X)$ 为条件随机场.</p><p>其中，$w\sim v$ 表示表示图 $G=(V,E)$ 中与与结点 $v$ 有边连接的所有结点 $w$, $w\neq v$ 表示结点 $v$ 以外的所有结点.</p><p> 注意：定义中并没有要求 $X$ 和 $Y$ 具有相同的结构. 但在现实中，<strong>一般假设 $X$ 和 $Y$ 有相同的图结构</strong>. 即主要考虑线性链的情况如下：</p><p><img src="http://pbug3xg5x.bkt.clouddn.com/CRF1.jpg" alt="线性链"></p><p>给出线性链条件随机场的定义：</p><p><strong>(线性链条件随机场)</strong> 设 $X=(X_1,X_2,…,X_n)$ , $Y=(Y_1,Y_2,…,Y_n)$ 均<strong>为线性链表示的随机变量序列</strong>，若在给定随机变量序列 $X$ 的条件下，随机变量序列 $Y$ 的条件概率分布 $P(Y|X)$ 构成条件随机场，即满足马尔可夫性</p><script type="math/tex; mode=display">P(Y_i|X,Y_1,...,Y_{i-1},Y_{i+1},...,Y_n) = P(Y_i|X,Y_{i-1},Y_{i+1})    \tag{2}</script><p>则称 $P(Y|X)$ 为线性链条件随机场.</p><h2 id="无向图"><a href="#无向图" class="headerlink" title="无向图"></a>无向图</h2><p> 要理解条件随机场，首先我们要引入<strong>无向图(一般可指马尔可夫网络)</strong>的概念.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/CRF2.jpg" alt="UGM"></p><p> 如果一个图太大，则我们可以用<strong>因子分解</strong>(<em>factoriazation</em>)将 $P=(Y)$ 写成若干个联合概率的乘积。 具体来说，即将一个图分为若干个<strong>最大团</strong>(任何两个结点均有边连接，且不能加入任何一个结点成为更大的 <strong>团</strong>).</p><p>那么我们有：</p><script type="math/tex; mode=display">P(Y) = \frac{1}{Z(x)}\prod_c \psi_c(Y_c)    \tag{3}</script><p>其中，$Z(x) = \sum_Y \prod_c \psi_c(Y_c)$，归一化是为了让结果算作概率.</p><p>于是，上图中：</p><script type="math/tex; mode=display">P(Y) = \frac{1}{Z(x)}(\psi_1(X_1,X_3,X_4)\cdot\psi_2(X_2,X_3,X_4))    \tag{4}</script><p>其中，$\psi_c(Y_c)$ 是一个最大团 $C$ 上随机变量的联合概率，一般去指数函数：</p><script type="math/tex; mode=display">\psi_c(Y_c) = e^{-E(Y_c)}=e^{\sum_k\lambda_kf_k(c,y|c,x)}</script><p>称$\psi_c(Y_c)$ 为 <strong>势函数</strong>(<em>potential function</em>).</p><p>那么概率无向图的联合概率分布可以因子分解为：</p><script type="math/tex; mode=display">\begin{align}P(Y) &= \frac{1}{Z(x)}\prod_c \psi_c(Y_c)  \\&= \frac{1}{Z(x)}\prod_cexp({\sum_k\lambda_kf_k(c,y|c,x)} \\&= \frac{1}{Z(x)}exp({\sum_c\sum_k\lambda_kf_k(y_i,y_{i-1},x,i)})\end{align}    \tag{5}</script><p><strong>概率无向图模型的因子分解由 Hammersley-Clifford 定理保证</strong>. 此处不展开.</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="建模公式"><a href="#建模公式" class="headerlink" title="建模公式"></a>建模公式</h3><p>我们给定长度为$T$ 的<strong>观测序列</strong> $O=(O_1,O_2,…,O_T)$ 和 <strong>状态序列</strong> $I=(I_1,I_2,…,I_T)$.</p><p>由式(2)和式(5)我s们可以得到<strong>条件随机场的建模公式</strong>：</p><script type="math/tex; mode=display">\begin{align}P(I|O) &= \frac{1}{Z(O)}\prod_i \psi_i(I_i|O)    \\&= \frac{1}{Z(O)}\prod_i exp(\sum_k\lambda_kf_k(O,I_{i-1},I_i,i))    \\&= \frac{1}{Z(O)}exp(\sum^T_{i=1}\sum^M_{k=1}\lambda_kf_k(O,I_{i-1},I_i,i))\end{align}    \tag{6}</script><p>其中 </p><ul><li>下标 $i$ 表示当前所在结点($token$)的位置.</li><li>下标 $k$ 表示这是第几个特征函数，并且每个特征函数都赋予一个<strong>权重</strong> $\lambda_k$. 在每个团里，我们为每个 $token_i$ 构造 $M$ 个特征，每个特征执行一定的限定作用，建模的时候再为每个特征函数加权求和.</li><li>其中，$Z(O)=\sum<em>I exp(\sum^T</em>{i=1}\sum^M<em>{k=1}\lambda_kf_k(O,I</em>{i-1},I_i,i))$，表示在所有可能的状态序列上求和.</li><li>$f_k$ 表示 <strong>特征函数</strong>.</li><li>$P(I|O)$ 表示给定了<strong>一条观测序列</strong> $O=(o_1,…,o_i)$ 的条件下，所求出来的<strong>状态序列</strong>$I = (i_1,…,i_i)$ 的概率.</li></ul><h3 id="特征函数"><a href="#特征函数" class="headerlink" title="特征函数"></a>特征函数</h3><p>特征函数 $f_k$ ，实际上是两种特征函数的结合： <strong>转移特征</strong>和<strong>状态特征</strong>.</p><p>对<strong>建模公式(6)</strong>展开，我们有：</p><script type="math/tex; mode=display">\begin{align}P(I|O) &= \frac{1}{Z(O)}exp(\sum^T_{i=1}\sum^M_{k=1}\lambda_kf_k(O,I_{i-1},I_i,i))     \\&= \frac{1}{Z(O)}exp[\sum_{i=1}^T\sum_{j=1}^J\eta_kt_k(O,I_{i-1},I_i,i)+\sum^T_{i=1}\sum^L_{l=1}\mu_ls_l(O,I_i,i)]\end{align}    \tag{7}</script><p>其中：</p><ul><li><p>$t_j$ 为 $i$ 处的转移特征，对应权重 $\eta_j$，每个 $token_i$ 都有 $J$ 个特征，转移特征是对前后 $token$ 的限定.</p><ul><li>比如：<script type="math/tex; mode=display">t_{j=1}(O,I_{i-1},I_i,i) =\left \{\begin{align}&1 \qquad满足特点转移条件，比如前一个token 是“T”\\  &0 \qquad other\end{align}\right.</script></li></ul></li><li><p>$s_l$ 为 $i$ 处的状态特征，对应权重 $\mu_l$，每个 $token_i$ 都有 $L$ 个特征.</p><ul><li>比如<script type="math/tex; mode=display">s_{l=1}(O,I_i,i) =\left \{\begin{align}&1 \qquad满足特点状态条件，比如当前的token 是“R”\\  &0 \qquad other\end{align}\right.</script></li></ul></li></ul><p>满足特征条件就取值为1，否则没贡献，甚至可以给它打负分.</p><p>将它们合在一起，即为<strong>特征函数 $f_k$</strong>.  </p><script type="math/tex; mode=display">f_k(O,I_{i-1},I_i,i) = \left \{\begin{align}&t_k(O,I_{i-1},I_i,i), \qquad k=1,2,...,J    \\&s_{l}(O,I_i,i),  \qquad k= J+l; l=1,2,...,L\end{align}    \tag{8}\right.</script><p> 实际上，我们还可以进一步简化，</p><p>对转移特征与状态特征在各个位置 $i$ 求和，记作</p><script type="math/tex; mode=display">f_k(O,I) = \sum_{i=1}^T f_k(O,I_{i-1},I_i,i)    \tag{9}</script><p>对应地，权值记作</p><script type="math/tex; mode=display">\lambda_k =\left \{\begin{align}&\eta_k, \qquad k=1,2,...,J    \\&\mu_l,  \qquad k= J+l; l=1,2,...,L\end{align}    \tag{10}\right.</script><p>再进一步简化：</p><p>以 $\lambda$ 表示<strong>权值向量</strong>，即</p><script type="math/tex; mode=display">\lambda = (\lambda_1,\lambda_2,...,\lambda_k)^T    \tag{11}</script><p>以 $F(O,I,i)$ 表示<strong>全局特征向量</strong>，即</p><script type="math/tex; mode=display">F(O,I)=(f_1(O,I),f_2(O,I),...,f_k(O,I))^T    \tag{12}</script><p>则原来的乘积求和可以表示为向量内积的形式，条件随机场可以简化为：</p><script type="math/tex; mode=display">P(I|O) = \frac{exp(\lambda\cdot F(O,I))}{Z_\lambda(x)}    \tag{13}</script><p>其中，</p><script type="math/tex; mode=display">Z_\lambda = \sum_I exp(\lambda\cdot F(O,I))    \tag{14}</script><h2 id="运行过程"><a href="#运行过程" class="headerlink" title="运行过程"></a>运行过程</h2><blockquote><ol><li>预定义特征函数 .</li><li>在给定的数据上，训练模型，确定参数 $\lambda_k$ .</li><li>用训练好的模型解决预测问题.</li></ol></blockquote><h3 id="概率计算"><a href="#概率计算" class="headerlink" title="概率计算"></a>概率计算</h3><p> 我们像隐马尔可夫模型那样，引入前向-后向向量，递归地计算概率和期望值.</p><h4 id="前向-后向算法"><a href="#前向-后向算法" class="headerlink" title="前向-后向算法"></a>前向-后向算法</h4><p> 首先，引进特殊的起点和终点状态标记 $I<em>o=start, I</em>{T}=stop$.</p><p> 对 $i=1,2,…,T$ ，定义<strong>前向向量</strong> $\alpha_i(O)$</p><script type="math/tex; mode=display">\alpha_0(I|O)=\left \{\begin{align}&1, \qquad I=start    \\&0,  \qquad 否则\end{align}    \tag{15}\right.</script><p>递推公式为(注意，此处以及后面的<strong>上标T为转置的意思</strong>)：</p><script type="math/tex; mode=display">\alpha_i^T (I_i|O)= \alpha_{i-1}^T(I_{i-1}|O)[M_i(I_{i-1},I_{i}|O)], \quad i=1,2,...T    \tag{16}</script><p>又可表示为：</p><script type="math/tex; mode=display">\alpha_i^T(O) = \alpha_{i-1}^T(O)M_i(O)    \tag{17}</script><p>$\alpha_i(I_i|O)$ 表示在位置 $i$ 的标记是 $y_i$ 并且到位置 $i$ 的前部分标记序列的非规范化概率，$y_i$ 可取的值有 $m$ 个，所以 $\alpha_i(O)$ 是 $m $ 维列向量.</p><hr><p>其中，我们对观测序列 $O$ 的每一个位置 $i=1,2,…,T$ ，定义一个 $m$ 阶矩阵( $m$ 是标记 $I_i$ 取值的个数) </p><script type="math/tex; mode=display">M_i(O) = [M_i(I_{i-1},I_i)|O]    \tag{18}</script><script type="math/tex; mode=display">Mi(I_{i-1},I_i|O) = exp(W_i(I_{i-1},I_i|O))    \tag{19}</script><script type="math/tex; mode=display">W_i(I_{i-1},I_i|O)= \sum^M_{k=1}\lambda_kf_k(I_{i-1},I_i,O,i)    \tag{20}</script><hr><p>同样地，对 $i=0,1,..,T$ 定义后向向量 $\beta_i(O)$:</p><script type="math/tex; mode=display">\beta_T(I_T|O)=\left \{\begin{align}&1, \qquad I=stop    \\&0,  \qquad 否则\end{align}    \tag{21}\right.</script><script type="math/tex; mode=display">\beta_i(I_i|O) = [M_i(I_i,I_{i+1}|O)]\beta_{i+1}(I_{i+1}|O)    \tag{22}</script><p>又可表示为：</p><script type="math/tex; mode=display">\beta_i(O) = M_{i+1}(O)\beta_{i+1}(O)    \tag{23}</script><p>$\beta_i(I_i|O)$ 表示在位置 $i$ 的标记为 $I_i$ 并且从 $i+1$ 到 $T$ 的后部分标记序列的非规范化概率.</p><p> 由前向-后向向量定义不难得到：</p><script type="math/tex; mode=display">Z(O) = \alpha_T^T(O) \cdot1= 1^T\beta_1(O)    \tag{24}</script><p>这里，<strong>$1$ 是元素均为 $1$ 的 $m$ 维列向量</strong>.</p><p> 有前向-后向向量，可以比较方便的计算概率和期望值，这里不进行展开.</p><h3 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h3><p> 条件随机场实际上是定义在时序数据上的对数线性模型，可以通过<strong>极大似然估计</strong>求解参数.</p><p> 由式(6)和式(9)，可得到训练数据的<strong>对数似然函数</strong>为</p><script type="math/tex; mode=display">L(\lambda) = \log \prod_{x,y}P_\lambda(I|O) = \sum^T_{j=1} \sum^M_{k=1}\lambda_kf_k(O_j,I_j)-\sum^T_{j=1}\log Z_\lambda(O_j)    \tag{25}</script><p>可以由改进的<strong>迭代尺度法IIS、梯度下降法、牛顿法</strong>等方法来求解参数，此处不进行展开.</p><h3 id="预测算法"><a href="#预测算法" class="headerlink" title="预测算法"></a>预测算法</h3><p> 条件随机场的预测问题是给定条件随机场 $P(Y|X)$ 和输入序列(观测序列) $x$，求条件概率最大的输出序列(标记序列)的 $y^\ast$，即对观测序列进行标注.</p><p> 与<strong>隐马尔可夫模型</strong>的预测方法一样，条件随机场的预测方法也是<strong>维特比算法</strong>.</p><blockquote><p>输入：模型特征向量 $F(O|I)$(式12) 和权值向量 $\lambda$，观测序列 $O=(O_1,O_2,…,O_T)$.</p><p>初始化</p><script type="math/tex; mode=display">\delta_1(j) = \lambda \cdot F_1(I_o=start,I_1=j,O), \quad j=1,2,...,m</script><p>递推, 对 $i=2,3,…,T$</p><script type="math/tex; mode=display">\delta_i(l) = \max_{1\le j \le m}\{\delta_{i-1}(j)+\lambda\cdot F_i(I_{i-1}=j,I_i=l,O)\},\quad l=1,2,...,m    \\\psi_i(l) = \arg\max_{1 \le j \le m}\{\delta_{i-1}(j)+\lambda\cdot F_i(I_{i-1}=j,I_i=l,O)\} , \quad l=1,2,...,m</script><p>终止</p><script type="math/tex; mode=display">\max_y (\lambda\cdot F(O,I)) = \max_{1 \le j \le m}\delta_T(j)    \\I_T^\ast= \arg \max_{1 \le j\le m} \delta_T(j)</script><p>返回路径</p><script type="math/tex; mode=display">I^\ast_i = \psi_{i+1}(I_{i+1}^\ast), \qquad i=T-1,T-2,...,1</script><p>输出：最优路径 $I^\ast = (I_1^\ast,I^\ast_2,…,I_T^\ast)$.</p><p>参考资料：</p><p>[1] 李航. 统计学习方法[M], 北京: 清华大学出版社, 2012: 191-210</p><p>[2] Scofield-知乎. 如何用简单易懂的例子解释条件随机场（CRF）模型？它和HMM有什么区别？[EB/OL], <a href="https://www.zhihu.com/question/35866596" target="_blank" rel="noopener">https://www.zhihu.com/question/35866596</a>, : 2018-3-21/2018-8-3.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;条件随机场&quot;&gt;&lt;a href=&quot;#条件随机场&quot; class=&quot;headerlink&quot; title=&quot;条件随机场&quot;&gt;&lt;/a&gt;条件随机场&lt;/h1&gt;&lt;p&gt; 首先，&lt;strong&gt;概率图模型&lt;/strong&gt;(&lt;em&gt;Probabilistic graphical model&lt;/em&gt;)的体系结构如下：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog-fig.oss-cn-shenzhen.aliyuncs.com/PGM1.jpg&quot; alt=&quot;PGM&quot;&gt;&lt;/p&gt;
&lt;p&gt; 可见，条件随机场是&lt;strong&gt;无向图模型&lt;/strong&gt;(&lt;em&gt;Unidirected graphical models&lt;/em&gt;)中&lt;strong&gt;马尔可夫网络&lt;/strong&gt;(&lt;em&gt;Markov networks&lt;/em&gt;)的一种.&lt;/p&gt;
&lt;p&gt; 具体地，&lt;strong&gt;条件随机场&lt;/strong&gt;(&lt;em&gt;Conditional random field&lt;/em&gt;) 是给定随机变量 $X$ 的条件下，随机变量 $Y$ 的马尔可夫随机场.  这里主要介绍定义在线性链上的特殊的条件随机场，称为&lt;strong&gt;线性链条件随机场&lt;/strong&gt; (&lt;em&gt;Linear chain conditional random field&lt;/em&gt;). 线性链条件随机场可以用于标注等问题.&lt;/p&gt;
    
    </summary>
    
    
      <category term="概率图模型" scheme="http://yoursite.com/categories/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="判别模型" scheme="http://yoursite.com/tags/%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="条件随机场" scheme="http://yoursite.com/tags/%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA/"/>
    
      <category term="概率图模型" scheme="http://yoursite.com/tags/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>隐马尔可夫模型</title>
    <link href="http://yoursite.com/2021/01/06/probabilistic-graphical-model/HMM/"/>
    <id>http://yoursite.com/2021/01/06/probabilistic-graphical-model/HMM/</id>
    <published>2021-01-06T15:09:16.959Z</published>
    <updated>2021-01-06T15:09:16.959Z</updated>
    
    <content type="html"><![CDATA[<h1 id="隐马尔可夫模型"><a href="#隐马尔可夫模型" class="headerlink" title="隐马尔可夫模型"></a>隐马尔可夫模型</h1><p> <strong>隐马尔可夫模型</strong>(<em>Hidden Markov Model，HMM</em>)，是结构最简单的<strong>动态贝叶斯网</strong>(<em>dynamic Bayesian network</em>)，是一种著名的有向图模型，主要用于时序数据建模，在<strong>语音识别</strong>、<strong>自然语言处理</strong>等邻域有广泛应用.</p><a id="more"></a><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><h3 id="模型变量"><a href="#模型变量" class="headerlink" title="模型变量"></a>模型变量</h3><p>隐马尔可夫模型中的变量可分为两组：</p><ul><li><strong>状态变量</strong> $Q={q_1,q_2,…,q_N}$，其中 $q_i$ 表示第 $i$ 时刻的系统状态，$N$是可能的状态数，通常假定状态变量是隐藏的、不可被观测的，因此状态变量也称为隐变量.</li><li><strong>观测变量</strong> $V = {v_1,v_2,…,v_M}$，其中 $x_i$ 表示第 $i$ 时刻的观测值，$M$是可能的观测数.</li><li>对应地，$I=(i_1,i_2,…,i_T)$ 是长度为 $T$ 的<strong>状态序列</strong>，$O=(o_1,o_2,…,o_T)$ 是对应的<strong>观测序列</strong>.</li></ul><p>注意，隐马尔可夫模型有如下两个假设：</p><ul><li>(<strong>观测独立性假设</strong>) 观测变量的取值仅依赖与状态变量，即 $x_t$ 由 $y_t$ 确定，与其他状态变量及观测变量的取值无关.</li><li>(<strong>齐次马尔可夫假设</strong>) $t$ 时刻的状态 $q<em>t$ 仅依赖于 $t-1$ 时刻的状态 $q</em>{t-1}$，与此前的 $t-2$ 个状态无关. 即”<strong>马尔可夫链</strong>(<em>Markov chain</em>)”.  </li></ul><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/hmm5.jpg" alt="两个假设"></p><h3 id="模型参数"><a href="#模型参数" class="headerlink" title="模型参数"></a>模型参数</h3><p>除了上面的状态信息之外，想要确定一个隐马尔可夫模型，还需要以下3组参数：</p><ul><li><p><strong>状态转移概率矩阵</strong>:</p><script type="math/tex; mode=display">A = [a_{ij}]_{N\times N}    \tag{1}</script><p>其中，</p><script type="math/tex; mode=display">a_{ij} = P(i_{t+1}=q_j|i_t=q_i),\quad i=1,2,...,N; \ j=1,2,...N    \tag{2}</script><p>是在时刻 $t$ 处于状态 $q_i$ 的条件下，在 $t+1$ 时刻转移到状态 $q_j$ 的概率.</p></li><li><p><strong>输出观测概率矩阵</strong>:</p><script type="math/tex; mode=display">B = [b_j(k)]_{N\times M}    \tag{3}</script><p>其中，</p><script type="math/tex; mode=display">b_j(k) = P(o_t = v_k|i_t=q_j),\quad k=1,2,...,M;\ j=1,2,...N    \tag{4}</script><p>是在时刻 $t$ 处于状态 $q_j$ 的条件下，生成观测 $v_k$ 的概率.</p></li><li><p><strong>初始状态概率向量</strong>:</p><script type="math/tex; mode=display">\pi = (\pi_i)    \tag{5}</script><p>其中，</p><script type="math/tex; mode=display">\pi_i = P(i_1=q_i), \quad i=1,2,...N    \tag{6}</script><p>是时刻 $t=1$ 处于状态 $q_i$ 的概率.</p></li></ul><p>$A,B,\pi$ 称为隐马尔可夫模型的<strong>三要素</strong>. 因此，隐马尔可夫模型 $\lambda$ 可以用三元符号表示：</p><script type="math/tex; mode=display">\lambda = (A,B,\pi)    \tag{7}</script><h3 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h3><p>假设你是一个医生，眼前有个病人，你的任务是确定他是否得了感冒。</p><ul><li>首先，病人的状态($Q$)只有两种：{感冒，没有感冒}。</li><li>然后，病人的感觉（观测$V$）有三种：{正常，冷，头晕}。</li><li>手头有病人的病例，你可以从病例的第一天确定 $\pi$（初始状态概率向量）；</li><li>然后根据其他病例信息，确定 $A$（状态转移矩阵）也就是病人某天是否感冒和他第二天是否感冒的关系；</li><li>还可以确定 $B$（观测概率矩阵）也就是病人某天是什么感觉和他那天是否感冒的关系。</li></ul><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/hmm2.jpg" alt></p><h3 id="观测序列的生成"><a href="#观测序列的生成" class="headerlink" title="观测序列的生成"></a>观测序列的生成</h3><p> 根据隐马尔可夫模型定义，可以将一个长度为 $T$ 的观测序列 $O=(o_1,o_2,…o_n)$ 的生成过程描述如下：</p><blockquote><p>输入：隐马尔可夫模型 $\lambda =(A,B,\pi)$，观测序列长度 $T$ ;</p><p>按照初始状态分布 $\pi$ 产生状态 $i_1$</p><p><strong>for</strong> t = 1, 2, …, T</p><p>​    按照状态 $i<em>t$ 的观测概率分布 $b</em>{i_t}(k)$ 生成 $o_t$</p><p>​    按照状态 $i<em>t$ 的状态转移概率分布 ${a</em>{i<em>ti</em>{t+1}}}$ 产生状态 $i<em>{t+1},\quad i</em>{t+1}=1,2,…N$</p><p><strong>end for</strong></p><p>输出：观测序列 $O=(o_1,o_2,…o_T)$.</p></blockquote><h3 id="三个基本问题"><a href="#三个基本问题" class="headerlink" title="三个基本问题"></a>三个基本问题</h3><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/three_problem.png" alt="三个基本问题"></p><ol><li><strong>概率计算问题</strong>.  给定模型 $\lambda=(A,B,\pi)$ 和观测序列 $O=(o_1,o_2,…o_T)$，计算在模型 $\lambda$ 下观测序列 $O$ 出现的概率 $P(O|\lambda)$ .</li><li><strong>学习问题</strong>. 一直观测序列 $O =(o_1,o_2,…,o_T)$，估计模型 $\lambda=(A,B,\pi)$ 参数，使得在该模型下观测序列概率 $P(O|\lambda)$ 最大. 即用极大似然的方法估计参数.</li><li><strong>预测问题</strong>. 也称为解码(decoding)问题. 已知模型 $\lambda= (A,B,\pi)$ 和观测序列 $O=(o_1,o_2,…,o_T)$，求对给定观测序列条件概率 $P(I|O)$ 最大的状态序列 $I=(i_1,i_2,…i_T)$.  即给定观测序列，求最有可能的对应的状态序列.</li></ol><h2 id="概率计算方法"><a href="#概率计算方法" class="headerlink" title="概率计算方法"></a>概率计算方法</h2><h3 id="前向算法-forward-algorithm"><a href="#前向算法-forward-algorithm" class="headerlink" title="前向算法(forward algorithm)"></a>前向算法(forward algorithm)</h3><h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p> 给定隐马尔可夫模型 $\lambda$ ，定义到时刻 $t$ 部分观测序列为 $o_1,o_2,…,o_t$ 且状态为 $q_i$ 的概率为前向概率，记作：</p><script type="math/tex; mode=display">\alpha_t(i) = P(o_1,o_2,...,o_t,\quad i_t=q_i|\lambda)    \tag{8}</script><h4 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h4><blockquote><p>输入：隐马尔可夫模型 $\lambda$，观测序列 $O$</p><p>初始化 $\alpha_1(i)= \pi_ib_i(o_1),\quad i=1,2,…N \tag{9}$</p><p><strong>for</strong> t =1, 2, …, T-1</p><script type="math/tex; mode=display">\alpha_{t+1}(i) = [\sum^N_{j=1}\alpha_t(j)a_{ji}]b_i(o_{t+1}),\quad i=1,2,...N        \tag{10}</script><p><strong>end for</strong></p><p>计算 $P(O|\lambda)$</p><script type="math/tex; mode=display">P(O|\lambda) = \sum^N_{i=1}\alpha_T(i)    \tag{11}</script><p>输出：观测序列概率 $P(O|\lambda)$</p></blockquote><h3 id="后向算法-backward-algorithm"><a href="#后向算法-backward-algorithm" class="headerlink" title="后向算法(backward algorithm)"></a>后向算法(backward algorithm)</h3><h4 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h4><p> 给定隐马尔可夫模型 $\lambda$，定义在时刻 $t$ 状态为 $q<em>i$ 的条件下，从 $t+1$ 到 $T$ 的部分观测序列为 $o</em>{t+1},o<em>{t+2},…,o</em>{T}$ 为后向概率，记作：</p><script type="math/tex; mode=display">\beta_t(i) = P(o_{t+1},o_{t+2},...,o_T|i_t = q_i,\lambda)    \tag{12}</script><h4 id="算法-1"><a href="#算法-1" class="headerlink" title="算法"></a>算法</h4><blockquote><p>输入：隐马尔可夫模型 $\lambda$，观测序列 $O$</p><p>初始化 $\beta_T(1) =1 ,\quad i=1,2,…N \tag{13}$</p><p><strong>for</strong> t = T-1, T-2, …, 1</p><script type="math/tex; mode=display">\beta_t(i) = \sum^N_{j=1}a_{ij}b_j(o_{t+1})\beta_{t+1}(j) \tag{14}</script><p><strong>end for</strong></p><p>计算 $P(O|\lambda)$</p><script type="math/tex; mode=display">P(O|\lambda) = \sum^N_{i=1}\pi_ib_i(o_1)\beta_1(i)    \tag{15}</script><p>输出：观测序列概率 $P(O|\lambda)$</p></blockquote><h3 id="一些概率和期望值的计算"><a href="#一些概率和期望值的计算" class="headerlink" title="一些概率和期望值的计算"></a>一些概率和期望值的计算</h3><ol><li><p>给定模型 $\lambda$ 和观测 $O$，<strong>在时刻 $t$ 处于状态 $q_i$ 的概率</strong>，记</p><script type="math/tex; mode=display">\gamma_t(i) = P(i_t =q_i|O,\lambda)    \tag{16}</script><p>可以通过前向后向概率计算. 事实上，</p><script type="math/tex; mode=display">\gamma_t(i) = P(i_t =q_i|O,\lambda) = \frac{P(i_t=q_i,O|\lambda)}{P(O|\lambda)}</script><p>由前向概率和后向概率的定义可知：</p><script type="math/tex; mode=display">\alpha_t(i) \beta_t(i)= P(i_t=q_i ,O|\lambda)</script><p>于是有</p><script type="math/tex; mode=display">\gamma_t(i) = \frac{\alpha_t(i)\beta_t(i)}{P(O|\lambda)}=\frac{\alpha_t(i)\beta_t(i)}{\sum^N_{j=1}\alpha_t(j)\beta_t(j)}\tag{17}</script></li><li><p>给定模型 $\lambda$ 和观测 $O$，<strong>在时刻 $t$ 处与状态 $q_i$ 且在时刻 $t+1$ 处于状态 $q_j$ 的概率</strong>. 记</p><script type="math/tex; mode=display">\xi_t(i,j) = P(i_t=q_i,\ i_{t+1}=q_j|O,\lambda)    \tag{18}</script><p>可以通过前向后向概率计算</p><script type="math/tex; mode=display">\xi_t(i,j) = \frac{P(i_t=q_i,\ i_{t+1}=q_j,O,|\lambda)}{P(O|\lambda)} = \frac{P(i_t=q_i,\ i_{t+1}=q_j,O,|\lambda)}{\sum^N_{i=1}\sum^N_{j=1}P(i_t=q_i,\ i_{t+1}=q_j,O|\lambda)}</script><p>而</p><script type="math/tex; mode=display">P(i_t=q_i,\ i_{t+1}=q_j,O,|\lambda) = \alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)</script><p>所以</p><script type="math/tex; mode=display">\xi_t(i,j) == \frac{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{\sum^N_{i=1}\sum^N_{j=1}P(i_t=q_i,\ i_{t+1}=q_j,O|\lambda)}    \tag{19}</script></li><li><p>于是将 $\gamma_t(i)$ 和 $\xi_t(i,j) $ 对各个时刻求和，可以得到一些有用的期望值：</p><ul><li><p>在观测 $O$ 下状态 $i$ 出现的期望值</p><script type="math/tex; mode=display">\sum^T_{t+1}\gamma_t(i)    \tag{20}</script></li><li><p>在观测 $O$ 下由状态 $i$ 转移的期望值</p><script type="math/tex; mode=display">\sum^{T-1}_{t=1} \gamma_t(i)    \tag{21}</script></li><li><p>在观测 $O$ 下有状态 $i$ 转移到 状态 $j$ 的期望值</p><script type="math/tex; mode=display">\sum_{t=1}^{T-1}\xi_t(i,j)    \tag{22}</script></li></ul></li></ol><h2 id="学习方法"><a href="#学习方法" class="headerlink" title="学习方法"></a>学习方法</h2><p>  这里我们只关注非监督的学习算法，有监督的学习算法在有标注数据的情况下，可以用 <strong>极大似然估计</strong> 很方便地估计参数.  </p><p>   对于非监督的情况，我们可以<strong>将状态变量视作隐变量</strong>，那么隐马尔可夫模型实际上上一个含有隐变量的概率模型，</p><script type="math/tex; mode=display">P(O|\lambda) = \sum_IP(O|I,\lambda)P(I|\lambda)    \tag{23}</script><p>使用<strong>EM算法</strong>估计参数，这个算法由Baum和Weich发明，所以被称作<strong>Baum-Weich 算法</strong>.</p><h3 id="Baum-Weich-算法"><a href="#Baum-Weich-算法" class="headerlink" title="Baum-Weich 算法"></a>Baum-Weich 算法</h3><ol><li><p>确定完全数据的对数似然函数</p><p>所有观测数据写成 $O=(o_1,o_2,…,o_T)$，所有的隐数据写成 $I=(i_1,i_2,…i_T)$，完全数据是 $(O,I)=(o_1,o_2,…,o_T,i_1,i_2,…,i_T)$. 完全数据的<strong>对数似然函数</strong>是 $log P(O,I|\lambda)$.</p></li><li><p><strong>EM算法的E步：求 $Q$ 函数 $Q(\lambda,\bar{\lambda})$</strong> </p><script type="math/tex; mode=display">Q(\lambda,\bar{\lambda})=\sum_Ilog P(O,I|\lambda)P(O,I|\bar{\lambda})    \tag{24}</script><p>其中，$\bar{\lambda}$ 是隐马尔可夫模型参数的当前估计值，$\lambda$ 是要极大化的隐马尔可夫模型参数.</p><script type="math/tex; mode=display">P(O,I|\lambda) = \pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)...a_{i_{T-1}i_T}b_{i_T}(o_T)    \tag{25}</script><p>于是函数 $Q(\lambda,\bar{|\lambda})$ 可以写成</p><script type="math/tex; mode=display">\begin{align}Q(\lambda,\bar{\lambda}) =& \sum_I\log\pi_{i_1}P(O,I|\bar{\lambda})    \\&+\sum_I(\sum^{T-1}_{t=1}\log a_{i_ti_{t+1}})P(O,I|\bar{\lambda})+ \sum_I(\sum^T_{t=1}\log b_{i_t}(o_t))P(O,I|\bar{\lambda})\end{align}    \tag{26}</script></li><li><p><strong>EM算法的M步：</strong></p><p><strong>对式(26)中的每一项分别应用拉格朗日乘子法</strong>，可得 </p><script type="math/tex; mode=display">\pi_i = \frac{P(O,i_1=i|\bar{\lambda})}{P(O|\bar{\lambda})}    \tag{27}</script><script type="math/tex; mode=display">a_{ij} = \frac{\sum^{T-1}_{t=1}P(O,i_t=i,i_{t+1}=j|\bar{\lambda})}{\sum^{T-1}_{t=1}P(O,i_t=i|\bar{\lambda})}    \tag{28}</script><script type="math/tex; mode=display">b_j(k) = \frac{\sum^T_{t=1}P(O,i_t=i|\bar{\lambda})I(o_t=v_k)}{\sum^T_{t=1}P(O,i_t=j|\bar{\lambda})}    \tag{29}</script><p>对各概率分别用 $\gamma_t(i), \xi_t(i,j)$ 表示，可将相应的公式写成：</p><script type="math/tex; mode=display">a_{ij} =\frac{\sum^{T-1}_{t=1}\xi_t(i,j)}{\sum^{T-1}_{t=1}\gamma_t(i)}    \tag{30}</script><script type="math/tex; mode=display">b_j(k) = \frac{\sum^T_{t=1,o_t=v_k}\gamma_t(j)}{\sum^T_{t=1}\gamma_t(j)}    \tag{31}</script><script type="math/tex; mode=display">\pi_i = \gamma_1(i)    \tag{32}</script></li></ol><h2 id="预测方法"><a href="#预测方法" class="headerlink" title="预测方法"></a>预测方法</h2><p> 隐马尔可夫模型预测主要有两种算法：<strong>近似算法与维比特算法(Viterbi algorithm)</strong>.</p><h3 id="近似算法"><a href="#近似算法" class="headerlink" title="近似算法"></a>近似算法</h3><p> 近似算法的思想是，在每个时刻 $t$ 选择在该时刻最有可能出现的状态 $i^\ast<em>t$，从而得到一个状态序列 $I^\ast=(i^\ast_1,i_2^\ast,…,i^\ast</em>{T})$.</p><p>算法流程如下：</p><blockquote><p>输入：模型 $\lambda=(A,B,\pi)$ 和观测 $O=(o_1,o_2,…,o_T)$;</p><p>计算在时刻 $t$ 处于状态 $q_i$ 的概率 $\gamma_t(i)$</p><script type="math/tex; mode=display">\gamma_t(i) = \frac{\alpha_t(i)\beta_t(i)}{P(O|\lambda)}=\frac{\alpha_t(i)\beta_t(i)}{\sum^N_{j=1}\alpha_t(j)\beta_t(j)}    \tag{33}</script><p>在每一时刻 $t$ 最有可能的状态 $i_t$ 是</p><script type="math/tex; mode=display">i^\ast_t = \arg \max_{1\le i\le N}[\gamma_t(i)], \quad i=1,2,...,T    \tag{34}</script><p>输出：状态序列 $I^\ast=(i^\ast<em>1,i_2^\ast,…,i^\ast</em>{T})$.</p></blockquote><ul><li>优点：计算简单</li><li>缺点：不能保证预测的状态序列整体是最有可能的状态序列</li></ul><h3 id="维特比算法"><a href="#维特比算法" class="headerlink" title="维特比算法"></a>维特比算法</h3><blockquote><p>维比特算法是用动态规划来解决隐马尔可夫模型预测问题，即用<strong>动态规划</strong>(<em>dynamic programming</em>) 求概率最大路径 (最优路径) . 这时<strong>一条路径对于着一个状态序列.</strong></p><p>根据动态规划原理，最优路径具有如下性质：如果最优路径在时刻 $t$ 通过结点 $i_t^\ast$ ，那么这一路径从结点 $i_t^\ast$ 到终点 $i_T^\ast$ 的部分路径，对于从 $i_t^\ast$ 到 $i_T^\ast$ 所有可能的部分路径来说，必须是最优的.  </p></blockquote><p> 依据这一原理，我们只需要从时刻 $t=1$ 开始，递推地计算在时刻 $t$ 状态为 $i$ 的各条部分路径的最大概率，直到得到时刻 $t=T$ 状态为 $i$ 的各条路径的最大概率.</p><p> 首先导入两个变量 $\delta$ 和 $\psi$ . 定义<strong>在时刻 $t$ 状态为 $i$ 的所有单个路径 $(i_1,i_2,…,i_t)$ 中概率最大值</strong>为</p><script type="math/tex; mode=display">\delta_t(i) = \max_{i_1,i_2,...,i_{t-1}}P(i_t=i,i_{t-1},...,i_1,\ o_t,...,o_1|\lambda), \quad i=1,2,...,N \tag{35}</script><p>可得<strong>变量 $\delta$ 的递推公式</strong>：</p><script type="math/tex; mode=display">\begin{align}\delta_t(i) &= \max_{i_1,i_2,...,i_{t}}P(i_{t+1}=i,i_{t},...,i_1,\ o_{t+1},...,o_1|\lambda),     \\&= \max_{1\le j\le N}[\delta_{t-1}(j)a_{ji}]b_i(o_t), \quad i=1,2,...,N; t=1,2,...,T-1    \\\end{align}    \tag{36}</script><p> 定义在时刻 <strong>$t$ 状态为 $i$ 的所有单个路径 $(i_1,i_2,…,i_t)$ 中概率最大的路径的第 $t-1$ 个结点</strong>为：</p><script type="math/tex; mode=display">\psi_t(i) = \arg \max_{1 \le j \le N} [\delta_{t-1}(j)a_{ji}], \quad i=1,2,...,N</script><p>下面是<strong>算法流程</strong>：</p><blockquote><p>输入：模型 $\lambda=(A,B,\pi)$ 和观测 $O=(o_1,o_2,…,o_T)$;</p><p>初始化 </p><script type="math/tex; mode=display">\delta_1(i) = \pi_ib_i(o_1), \quad i=1,2,...N    \\\psi_1(i) = 0 , \quad i=1,2,...,N</script><p><strong>for</strong> t = 2, 3, …, T</p><script type="math/tex; mode=display">\delta_t(i) = \max_{1\le j\le N}[\delta_{t-1}(j)a_{ji}]b_i(o_t), \quad i=1,2,...,N    \\\psi_t(i) = \arg \max_{1 \le j \le N} [\delta_{t-1}(j)a_{ji}], \quad i=1,2,...,N</script><p><strong>end for</strong></p><p>终止</p><script type="math/tex; mode=display">P^* = \max_{1\le i \le N} \delta_T(i)    \\i_T^* = \arg\max_{1\le i\le N}[\delta_T(i)]</script><p>最优路径回溯</p><p><strong>for</strong> t = T-1, T-2, …, 1</p><script type="math/tex; mode=display">i_t^* = \psi_{t+1}(i^*_{t+1})</script><p><strong>end for</strong></p><p>输出：最优路径 $I^\ast=(i^\ast<em>1,i_2^\ast,…,i^\ast</em>{T})$.</p></blockquote><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:UTF-8</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HMM</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    隐马尔可夫模型</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, A, B, pi)</span>:</span></span><br><span class="line">        self.A = A  <span class="comment"># 状态转移概率矩阵(N*N)</span></span><br><span class="line">        self.B = B  <span class="comment"># 观测概率矩阵(N*M)</span></span><br><span class="line">        self.pi = pi  <span class="comment"># 初始概率向量(N*1)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_forward</span><span class="params">(hmm, obs)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    前向算法</span></span><br><span class="line"><span class="string">    :param hmm: 隐马尔可夫模型</span></span><br><span class="line"><span class="string">    :param obs: 观测序列</span></span><br><span class="line"><span class="string">    :return: alpha: 前向概率矩阵 (N*T)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    N = hmm.A.shape[<span class="number">0</span>]</span><br><span class="line">    T = len(obs)</span><br><span class="line"></span><br><span class="line">    alpha = np.zeros((N, T))</span><br><span class="line">    alpha[:, <span class="number">0</span>] = hmm.pi * hmm.B[:, <span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, T):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">            alpha[i, t] = np.dot(alpha[:, t<span class="number">-1</span>], hmm.A[:, i]) * hmm.B[i, obs[t]]</span><br><span class="line">    <span class="comment"># print("alpha")</span></span><br><span class="line">    <span class="comment"># print(alpha)</span></span><br><span class="line">    <span class="keyword">return</span> alpha</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_backward</span><span class="params">(hmm, obs)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    后向算法</span></span><br><span class="line"><span class="string">    :param hmm: 隐马尔可夫模型</span></span><br><span class="line"><span class="string">    :param obs: 观测序列</span></span><br><span class="line"><span class="string">    :return: beta: 后向概率矩阵 (N*T)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    N = hmm.A.shape[<span class="number">0</span>]</span><br><span class="line">    T = len(obs)</span><br><span class="line"></span><br><span class="line">    beta = np.zeros((N, T))</span><br><span class="line">    beta[:, <span class="number">-1</span>:] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T<span class="number">-1</span>)):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">            beta[i, t] = np.sum(beta[:, t+<span class="number">1</span>] * hmm.A[i, :] * hmm.B[:, obs[t+<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> beta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hmm_training</span><span class="params">(hmm, obs)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练HMM</span></span><br><span class="line"><span class="string">    :param hmm: 隐马尔可夫模型</span></span><br><span class="line"><span class="string">    :param obs: 观测矩阵</span></span><br><span class="line"><span class="string">    :return: 0</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    N = hmm.A.shape[<span class="number">0</span>]</span><br><span class="line">    T = len(obs)</span><br><span class="line"></span><br><span class="line">    alpha = cal_forward(hmm, obs)</span><br><span class="line">    beta = cal_backward(hmm, obs)</span><br><span class="line"></span><br><span class="line">    gamma = np.zeros((N, T))</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T):</span><br><span class="line">        prob = np.sum(alpha[:, t]*beta[:, t])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">            gamma[i, t] = alpha[i, t] * beta[i, t] / prob</span><br><span class="line"></span><br><span class="line">    xi = np.zeros((N, N, T))</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(T<span class="number">-1</span>):</span><br><span class="line">        denominator = np.dot(np.dot(alpha[:, t].T, hmm.A) * hmm.B[:, obs[t + <span class="number">1</span>]].T, beta[:, t + <span class="number">1</span>])</span><br><span class="line">        <span class="comment"># print(denominator)</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">            numerator = alpha[i, t] * hmm.A[i, :] * hmm.B[:, obs[t + <span class="number">1</span>]].T * beta[:, t + <span class="number">1</span>].T</span><br><span class="line">            <span class="comment"># print(numerator)</span></span><br><span class="line">            xi[i, :, t] = numerator / denominator</span><br><span class="line"></span><br><span class="line">    <span class="comment"># print(xi[:, :, :-1])</span></span><br><span class="line">    hmm.pi = gamma[:, <span class="number">0</span>]</span><br><span class="line">    hmm.A = np.sum(xi, axis=<span class="number">2</span>) / np.sum(gamma[:, :<span class="number">-1</span>], axis=<span class="number">1</span>).reshape((<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    sum_gamma = np.sum(gamma, axis=<span class="number">1</span>)</span><br><span class="line">    M = hmm.B.shape[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(M):</span><br><span class="line">        mask = obs == k</span><br><span class="line">        hmm.B[:, k] = np.sum(gamma[:, mask], axis=<span class="number">1</span>) / sum_gamma</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">viterbi</span><span class="params">(hmm, obs)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    维特比算法求预测问题</span></span><br><span class="line"><span class="string">    :param hmm: 隐马尔可夫模型</span></span><br><span class="line"><span class="string">    :param obs: 观测序列</span></span><br><span class="line"><span class="string">    :return: p_star: 输出的状态序列的概率</span></span><br><span class="line"><span class="string">             i_star: 输出的状态序列</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    N = np.shape(hmm.B)[<span class="number">0</span>]</span><br><span class="line">    M = np.shape(hmm.B)[<span class="number">1</span>]</span><br><span class="line">    T = len(obs)</span><br><span class="line"></span><br><span class="line">    delta = np.zeros((N, T))</span><br><span class="line">    psi = np.zeros((N, T))</span><br><span class="line"></span><br><span class="line">    delta[:, <span class="number">0</span>] = hmm.pi[:] * hmm.B[:, obs[<span class="number">0</span>]]</span><br><span class="line">    psi[:, <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">1</span>, T):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">            delta[i, t] = np.max(delta[:, t<span class="number">-1</span>] * hmm.A[:, i]) * hmm.B[i, obs[t]]</span><br><span class="line">            max =<span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(N):</span><br><span class="line">                tmp = delta[j, t<span class="number">-1</span>] * hmm.A[j, i]</span><br><span class="line">                <span class="keyword">if</span> tmp &gt; max:</span><br><span class="line">                    max = tmp</span><br><span class="line">                    psi[i, t] = j</span><br><span class="line">    <span class="comment">#             print(delta[j, t-1] * hmm.A[j, i])</span></span><br><span class="line">    <span class="comment">#         print(psi[i, t])</span></span><br><span class="line">    <span class="comment"># print(delta)</span></span><br><span class="line">    <span class="comment"># print(psi)</span></span><br><span class="line">    p_star = np.max(delta[:, T<span class="number">-1</span>])</span><br><span class="line">    <span class="comment"># print(p_star)</span></span><br><span class="line">    i_star = np.zeros((T, <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    i_star[T<span class="number">-1</span>] = np.argmax(delta[:, T<span class="number">-1</span>])</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> reversed(range(T<span class="number">-1</span>)):</span><br><span class="line">        i_star[t] = psi[int(i_star[t+<span class="number">1</span>]), t+<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> p_star, i_star</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(hmm)</span>:</span></span><br><span class="line">    obs = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line">    hmm_training(hmm, obs)</span><br><span class="line">    prob, i = viterbi(hmm, obs)</span><br><span class="line">    print(<span class="string">"--------A:"</span>)</span><br><span class="line">    print(hmm.A)</span><br><span class="line">    print(<span class="string">"--------B:"</span>)</span><br><span class="line">    print(hmm.B)</span><br><span class="line">    print(<span class="string">"--------PI:"</span>)</span><br><span class="line">    print(hmm.pi)</span><br><span class="line">    print(<span class="string">"--------prob:"</span>)</span><br><span class="line">    print(prob)</span><br><span class="line">    print(<span class="string">"--------I:"</span>)</span><br><span class="line">    print(i)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">states = (<span class="string">'Healthy'</span>, <span class="string">'Fever'</span>)</span><br><span class="line"></span><br><span class="line">observations = (<span class="string">'Normal'</span>, <span class="string">'Cold'</span>, <span class="string">'Dizzy'</span>)</span><br><span class="line"><span class="comment"># 初始状态概率向量</span></span><br><span class="line">start_probability = [<span class="number">0.6</span>, <span class="number">0.4</span>]</span><br><span class="line"><span class="comment"># 状态转移概率矩阵</span></span><br><span class="line">transition_probability = [[<span class="number">0.7</span>, <span class="number">0.3</span>], [<span class="number">0.4</span>, <span class="number">0.6</span>]]</span><br><span class="line"><span class="comment"># 观测概率矩阵</span></span><br><span class="line">emission_probability = [[<span class="number">0.5</span>, <span class="number">0.4</span>, <span class="number">0.1</span>], [<span class="number">0.1</span>, <span class="number">0.3</span>, <span class="number">0.6</span>]]</span><br><span class="line">HMM.A = np.array(transition_probability)</span><br><span class="line">HMM.B = np.array(emission_probability)</span><br><span class="line">HMM.pi = np.array(start_probability)</span><br><span class="line"></span><br><span class="line">main(HMM)</span><br></pre></td></tr></table></figure><blockquote><p>参考资料：</p><p>[1] 李航. 统计学习方法[M], 北京: 清华大学出版社, 2012: 171-189.</p><p>[2] 周志华. 机器学习[M], 北京: 清华大学出版社, 2016: 319-322.</p><p>[3] Kevin Chan. 隐马尔科夫模型（HMM）及其Python实现[EB/OL], <a href="https://applenob.github.io/hmm.html" target="_blank" rel="noopener">https://applenob.github.io/hmm.html</a>, 2016-12-15/2018-8-2.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;隐马尔可夫模型&quot;&gt;&lt;a href=&quot;#隐马尔可夫模型&quot; class=&quot;headerlink&quot; title=&quot;隐马尔可夫模型&quot;&gt;&lt;/a&gt;隐马尔可夫模型&lt;/h1&gt;&lt;p&gt; &lt;strong&gt;隐马尔可夫模型&lt;/strong&gt;(&lt;em&gt;Hidden Markov Model，HMM&lt;/em&gt;)，是结构最简单的&lt;strong&gt;动态贝叶斯网&lt;/strong&gt;(&lt;em&gt;dynamic Bayesian network&lt;/em&gt;)，是一种著名的有向图模型，主要用于时序数据建模，在&lt;strong&gt;语音识别&lt;/strong&gt;、&lt;strong&gt;自然语言处理&lt;/strong&gt;等邻域有广泛应用.&lt;/p&gt;
    
    </summary>
    
    
      <category term="概率图模型" scheme="http://yoursite.com/categories/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="生成模型" scheme="http://yoursite.com/tags/%E7%94%9F%E6%88%90%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="概率图模型" scheme="http://yoursite.com/tags/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B/"/>
    
      <category term="隐马尔可分模型" scheme="http://yoursite.com/tags/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%88%86%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>极大似然估计</title>
    <link href="http://yoursite.com/2021/01/06/optimization-theory/MLE/"/>
    <id>http://yoursite.com/2021/01/06/optimization-theory/MLE/</id>
    <published>2021-01-06T15:09:16.958Z</published>
    <updated>2021-01-06T15:09:16.958Z</updated>
    
    <content type="html"><![CDATA[<h1 id="极大似然估计-Maximum-Likelihood-Estimation"><a href="#极大似然估计-Maximum-Likelihood-Estimation" class="headerlink" title="极大似然估计(Maximum Likelihood Estimation )"></a>极大似然估计(<em>Maximum Likelihood Estimation</em> )</h1><h3 id="绪言"><a href="#绪言" class="headerlink" title="绪言"></a>绪言</h3><blockquote><p>​    对概率函数 $P(X|c)$ 来说，由于它涉及关于 $x$ 所有属性的<strong>联合概率</strong>，直接根据样本出现的频率来估计将会遇到严重的困难. 例如，假设样本的 $d $ 个属性都是二值的，则样本空间将有 $2^d$种可能的取值，在现实应用中，这个值往往远大于训练样本数 $m$ ，也就是说，很多样本取值在训练集中根本没有出现，直接使用频率来估计条件概率 $P(x |c) $ 显然不可行，因为“未被观测到”与“出现的概率为零”通常是不同的.</p><p>——  周志华《机器学习》</p></blockquote><p>  我们设关于类别 $c$ 的条件概率为 $P(x |c)$ ，假设 $P(x|c)$ 有确定的形式并且被参数 $\theta$ 唯一确定，那么实际上我们的训练过程就是要利用数据集 $D$训练出较为合适的参数 $\theta$，即<strong>参数估计</strong>(<em>parameter estimation</em>)过程 . 这里，我们把 $P(x|c)$ 记为 $P(x |\theta)$.</p><a id="more"></a><h3 id="似然-likelihood"><a href="#似然-likelihood" class="headerlink" title="似然(likelihood)"></a>似然(<em>likelihood</em>)</h3><blockquote><p>​    在数理统计学中，<strong>似然函数</strong>是一种关于统计模型中的参数的函数，表示模型参数中的<strong>似然性</strong>。似然函数在统计推断中有重大作用，如在最大似然估计和费雪信息之中的应用等等。“似然性”与“或然性”或“概率“意思相近，都是指某种事件发生的可能性，但是在统计学中，“似然性”和“或然性”或“概率”又有明确的区分。概率用于在已知一些参数的情况下，预测接下来的观测所得到的结果，而似然性则是用于在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计。 </p><p>—— 维基百科 <a href="https://zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0" target="_blank" rel="noopener">似然函数</a></p></blockquote><p>  上述是<strong>似然函数</strong>(通常简称<strong>似然</strong>)在<strong>数理统计</strong>中的意义，因此<strong>极大似然估计</strong>即是利用观测结果，对参数进行估计的一种方法。</p><h3 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h3><blockquote><p>​    对于参数估计，统计学界的两个学派分别提供了不同的解决方案：频率主义学派(Frequentist) 认为参数虽然未知，但是却是客观存在的固定值，因此，可通过优化似然函数等准则来确定参数值；贝叶斯学派(Bayesian)则认为参数是未观察到的随机变量，其本身也可有分布，因此可以假定参数服从一个先验分布，然后基于观测到的数据来计算参数的后验分布. </p><p>——  周志华《机器学习》</p></blockquote><p>  而频率主义学派的<strong>极大似然估计(<em>Maximum Likelihood Estimation</em>，简称 <em>MLE</em>)</strong> 在机器学习的一些算法(<a href="https://uniblog.cn/2018/06/07/logistic-regression/" target="_blank" rel="noopener">逻辑回归</a>、<a href="https://uniblog.cn/2018/07/16/naive-bayes/" target="_blank" rel="noopener">朴素贝叶斯</a>等)的推导上有很重要的作用.</p><p>设 $D_c$ 为 训练集 $D$ 中第 $c$ 类样本所组成的集合，假设它们独立同分布，则参数 $\theta $ 对与数据$D_c$ 的<strong>似然</strong>，是</p><script type="math/tex; mode=display">L(\theta) = P(D_c | \theta) = \prod_{x \in D_c} P(x | \theta)    \tag{1.1}</script><p> 式$(1.1)$ 中的连乘操作易造成<strong>下溢</strong>，通常使用<strong>对数似然</strong>(log-likelihood)</p><script type="math/tex; mode=display">\begin{align*}LL(\theta) &= log P(D_c|\theta) \\&= \sum_{x \in D_c} log P(x|\theta)\end{align*}    \tag{1.2}</script><p>因此，对参数 $\theta$ 进行极大似然估计，就是去寻找能够最大化似然 $P(D_c|\theta)$ 的参数值 $\theta$ . 即在 $\theta$ 的所有可能取值中，找到一个能使数据出现的“可能性”最大的值.</p><blockquote><p>参考资料：<br>[1] 周志华. 机器学习[M]. 北京: 清华大学出版社. 2016: 147-154.</p><p>[2] 维基百科. 似然函数[DB/OL], <a href="https://zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0</a>, 2018-02-27/2018-07-16.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;极大似然估计-Maximum-Likelihood-Estimation&quot;&gt;&lt;a href=&quot;#极大似然估计-Maximum-Likelihood-Estimation&quot; class=&quot;headerlink&quot; title=&quot;极大似然估计(Maximum Likelihood Estimation )&quot;&gt;&lt;/a&gt;极大似然估计(&lt;em&gt;Maximum Likelihood Estimation&lt;/em&gt; )&lt;/h1&gt;&lt;h3 id=&quot;绪言&quot;&gt;&lt;a href=&quot;#绪言&quot; class=&quot;headerlink&quot; title=&quot;绪言&quot;&gt;&lt;/a&gt;绪言&lt;/h3&gt;&lt;blockquote&gt;
&lt;p&gt;​    对概率函数 $P(X|c)$ 来说，由于它涉及关于 $x$ 所有属性的&lt;strong&gt;联合概率&lt;/strong&gt;，直接根据样本出现的频率来估计将会遇到严重的困难. 例如，假设样本的 $d $ 个属性都是二值的，则样本空间将有 $2^d$种可能的取值，在现实应用中，这个值往往远大于训练样本数 $m$ ，也就是说，很多样本取值在训练集中根本没有出现，直接使用频率来估计条件概率 $P(x |c) $ 显然不可行，因为“未被观测到”与“出现的概率为零”通常是不同的.&lt;/p&gt;
&lt;p&gt;——  周志华《机器学习》&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;  我们设关于类别 $c$ 的条件概率为 $P(x |c)$ ，假设 $P(x|c)$ 有确定的形式并且被参数 $\theta$ 唯一确定，那么实际上我们的训练过程就是要利用数据集 $D$训练出较为合适的参数 $\theta$，即&lt;strong&gt;参数估计&lt;/strong&gt;(&lt;em&gt;parameter estimation&lt;/em&gt;)过程 . 这里，我们把 $P(x|c)$ 记为 $P(x |\theta)$.&lt;/p&gt;
    
    </summary>
    
    
      <category term="优化理论" scheme="http://yoursite.com/categories/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="极大似然估计" scheme="http://yoursite.com/tags/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/"/>
    
      <category term="概率论" scheme="http://yoursite.com/tags/%E6%A6%82%E7%8E%87%E8%AE%BA/"/>
    
  </entry>
  
  <entry>
    <title>EM算法</title>
    <link href="http://yoursite.com/2021/01/06/optimization-theory/EM/"/>
    <id>http://yoursite.com/2021/01/06/optimization-theory/EM/</id>
    <published>2021-01-06T15:09:16.958Z</published>
    <updated>2021-01-06T15:09:16.958Z</updated>
    
    <content type="html"><![CDATA[<h1 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h1><p> <strong>期望极大算法</strong>(<em>expectation maximization algorithm</em>)，简称<strong>EM算法</strong>，是一种迭代算法，可以用于含义隐变量(<em>hidden variable</em>) 的概率模型参数的极大似然估计，或极大后验概率估计. </p><a id="more"></a><h2 id="延森不等式"><a href="#延森不等式" class="headerlink" title="延森不等式"></a>延森不等式</h2><p>  在介绍EM算法之前，首先介绍一个重要的不等式——<strong>延森不等式</strong>(<em>Jensen’s Inequality</em>)</p><ul><li>令 $x_1&lt; x_2$ , $f(x)$ 是一个凸函数，且如果它有二阶导数，其二阶导数恒大于等于0，那么有：<script type="math/tex; mode=display">tf(x_1) + (1-t)f(x) \ge f(tx_1+(1-t)x_2)    \quad 0\le t\le1\tag{1}</script><img src="http://pbug3xg5x.bkt.clouddn.com/jensen.png" alt="延森不等式"></li></ul><p>​    如果我们在此条件下，假设 $x$ 为随机变量，则有：</p><script type="math/tex; mode=display">E[f(x)] \ge f[E(x)]    \tag{2}</script><ul><li><p>同理，当 $f(x)$ 为凹函数时，有：</p><script type="math/tex; mode=display">E[f(x)] \le f[E(x)]    \tag{3}</script></li><li><p>进一步，如果函数的二阶导数大于0，那么：</p><script type="math/tex; mode=display">\begin{align}E[f(x)] = f[E(x)]  &\Leftrightarrow x为常量    \\&\Leftrightarrow x = E(x)\end{align}\tag{4}</script></li></ul><h2 id="EM算法-1"><a href="#EM算法-1" class="headerlink" title="EM算法"></a>EM算法</h2><h3 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h3><p> 假设训练集 ${x^{(1)},x^{(2)},…,x^{(m)}}$ 是由$m$个独立的无标记样本构成。我们有这个训练集的概率分布模型 $p(x,z;θ)$ ，但是我们只能观察到 $x$ 。我们需要使参数 $θ$ 的对数似然性最大化，即：</p><script type="math/tex; mode=display">\begin{align}arg \max_\theta l(\theta) &= arg \max_\theta \sum_{i=1}^m \log P(x^{(i)};\theta) \\&= arg \max_\theta \sum^m_{i=1} \log \sum_z P(x^{(i)},z^{(i)};\theta)\end{align}\tag{5}</script><h3 id="形式化过程"><a href="#形式化过程" class="headerlink" title="形式化过程"></a>形式化过程</h3><p> 具体来说，我们需要每次为函数$\log P(x;\theta)$上的点$\theta$，找到一个凹函数 $g(\theta) \le \log P(x;\theta)$， 每次取 $g(\theta)$ 的最大值点为下一个 $\theta$ ，迭代直到目标函数 $logP(x;\theta)$ 达到局部最大值.</p><p> 如下图所示：</p><p><img src="http://pbug3xg5x.bkt.clouddn.com/EM1.png" alt="EM"></p><h3 id="推导"><a href="#推导" class="headerlink" title="推导"></a>推导</h3><p>我们假设每一个 $z^{(i)}$ 的分布函数为 $Q_i$，所以有 $\sum_ZQ_i(z)=1, Q_i(z) \ge0$，有：</p><script type="math/tex; mode=display">\begin{align}l(\theta) &= \sum_i\log\sum_{z^{(i)}}p(x^{(i)},z^{(i)};\theta)    \\&= \sum_i \log \sum_{z^{(i)}}Q_i(z^{(i)})\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}    \\&\ge \sum_i \sum_{z^{(i)}}Q_i(z^{(i)})\log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}\end{align}    \tag{6}</script><p> 我们知道 $\log$ 是一个凹函数，如果把 $Q<em>i(z^{(i)})$ 看作随机变量，$\sum</em>{z^{(i)}}Q_i(z^{(i)}) \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$看作随机变量的概率分布函数 $\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}$ 的期望，则由延森不等式(3)可得到上述不等式.</p><p> 这样以来，$\theta$ 的对数似然函数 $l(\theta)$ 便有了一个下界，但我们希望得到一个更加紧密的下界，也就是使等号成立的情况. 那么有：</p><script type="math/tex; mode=display">\frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})} = c \quad(c为常数)    \tag{7}</script><p>则：</p><script type="math/tex; mode=display">Q_i(z^{(i)}) = c*p(x^{(i)},z^{(i)};\theta)    \tag{8}</script><p>又 $\sum_ZQ_i(z)=1, Q_i(z) \ge0$，所以：</p><script type="math/tex; mode=display">\sum_ZQ_i(z^{(i)})= \sum_Z c*p(x^{(i)},z^{(i)};\theta)=1    \tag{9}</script><p>所以：</p><script type="math/tex; mode=display">c = \frac{1}{\sum_Z p(x^{(i)},z^{(i)};\theta)}    \tag{10}</script><p>那么再由式(8)，有：</p><script type="math/tex; mode=display">\begin{align}Q_i(z^{(i)}) &= \frac{p(x^{(i)},z^{(i)};\theta)}{\sum_Z p(x^{(i)},z^{(i)};\theta)}\\& = \frac{p(x^{(i)},z^{(i)};\theta)}{p(x^{(i)};\theta)}    \\&= p(z^{(i)}| x^{(i)};\theta)\end{align}    \tag{11}</script><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><blockquote><p><strong>repeat</strong></p><p>​         <strong>(E step) for each i</strong></p><script type="math/tex; mode=display">Q_i(z^{(i)}) := p(z^{(i)}| x^{(i)};\theta)</script><p>​     <strong>end for</strong></p><p>​         <strong>(M step)</strong></p><script type="math/tex; mode=display">\theta := arg\max_\theta\sum_i \sum_{z^{(i)}}Q_i(z^{(i)})\log \frac{p(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}</script><p><strong>until</strong> 似然函数达到最大值</p><p>参考资料</p><p>[1] danerer-CSDN. Andrew Ng机器学习课程笔记（十三）之无监督学习之EM算法[EB/OL] , <a href="https://blog.csdn.net/danerer/article/details/80282612#expectation-maximization-algorithm" target="_blank" rel="noopener">https://blog.csdn.net/danerer/article/details/80282612#expectation-maximization-algorithm</a>, 2018-5-11/2018-7-30.</p><p>[2] 维基百科. Jensen’s inequality[DB/OL], <a href="https://en.wikipedia.org/wiki/Jensen%27s_inequality" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Jensen%27s_inequality</a>, 2018-06-06/2018-7-30.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;EM算法&quot;&gt;&lt;a href=&quot;#EM算法&quot; class=&quot;headerlink&quot; title=&quot;EM算法&quot;&gt;&lt;/a&gt;EM算法&lt;/h1&gt;&lt;p&gt; &lt;strong&gt;期望极大算法&lt;/strong&gt;(&lt;em&gt;expectation maximization algorithm&lt;/em&gt;)，简称&lt;strong&gt;EM算法&lt;/strong&gt;，是一种迭代算法，可以用于含义隐变量(&lt;em&gt;hidden variable&lt;/em&gt;) 的概率模型参数的极大似然估计，或极大后验概率估计. &lt;/p&gt;
    
    </summary>
    
    
      <category term="优化理论" scheme="http://yoursite.com/categories/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="EM算法" scheme="http://yoursite.com/tags/EM%E7%AE%97%E6%B3%95/"/>
    
      <category term="无监督学习" scheme="http://yoursite.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>约束优化方法</title>
    <link href="http://yoursite.com/2021/01/06/optimization-theory/CO/"/>
    <id>http://yoursite.com/2021/01/06/optimization-theory/CO/</id>
    <published>2021-01-06T15:09:16.957Z</published>
    <updated>2021-01-06T15:09:16.958Z</updated>
    
    <content type="html"><![CDATA[<h1 id="约束优化方法-Constrained-Optimization"><a href="#约束优化方法-Constrained-Optimization" class="headerlink" title="约束优化方法(Constrained Optimization )"></a>约束优化方法(Constrained Optimization )</h1><p> 在优化问题中，往往会有约束条件，即我们希望在约束条件下求得优化问题的最优解。对于等式约束的优化问题，我们可以直接应用拉格朗日乘子法去求取最优值；对于含有不等式约束的优化问题，则可以转化为在满足 KKT 约束条件下应用拉格朗日乘子法求解。 </p><a id="more"></a><h2 id="约束优化"><a href="#约束优化" class="headerlink" title="约束优化"></a>约束优化</h2><h3 id="无约束优化"><a href="#无约束优化" class="headerlink" title="无约束优化"></a>无约束优化</h3><p> 我们首先来看一个无约束的优化问题：</p><script type="math/tex; mode=display">\min_x f(x)    \tag{1}</script><p>其中，$x\in R^N$，$f(x)$是凸函数，这种问题求解很简单，只需要找到令 $\nabla_xf(x) =0$ 的 $x$ 点即可.</p><h3 id="等式约束"><a href="#等式约束" class="headerlink" title="等式约束"></a>等式约束</h3><p>下面，我们给优化问题加上一个等式约束：</p><script type="math/tex; mode=display">\begin{align}&\min_x f(x)    \\&s.t \quad h_i(x) = 0 , \quad i=1,2,...,m\end{align}    \tag{2}</script><p>其中，$x$ 为 $n$ 维向量. 为了直观地看这个问题，我们下面假设这是一个二维的等式约束优化问题，如下图所示：</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/co1.png" alt="等式约束"></p><p> 图中虚线是 $f(x,y)$ 的等值线，而约束条件 $h(x,y)=0$ 是约束条件. 我们知道 $f(x,y)$ 的最优化问题需要在 $h(x,y) = 0 $ 的条件下求解，即图中虚线要与绿线相交或相切. 而显然我们可以看到，当虚线与绿线相切时，在约束条件 $h(x,y) =0$ 下， $f(x,y)$ 取得最小值.</p><p> 这时，我们看到，两函数的法向量平行的，即有：</p><script type="math/tex; mode=display">\nabla_{x,y} f(x,y) + \alpha\nabla_{x,y}h(x,y) = 0    \\    \tag{3}</script><p>拓展到 $n$ 维有：</p><script type="math/tex; mode=display">\nabla_{x} f(x) + \alpha\nabla_{x}h(x) = 0    \\    \tag{4}</script><p>又有 $h_{i}(x) = 0$，联立即得此优化问题的最优解.</p><p>于是，我们可以令原函数：</p><script type="math/tex; mode=display">L(x,\alpha) =  f(x) + \sum^m_{i=1}\alpha_i h_i(x)    \tag{5}</script><p>即问题转化为了求函数 $L(x)$ 的最优解. 即：</p><script type="math/tex; mode=display">\begin{align}\nabla_xL(x,\alpha) = 0    \\\nabla_\alpha L(x,\alpha) = 0\end{align}    \tag{6}</script><p>求得 $x$ 和 $\alpha$ 后，将 $x$ 代入 $f(x)$ 中即得到可行解，这便是<strong>拉格朗日乘子法</strong>(<em>Lagrangian Multiplier Method</em>)，其中 $\alpha=\alpha_1^T,\alpha_2^T,…,\alpha_m^T$ 称为拉格朗日乘子.</p><h3 id="不等式约束"><a href="#不等式约束" class="headerlink" title="不等式约束"></a>不等式约束</h3><p>那么，如果加入不等式约束：</p><script type="math/tex; mode=display">\begin{align}&\min_x f(x)    \\&s.t \quad g_i(x) \le 0 , \quad i=1,2,...,m\end{align}    \tag{7}</script><p>同样的，我们把问题简化为二维：</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/co2.png" alt="不等式约束"></p><p>显然，可行解有可能落在 $g(x) \le 0$ 里面或者是边界上，即如下两种情况：</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/CO3.png" alt="两种情况"></p><p>可见：</p><ul><li>当可行解 $x$ 落在 $0&lt;g(x)&lt;0$ 的区域内，此时直接极小化 $f(x)$ 即可；</li><li>当可行解 $x$ 落在 $g(x)=0$ 即边界上，此时等价于等式约束优化问题.</li></ul><p>于是，我们可以得到：</p><script type="math/tex; mode=display">\beta g(x) = 0    \tag{8}</script><p>即当 $\beta =0$ 时，此时约束不起作用； $\beta \neq 0$ 时，等价于等式约束.</p><p>另外，在等式约束中，我们并没有对乘子加以约束，但在不等式约束中，有：</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/CO4.png" alt="梯度方向"></p><p> 从图中可以看到，若要最小化 $f(x)$，约束域 $g(x) \le 0$ 的法向量应当与 $f(x)$ 的负梯度同向，即：</p><script type="math/tex; mode=display">\begin{align}-\nabla_x f(x) &= \beta \nabla_x g(x)    \\\beta &\ge 0\end{align}    \tag{9}</script><p>于是，与等式约束优化问题类似，不等式约束的优化问题，在满足一定条件下，便可以用拉格朗日乘子法解决，此条件即为<strong>KKT条件</strong>(<em>Karush-Kuhn-Tucker conditions</em>).</p><p>  我们给出形式化的约束优化问题：</p><script type="math/tex; mode=display">\begin{align}\min_x &f(x)    \\s.t. \ &h_i(x) = 0, \qquad i=1,2,...,m\\    &g_j(x) \le 0, \qquad j=1,2,...,k\\\end{align}    \tag{10}</script><p>写出拉格朗日函数：</p><script type="math/tex; mode=display">L(x,\alpha,\beta) = f(x) + \sum^m_{i=1}\alpha_i h_i(x) + \sum^k_{j=1}\beta g_i(x)    \tag{11}</script><p>综合上面的分析，我们有以下的<strong>KKT条件</strong>：</p><script type="math/tex; mode=display">\begin{align}\nabla_x L(x,\alpha,\beta) = 0    \\\beta_j g_j(x) =0 \\h_i(x) = 0    \\g_j(x) \le 0  \\\beta_j \ge0\end{align}    \tag{12}</script><p><strong>满足 KKT 条件后极小化拉格朗日函数，即可得到在不等式约束条件下的可行解。</strong>  </p><h2 id="对偶问题"><a href="#对偶问题" class="headerlink" title="对偶问题"></a>对偶问题</h2><p> 在优化理论中，对目标函数 $f(x)$ 的在约束条件下的最优化问题，可以转化为一个与之对应的<strong>对偶问题</strong>(<em>dual problem</em>)，而原来的问题称为<strong>原始问题</strong>(<em>primal problem</em>)，而对偶问题有如下几个良好的性质：</p><ul><li>对偶问题的对偶是原始问题；</li><li><strong>无论原始问题是否是凸的，对偶问题都是凸优化问题</strong>；</li><li>对偶问题可以给出原始问题一个下界；</li><li>当满足一定条件时，原始问题与对偶问题的解是完全等价的.</li></ul><h3 id="原始问题"><a href="#原始问题" class="headerlink" title="原始问题"></a>原始问题</h3><p>我们给定不等式约束优化问题：</p><script type="math/tex; mode=display">\begin{align}\min_x &f(x)    \\s.t. \ &h_i(x) = 0, \qquad i=1,2,...,m\\    &g_j(x) \le 0, \qquad j=1,2,...,k\\\end{align}</script><p>定义一个拉格朗日函数：</p><script type="math/tex; mode=display">L(x,\alpha,\beta) = f(x) + \sum^m_{i=1}\alpha_i h_i(x) + \sum^k_{j=1}\beta_j g_j(x)</script><p>我们知道 $h_i(x) =0$，且 $\beta_j \ge0 ,g_j(x)\le0$ 即 $\beta_j g_i(x) \le0$，所以有：</p><script type="math/tex; mode=display">f(x) = \max_{\alpha,\beta;\beta\ge0} L(x,\alpha,\beta) > L(x,\alpha,\beta)    \tag{13}</script><p>于是我们的优化问题转化为：</p><script type="math/tex; mode=display">\min_x f(x) = \min_x \max_{\alpha,\beta;\beta\ge0} L(x,\alpha,\beta)    \tag{14}</script><p>这便是我们的原始问题.</p><h3 id="对偶问题-1"><a href="#对偶问题-1" class="headerlink" title="对偶问题"></a>对偶问题</h3><p> 我们将原始问题的解记为 $p^{\ast}$，这里定义一个<strong>对偶函数</strong>：</p><script type="math/tex; mode=display">D(\alpha,\beta) = \min_xL(x,\alpha,\beta)    \tag{15}</script><p>然后我们定义对偶问题：</p><script type="math/tex; mode=display">\max_{\alpha,\beta;\beta\ge0} \min_x L(x,\alpha,\beta)    \tag{16}</script><p>接着我们定义对偶问题的最优解为</p><script type="math/tex; mode=display">d^{*} =\max_{\alpha,\beta;\beta\ge0}D(\alpha,\beta)    \tag{17}</script><p>易有：</p><script type="math/tex; mode=display">d^* \le p^*    \tag{18}</script><p> 我们把这个性质叫做<strong>弱对偶性</strong>(<em>weak duality</em>)，对所有优化问题都成立，即使原始问题非凸. 之前我们提到过，无论原始问题是什么形式，对偶问题总是凸的，所以由弱对偶性，我们通过求解对偶问题可以得到原始问题的一个下界.</p><p> 与弱对偶性相对的有一个<strong>强对偶性</strong>(<em>strong duality</em>)，即</p><script type="math/tex; mode=display">d^* = p^*        \tag{19}</script><p>于是，满足具有强对偶性的问题，我们可以通过求解对偶问题得到原始问题的解. 但我们需要一些条件，使得强对偶性成立，比如<strong>Slater条件和KKT条件</strong>.</p><ul><li><p><strong>Slater条件</strong>：存在x，使得不等式约束 $g(x)\le0$ 严格成立，即 $g(x) &lt;0$.</p><p> 如果<strong>原始问题是凸优化问题并且满足 Slater 条件的话，那么强对偶性成立</strong>。需要注意的是，这里只是指出了强对偶成立的一种情况，并不是唯一的情况。例如，对于某些非凸优化的问题，强对偶也成立. </p><p>(SVM 中的原始问题 是一个凸优化问题(二次规划也属于凸优化问题), Slater 条件在 SVM 中指的是存在一个超平面可将数据分隔开 ).</p></li></ul><ul><li><p><strong>但为了保证 $d^{_*}$ 是最优解, 还需要满足KKT条件</strong>：</p><p>我们假设 $x^{\ast}$ 与 $\alpha^{\ast},\beta^{\ast}$ 分别是原始问题（并不一定是凸的）和对偶问题的最优解，且满足强对偶性，则相应的极值的关系满足： </p><script type="math/tex; mode=display">\begin{aligned}  f(x^*) &= d^* = p^* =D(\alpha^*,\beta^*)  \\  &=\min_x f(x)+ \sum_{i = 1}^m \alpha_i^*h_i(x) + \sum_{j=1}^n\beta_j^*g_j(x) \\  & \le f(x^*)+ \sum_{i = 1}^m \alpha_i^*h_i(x^*) + \sum_{j=1}^n\beta_j^*g_j(x^*) \\ &\le f(x^*)  \end{aligned}    \tag{20}</script><p>看到第一个不等式，我们知道 $h_i(x)=0$ , 可以得到：</p><script type="math/tex; mode=display">\sum_{j=1}^n\beta_j^*g_j(x^*) \ge 0    \tag{21}</script><p>又有：$ \sum_{j=1}^n\beta_j^\ast g_j(x^\ast) \le 0 $ , 所以</p><script type="math/tex; mode=display">\sum_{j=1}^n\beta_j^*g_j(x^*) = 0     \tag{22}</script><p>由因为 $x^{\ast}$ 是 $f(x)$ 的极小值点，所以</p><script type="math/tex; mode=display">\nabla_x L(x,\alpha,\beta) = 0    \tag{23}</script><p>结合上述和一开始的约束条件，有：</p><script type="math/tex; mode=display">\begin{align}\nabla_x L(x,\alpha,\beta) = 0    \\\beta_j g_j(x) =0 \\h_i(x) = 0    \\g_j(x) \le 0  \\\beta_j \ge0\end{align}    \tag{24}</script><p>即<strong>KKT条件</strong>. </p></li></ul><blockquote><p>参考资料：</p><p>[1] ooon-博客园. 约束优化方法之拉格朗日乘子法与KKT条件[EB/OL]. <a href="http://www.cnblogs.com/ooon/p/5721119.html" target="_blank" rel="noopener">http://www.cnblogs.com/ooon/p/5721119.html</a> , 2016-07-30/2018-7-30. </p><p>[2] ooon-博客园. 拉格朗日对偶[EB/OL]. <a href="http://www.cnblogs.com/ooon/p/5723725.html" target="_blank" rel="noopener">http://www.cnblogs.com/ooon/p/5723725.html</a> , 2016-07-31/2018-7-30.</p><p>[3] feilong_csdn-CSDN. 支持向量机（SVM）必备知识(KKT、slater、对偶）[EB/OL]. <a href="https://blog.csdn.net/feilong_csdn/article/details/62427148" target="_blank" rel="noopener">https://blog.csdn.net/feilong_csdn/article/details/62427148</a>, 2017-03-16/2018-7-30.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;约束优化方法-Constrained-Optimization&quot;&gt;&lt;a href=&quot;#约束优化方法-Constrained-Optimization&quot; class=&quot;headerlink&quot; title=&quot;约束优化方法(Constrained Optimization )&quot;&gt;&lt;/a&gt;约束优化方法(Constrained Optimization )&lt;/h1&gt;&lt;p&gt; 在优化问题中，往往会有约束条件，即我们希望在约束条件下求得优化问题的最优解。对于等式约束的优化问题，我们可以直接应用拉格朗日乘子法去求取最优值；对于含有不等式约束的优化问题，则可以转化为在满足 KKT 约束条件下应用拉格朗日乘子法求解。 &lt;/p&gt;
    
    </summary>
    
    
      <category term="优化理论" scheme="http://yoursite.com/categories/%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="数学" scheme="http://yoursite.com/tags/%E6%95%B0%E5%AD%A6/"/>
    
      <category term="凸优化" scheme="http://yoursite.com/tags/%E5%87%B8%E4%BC%98%E5%8C%96/"/>
    
      <category term="微积分" scheme="http://yoursite.com/tags/%E5%BE%AE%E7%A7%AF%E5%88%86/"/>
    
  </entry>
  
  <entry>
    <title>Word2Vec</title>
    <link href="http://yoursite.com/2021/01/06/natural-language-processing/Word2Vec/"/>
    <id>http://yoursite.com/2021/01/06/natural-language-processing/Word2Vec/</id>
    <published>2021-01-06T15:09:16.957Z</published>
    <updated>2021-01-06T15:09:16.957Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><p>&emsp; Word2Vec是一种以<strong>无监督</strong>的方式来得到词向量的模型，其核心思想是通过上下文信息表达一个词语，将词语从原先的空间映射到新的空间中(本质上是一种降维)。这样得到的词向量相对于<strong>独热编码</strong>具有维度较低、词语之间相似度易衡量等优点.</p><p>&emsp; 具体地，Word2Vec首先将词语进行独热编码，然后将编码后的词向量输入神经网络，通过最小化误差来更新权值矩阵，最后将训练好的权值矩阵作为处理后的词向量矩阵. </p><a id="more"></a><p>&emsp; Word2Vec主要有CBOW、Skip-gram两种模型.  CBOW是根据上下文词语预测当前词；而Skip-Gram正好相反，根据当前词预测上下文. 如下图所示:   </p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/word2vec1.jpg" alt="CBOW and Skip-Gram"></p><p>对于CBOW，我们的目标函数为：</p><script type="math/tex; mode=display">L = \sum_{w\in C} \log p(w|context(w))</script><p>而对于Skip-gram，目标函数为：</p><script type="math/tex; mode=display">L = \sum_{w\in C}\log p(context(w)|w)</script><p>&emsp; Word2Vec有两个trick，分别是Hierarchical Softmax 和 Nagative Sampling，他们是Word2Vec提高训练效率和效果的两种技巧(trick).</p><h2 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h2><h2 id="Hierarchical-Softmax"><a href="#Hierarchical-Softmax" class="headerlink" title="Hierarchical Softmax"></a>Hierarchical Softmax</h2><h2 id="Nagative-Sampling"><a href="#Nagative-Sampling" class="headerlink" title="Nagative Sampling"></a>Nagative Sampling</h2>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Word2Vec&quot;&gt;&lt;a href=&quot;#Word2Vec&quot; class=&quot;headerlink&quot; title=&quot;Word2Vec&quot;&gt;&lt;/a&gt;Word2Vec&lt;/h1&gt;&lt;p&gt;&amp;emsp; Word2Vec是一种以&lt;strong&gt;无监督&lt;/strong&gt;的方式来得到词向量的模型，其核心思想是通过上下文信息表达一个词语，将词语从原先的空间映射到新的空间中(本质上是一种降维)。这样得到的词向量相对于&lt;strong&gt;独热编码&lt;/strong&gt;具有维度较低、词语之间相似度易衡量等优点.&lt;/p&gt;
&lt;p&gt;&amp;emsp; 具体地，Word2Vec首先将词语进行独热编码，然后将编码后的词向量输入神经网络，通过最小化误差来更新权值矩阵，最后将训练好的权值矩阵作为处理后的词向量矩阵. &lt;/p&gt;
    
    </summary>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="word2vec" scheme="http://yoursite.com/tags/word2vec/"/>
    
      <category term="词向量" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
    
  </entry>
  
  <entry>
    <title>NLP——分词</title>
    <link href="http://yoursite.com/2021/01/06/natural-language-processing/Word-Segmentation/"/>
    <id>http://yoursite.com/2021/01/06/natural-language-processing/Word-Segmentation/</id>
    <published>2021-01-06T15:09:16.956Z</published>
    <updated>2021-01-06T15:09:16.957Z</updated>
    
    <content type="html"><![CDATA[<h1 id="NLP——分词"><a href="#NLP——分词" class="headerlink" title="NLP——分词"></a>NLP——分词</h1><p><strong>分词</strong>，即给定一份词之间无间隔符的语料，将词与词切分开。分词是中文NLP任务里的基础性任务，绝大部分任务都需要先分词，再将词的特征输入模型中进行训练和预测（现在也有基于字的模型，但分词仍是中文NLP任务中的重点）。</p><p>一般来说，中文的分词主要面临着三大问题：</p><ul><li><p><strong>分词规范问题</strong>，即“什么是词，怎么界定词”的问题，同样一份语料，不同的人可能会给出截然不同的划分结果，那么究竟哪种是对的呢。即便我国很早就制定了《分词规范》，但大部分的规定都是举例和定性的方式体现，这种规范的操作尺度，极易受主观因素影响。</p></li><li><p><strong>歧义切分问题</strong>，比如“北京大学”，究竟是切分成一个词，还是要切分成“北京”和“大学”呢，虽然我们人类知道前者是对的，但机器是不知道的。</p></li><li><strong>未登录词（unknown word，又称Out Of Vocabulary, OOV）</strong>，即不在词典里的新词，又或者指训练语料中的从未出现的词。由于词典可以由大规模的训练语料获得，所以一般这两者我们可以看作一回事。</li></ul><h2 id="基于词典切分"><a href="#基于词典切分" class="headerlink" title="基于词典切分"></a>基于词典切分</h2><p>基于词典进行切分是最简单粗暴的方式，假设有一份词典，那么给定语料，直接基于词典进行匹配即可，但也存在一些问题。</p><p>假设词典中有：[…，“北京”，“北京大学”，“大学生“，“学生”，“生物”，“系”，…]，给定句子：”北京大学生物系”，我们人类知道，应该要划分成：北京大学/生物/系。</p><p>但机器要如何划分呢？词典中既有”北京“，也有”北京大学“，这就产生了歧义性的问题。</p><p>一种简单的方式是，把所有的可能都切分出来，即切分为：[“北京”，“北京大学”，“大学生“，“学生”，“生物”，“系”]。这便是<strong>全切分方式</strong>。</p><p>事实上，常见的切分方式有：</p><ul><li><p>全切分，即把所有的可能性的分词都列出来</p></li><li><p>最长匹配，往往较长的词会比较短的词更有意义，比如”北京大学“就比”北京”/“大学“要更合适。</p><ul><li><p><strong>正向最长匹配</strong>，给定字串，从前往后匹配最长的子串作为词。使用上述的例句，切分为：[”北京大学“， ”生物“，”系“]</p></li><li><p><strong>逆向最长匹配</strong>，给定字串，从后往前匹配最长的子串作为词。即：[”北京大学”，“生物，“系”]。可以看到，虽然匹配的规则不一样，但结果也是有可能一样的。</p></li><li><p><strong>双向最长匹配</strong>，结合了上述两种规则。流程如下：</p><ol><li>同时执行正向和逆向最长匹配，若两者词数不同，返回词数较少的那一个；</li><li>否则，返回两者中<strong>单字</strong>更少的一个。如果还是相同，那么就返回逆向最长匹配的结果。</li></ol><p>这是一种启发式的算法。语言学的研究发现，汉语中单字词的数量要远远小于非单字词的数量。所以，算法应当尽量地保留更多的完整词语，而减少单字。</p></li></ul></li></ul><p>虽然有了这些匹配的规则，可以对字串进行分词，但显然并没有解决<strong>歧义性和OOV</strong>的问题。因此我们需要利用一定的统计信息，来实现更精准的分词。</p><h2 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h2><h3 id="语言模型-1"><a href="#语言模型-1" class="headerlink" title="语言模型"></a>语言模型</h3><p>​    <strong>语言模型</strong>，是指使用概率语言，去描述语言的模型。简单来说，就是判断一句话”有多少概率是人话“。可以想到最简单的方法，我们看看一句话在所有话中是否出现过，如果是，则概率为1，否则为0。但这样显然是有问题的，我们不可能穷举所有的“人话”，即便可以，从一个巨大的句子库中搜索也是一个巨大的问题，而且新句子除非加入了句子库，否则它的概率永远为0。这种现象被称为<strong>数据稀疏</strong>。</p><blockquote><p>举个例子：”我考上了北京大学“ 和 ”我考上北京大学了“，这两句话只有细微的差别，但意思上是一样的，然而按照刚才的规则，他们是截然不同的两句话。如果第一句话在句子库中，而第二句不在，那么当我们去识别第一句时，会把它的概率计算为1，但识别第二句时，却是0，这显然是不合理的。</p></blockquote><p>​    我们可以考虑另一种方式。词是句子的组成部分，那么我们可以将句子切分为一个个的词，以词的概率去估计句子的概率。这里则又回到了<strong>分词</strong>的问题，我们可以先假定句子里的词已经做好了切分。那么句子的概率则可以简单地表示为如下的形式：</p><script type="math/tex; mode=display">p(s) = p(w_1)*p(w_2)...*p(w_n)</script><p>其中，$p(s)$为句子的概率，$p(w)$为词的概率，即把句子的概率表示为词概率的连乘的形式。词的概率可以由频率估计，比如北京在词库中一共出现了200次，而总次数为20000，那么$p(北京)=200/20000=0.01$。</p><p>​    回到前面的例子，”我考上了北京大学“ 和 ”我考上北京大学了“，如果采用如上的模型，并且分词结果一样，那么概率是一样的。那如果分词结果不一样呢？很显然，不一样的分词，往往计算出来的句子概率是不一样的（因子不一样了）。也就是说：分词-&gt;句子的概率，那么是否可以由句子的概率得到分词呢？显然是可以的，只要<strong>选择概率最大的那个分词结果即可</strong>。</p><p>​    <strong>这便是”基于语言模型分词”的核心思想，选择使得句子概率最大的分词结果。</strong></p><p>​    但目前的语言模型存在着一些问题，其中最大的问题是，无序性。</p><blockquote><p>举个例子，”牛吃草”和”草吃牛”，当前模型是不考虑语序的，因此他们的概率是一样的，但我们从常识上知道，前者是更合理的。</p></blockquote><p>​    为了使得模型考虑语序，我们需要为模型加上一些上下文的信息。以上文为例，我们只需要每次考虑当前词语的”上文词“。</p><script type="math/tex; mode=display">p(s) = p(w_1)*p(w_2|w_1)*p(w_3|w_1,w_2)...*p(w_n|w_1,w_2,...,w_{n-1})</script><p>我们可以使用<strong>极大似然估计(MLE)</strong>来计算这些概率（后验概率）。即：</p><script type="math/tex; mode=display">p(w_t|w_0,...,w_{t-1}) = \frac{c(w_0,...,w_t)}{c(w_0,...,w_{t-1})}</script><p>其中，$c(w_0,…w_t)$ 表示词$w_0,…,w_t$ 的共现次数。（这些词一起出现的次数）</p><p>然而，这并没有很好地解决数据稀疏的问题，随着句子长度的增加，语料库中很可能统计不到该句子的频次，导致其中$p(w<em>t|w_0,…,w</em>{t-1})$为0，进而相乘导致整句话的概率为0。</p><h3 id="n-元语法语言模型"><a href="#n-元语法语言模型" class="headerlink" title="n 元语法语言模型"></a>n 元语法语言模型</h3><p>​    为了解决这个问题，我们需要简化模型，即每次不考虑太长的信息。比如我们<strong>只考虑当前词的前一个词</strong>，则：</p><script type="math/tex; mode=display">p(s) = p(w_0)*p(w_1|w_0)*p(w_2|w_1)...*p(w_{t}|w_{t-1})</script><p>这样一来，就不容易出现句子长度太长导致的数据稀疏问题，并且计算的速度也得到了提升。我们把这种只考虑当前词的前一个词的语言模型称为<strong>二元语法(bi-gram)模型</strong>。</p><p>​    类似的，<strong>我们可以只考虑该单词前n-1个单词，即n元语法(n-gram)</strong>。一般来说，我们不会使用n&gt;3的语法模型，因为往往这样一来数据稀疏的问题又会变得显著起来。特别地，n=1时，称为一元语法(unigram)；n=2时，称为二元语法(bi-gram)；n=3时，称为三元语法(tri-gram)。</p><p>​    事实上，为了进一步缓解数据稀疏的问题，还需要引入数据平滑的策略。常见的方式会使用低阶的（如一元）语法，来平滑高阶语法，以防止计算的概率为0。比如这样：</p><script type="math/tex; mode=display">p(w_t|w_{t-1}) = \lambda p(w_t|w_{t-1}) + (1-\lambda)p(w_t)</script><p>其中，$\lambda $ 为常数平滑因子，一般为0~1。这里的中心思想在于，引入更”稠密”的低阶语法特征，来防止高阶语法的概率为0。</p><p>​    另外，也可以使用取对数的方式，将连乘转化为累加，可以进一步防止某个因子为0而导致整句话的概率为0。这里不在赘述。</p><h2 id="基于序列标注切分"><a href="#基于序列标注切分" class="headerlink" title="基于序列标注切分"></a>基于序列标注切分</h2><p>​    此外，我们可以将分词转化为一个<strong>序列标注</strong>的任务。序列标注即对输入序列进行标注，可以应用于命名实体识别、句法分析、分词等。特别地，在分词任务上，可以给字标注上“B”、“M”、“E”、”S”的标签，分别代表词的开始(begin)、中间(middle)，结束(end)，以及单字(single)。</p><blockquote><p>再以“我考上了北京大学”为例，序列标注可以以如下的形式：</p><p>我/S，考/B，上/E，了/S，北/B，京/M，大/M，学/E</p><p>对应的分词为：我/考上/了/北京大学</p><p>实际上，也并不需要用4个标签，BM两个标签其实就足以将词划分开，只需约定下一个词的B即表示本词已结束即可，如：</p><p>我/B，考/B，上/M，了/B，北/B，京/M，大/M，学/M</p><p>划分出来和上面是一样的结果。</p></blockquote><p>​    可以用来做序列标注任务的模型有不少，最常用的<strong>统计机器学习模型</strong>一般是<strong>隐马尔可夫模型(HMM)</strong>和<strong>条件随机场(CRF)</strong>。这里面内容繁多，可另开一章，这里不再详述。</p><p>​    近年来，深度学习的崛起改变了AI的格局，凭借深度、广度、算力，击败了各种从前基于规则的、基于统计机器学习的方法。可以说，人工智能迎来了深度学习的时代。</p><p>​    自然语言处理界也不例外，深度学习的”end-to-end”特性（即输入-模型-输出），使得各种任务都可以转化成“分类”任务。”输入”一般是做一些预处理，这里不再赘述。可以简单地将”输出”分为输出为一个类别的任务，和输出为一个序列的任务。前者最常见的是文本分类、情感分类；而后者本质上就是多次输出类别，常见的任务则是命名实体识别、分词、翻译、对话等。</p><p>​    “模型”实际上就是广大深度学习的研究者研究的内容，比较常见的模型如卷积神经网络（CNN）、循环神经网络（RNN），以及近年来大热的Transformer，研究者们会根据数据和任务，对这些模型做改进和结合，成为新的模型。这里面的知识可谓是博大精深，并不是小小一篇文章就能概括完的。</p><p>​    同样地，我们可以用深度学习来进行<strong>序列标注</strong>，以实现分词。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;NLP——分词&quot;&gt;&lt;a href=&quot;#NLP——分词&quot; class=&quot;headerlink&quot; title=&quot;NLP——分词&quot;&gt;&lt;/a&gt;NLP——分词&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;分词&lt;/strong&gt;，即给定一份词之间无间隔符的语料，将词与词切分开。分词是中文N
      
    
    </summary>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="分词" scheme="http://yoursite.com/tags/%E5%88%86%E8%AF%8D/"/>
    
  </entry>
  
  <entry>
    <title>Byte Pair Encoding</title>
    <link href="http://yoursite.com/2021/01/06/natural-language-processing/Byte-Pair-Encoding/"/>
    <id>http://yoursite.com/2021/01/06/natural-language-processing/Byte-Pair-Encoding/</id>
    <published>2021-01-06T15:09:16.956Z</published>
    <updated>2021-01-06T15:09:16.956Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Byte-Pair-Encoding"><a href="#Byte-Pair-Encoding" class="headerlink" title="Byte Pair Encoding"></a>Byte Pair Encoding</h1><p>​    <strong>Byte Pari Encoding（BPE）</strong>）是一种简单的数据压缩技巧，最初在1994年被提出，而如今广泛应用于各种现代的NLP模型（如BERT、GPT-2、XLM…）。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>​    如今NLP的发展，由刚开始的基于频率的稀疏词向量（如词袋、N-gram），到前几年通过预训练（word2vec，glove）生成语义表示的稠密词向量，再到划时代意义的BERT，NLP界越来越重视通过预训练学习语义，从而得到好的词向量表示。</p><p>​    然而这其中有个问题需要解决，基本上所有的NLP任务都需要构造一个词表，往往是选取出现频率最高的N个词加入词表。这其中有什么问题呢，举个例子，BERT的预训练语料是维基百科，如此庞大的语料库，词量肯定也是惊人的高。如果我们构造的词表太小，很多频率较低的词语就会被表示为一个掩码（又或是被去除），在模型中就根本“看不到”，这不仅会让这些低频词无法被学习，还会影响上下文中的高频词的学习。但如果词表过大，就会带来效率问题。</p><p>​    BPE就是一种很好的方法，在不扩大词表的情况下，能够让更多的词语“被模型看到”。</p><a id="more"></a><h2 id="原始BPE"><a href="#原始BPE" class="headerlink" title="原始BPE"></a>原始BPE</h2><p>​    BPE最早在1994年由Philip Gage提出（<a href="https://www.drdobbs.com/a-new-algorithm-for-data-compression/184402829" target="_blank" rel="noopener">A New Algorithm for Data Compression</a>），是一种数据压缩的技术，通过使用数据中未出现的字节代替常见的连续字节对。</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/bpe1.png" alt></p><h2 id="子词标记（Subword-Tokenization）的BPE"><a href="#子词标记（Subword-Tokenization）的BPE" class="headerlink" title="子词标记（Subword Tokenization）的BPE"></a>子词标记（Subword Tokenization）的BPE</h2><p>​    方法最早在<a href="https://arxiv.org/pdf/1508.07909.pdf" target="_blank" rel="noopener">Neural Machine Translation of Rare Words with Subword Units</a>提出，最初是为了解决神经网络翻译（NMT）中的OOV（Out of vocabulary）问题，即处理的词表是定长的，而翻译却往往会在词表之外。同样的，这个问题各种NLP任务也会遇到。</p><p>​    为了进行子词标记，BPE简单地做了一些调整，经常出现的子字对合并在一起，而不是被另一个字节代替。这样一来，低频的词就会被分割成几个高频的子词，例如athazagoraphobia可能就会被分割成[‘▁ath’, ‘az’, ‘agor’, ‘aphobia’]。</p><ul><li>第0步，初始化词表；</li><li>第1步，把每个词表示为字符的组合，为了防止与其他词混淆，在词的后面再加入一个特殊的标识”&lt;\w&gt;”。例如hello，就会被分割成：”h e l l o <_ w>“ 。</_></li><li>第2步，遍历词表，统计所有的字符对（其实就是2-gram），如：(h, e), (e, l), (l, l), (l, o), (o, <_ w>)。</_></li><li>第3步，合并频率最高的字符对，将合成的新子词加入词表（或词频+1）。例如，(h, e)出现频率最高，那么就将他们合并，变成新子词he，再将这个新词加入词表，同时原来的单词就变成“he l l o <_ w>”。</_></li><li>第4步，重复2、3步，直到达到迭代次数或达到所需的词汇量。（超参数）</li></ul><h2 id="编码与解码"><a href="#编码与解码" class="headerlink" title="编码与解码"></a>编码与解码</h2><p>​    得到子词的词表后，我们先需要对子词表进行长度从大到小的排序。编码时，我们对每个单词，遍历子词表，寻找是否有子词是该单词的子串，然后将该单词分割成若干子词。若某一子串没有找到对应的子词，那么将它替换为一个特殊的标记如“<_ unk>”。</_></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 给定单词序列</span><br><span class="line">[“the&lt;/w&gt;”, “biggest&lt;/w&gt;”, “stone&lt;/w&gt;”]</span><br><span class="line"></span><br><span class="line"># 假设已有排好序的subword词表</span><br><span class="line">[“er&lt;/w&gt;”, “tain&lt;/w&gt;”, “tone”, “est&lt;/w&gt;”, “big”, “the&lt;/w&gt;”, “one&lt;/w&gt;”, &quot;s&quot;, &quot;g&quot;]</span><br><span class="line"></span><br><span class="line"># 迭代结果</span><br><span class="line">&quot;the&lt;/w&gt;&quot; -&gt; [&quot;the&lt;/w&gt;&quot;]</span><br><span class="line">&quot;biggest&lt;/w&gt;&quot; -&gt; [&quot;big&quot;, &quot;g&quot;,&quot;est&lt;/w&gt;&quot;]</span><br><span class="line">&quot;stone&lt;/w&gt;&quot; -&gt; [&quot;st&quot;, &quot;tone&lt;/w&gt;&quot;]</span><br></pre></td></tr></table></figure><p>​    这种方式的好处是，可以很好地适应新语料，缺点是，如果语料很大，这一编码方式效率较慢，因此一般是在训练模型之前，预编码好语料。还有一种方式是，在构建子词表的时候，其实就可以对构建子词表的语料进行编码了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># 给定单词序列</span><br><span class="line">[..., “the&lt;/w&gt;”, “biggest&lt;/w&gt;”, “stone&lt;/w&gt;”, ...] # 假设语料库中还有其他词语</span><br><span class="line"></span><br><span class="line"># 第一步</span><br><span class="line"># [..., &quot;t h e &lt;/w&gt;&quot;, &quot;b i g g e s t &lt;/w&gt;&quot;, &quot;s t o n e &lt;/w&gt;&quot;, ...]</span><br><span class="line"></span><br><span class="line"># 第二步</span><br><span class="line"># ..., (t, h), (h, e), (e, &lt;/w&gt;), (b, i), ...</span><br><span class="line"></span><br><span class="line"># 第三步，假设he频率最高</span><br><span class="line"># [..., &quot;t he &lt;/w&gt;&quot;, &quot;b i g g e s t &lt;/w&gt;&quot;, &quot;s t o n e &lt;/w&gt;&quot;, ...]</span><br><span class="line"></span><br><span class="line"># 继续第二步</span><br><span class="line"># ..., (t , he), (he, &lt;/w&gt;), (e, &lt;/w&gt;), (b, i), ...</span><br><span class="line"></span><br><span class="line"># ... 第三步</span><br><span class="line"># ... 迭代，直到次数满足，或词表大小满足</span><br><span class="line"></span><br><span class="line"># 最终结果</span><br><span class="line"># [..., &quot;the&lt;/w&gt;&quot;, &quot;big g est&lt;/w&gt;&quot;, &quot;s tone&lt;/w&gt;&quot;]</span><br></pre></td></tr></table></figure><p>​    可以看到，这个时候，我们直接将改变后的每个单词，按照空格符分割即可。</p><p>​    解码的过程就简单多了，以”<_ w>“作为分割符，将子词合并为单词即可</_></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 假设模型预测后得到</span><br><span class="line">[“the&lt;/w&gt;”, &quot;big&quot;, &quot;g&quot;, &quot;est&lt;/w&gt;&quot;, &quot;s&quot;, &quot;tone&lt;/w&gt;&quot;]</span><br><span class="line"></span><br><span class="line"># 解码</span><br><span class="line">the biggest stone</span><br></pre></td></tr></table></figure><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter, defaultdict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_vocab</span><span class="params">(corpus: str)</span> -&gt; dict:</span></span><br><span class="line">    <span class="string">"""Step 1. 建立词表"""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 把每个字符分割出来，并在词语最后加上&lt;/w&gt;，如:"hello"-&gt;"h e l l o &lt;/w&gt;"</span></span><br><span class="line">    tokens = [<span class="string">" "</span>.join(word) + <span class="string">" &lt;/w&gt;"</span> <span class="keyword">for</span> word <span class="keyword">in</span> corpus.split()]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 统计语料中的词频</span></span><br><span class="line">    vocab = Counter(tokens)  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vocab</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_stats</span><span class="params">(vocab: dict)</span> -&gt; dict:</span></span><br><span class="line">    <span class="string">"""Step 2. 分割字符串，并统计字符对的频率"""</span></span><br><span class="line"></span><br><span class="line">    pairs = defaultdict(int)</span><br><span class="line">    <span class="keyword">for</span> word, frequency <span class="keyword">in</span> vocab.items():</span><br><span class="line">        symbols = word.split() <span class="comment"># 以空格分割</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 统计2-gram频率</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(symbols) - <span class="number">1</span>):</span><br><span class="line">            pairs[symbols[i], symbols[i + <span class="number">1</span>]] += frequency</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_vocab</span><span class="params">(pair: tuple, v_in: dict)</span> -&gt; dict:</span></span><br><span class="line">    <span class="string">"""合并频率最高的对"""</span></span><br><span class="line">    </span><br><span class="line">    v_out = &#123;&#125;</span><br><span class="line">    bigram = re.escape(<span class="string">' '</span>.join(pair))</span><br><span class="line">    p = re.compile(<span class="string">r'(?&lt;!\S)'</span> + bigram + <span class="string">r'(?!\S)'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> v_in:</span><br><span class="line">        <span class="comment"># 将词表中所有的最高词频对替换，如："h e l l o" -&gt; "he l l o"</span></span><br><span class="line">        w_out = p.sub(<span class="string">''</span>.join(pair), word)</span><br><span class="line">        v_out[w_out] = v_in[word]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> v_out</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">corpus = <span class="string">"I had seen the biggest stone."</span></span><br><span class="line">vocab = build_vocab(corpus)  <span class="comment"># Step 1</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">num_merges = <span class="number">50</span> <span class="comment"># 迭代次数，超参数</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_merges):</span><br><span class="line"></span><br><span class="line">    pairs = get_stats(vocab)  <span class="comment"># Step 2</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> pairs:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># step 3</span></span><br><span class="line">    best = max(pairs, key=pairs.get)</span><br><span class="line">    vocab = merge_vocab(best, vocab)</span><br></pre></td></tr></table></figure><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>​     [1] Sennrich, Rico, Barry Haddow, and Alexandra Birch. “Neural machine translation of rare words with subword units.”<em>arXiv preprint arXiv:1508.07909</em>(2015).</p><p>​    [2] <a href="https://towardsdatascience.com/@JaswalAkash?source=post_page-----eb36c7df4f10----------------------" target="_blank" rel="noopener">Akashdeep Singh Jaswal</a>, <a href="https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10" target="_blank" rel="noopener">Byte Pair Encoding — The Dark Horse of Modern NLP</a>, 2019.</p><p>​    [3] <a href="mailto:plmsmile@126.com" target="_blank" rel="noopener">PLM</a>, <a href="https://plmsmile.github.io/2017/10/19/subword-units/" target="_blank" rel="noopener">subword-units</a>, 2017.</p><p>​    [4] <a href="https://www.zhihu.com/people/luke-china" target="_blank" rel="noopener">Luke</a>, 深入理解NLP Subword算法：BPE、WordPiece、ULM, 2020.</p><p>​    [5] Byte pair encoding - Wikipedia - <a href="https://en.wikipedia.org/wiki/Byte_pair_encoding" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/B</a>.</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Byte-Pair-Encoding&quot;&gt;&lt;a href=&quot;#Byte-Pair-Encoding&quot; class=&quot;headerlink&quot; title=&quot;Byte Pair Encoding&quot;&gt;&lt;/a&gt;Byte Pair Encoding&lt;/h1&gt;&lt;p&gt;​    &lt;strong&gt;Byte Pari Encoding（BPE）&lt;/strong&gt;）是一种简单的数据压缩技巧，最初在1994年被提出，而如今广泛应用于各种现代的NLP模型（如BERT、GPT-2、XLM…）。&lt;/p&gt;
&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;​    如今NLP的发展，由刚开始的基于频率的稀疏词向量（如词袋、N-gram），到前几年通过预训练（word2vec，glove）生成语义表示的稠密词向量，再到划时代意义的BERT，NLP界越来越重视通过预训练学习语义，从而得到好的词向量表示。&lt;/p&gt;
&lt;p&gt;​    然而这其中有个问题需要解决，基本上所有的NLP任务都需要构造一个词表，往往是选取出现频率最高的N个词加入词表。这其中有什么问题呢，举个例子，BERT的预训练语料是维基百科，如此庞大的语料库，词量肯定也是惊人的高。如果我们构造的词表太小，很多频率较低的词语就会被表示为一个掩码（又或是被去除），在模型中就根本“看不到”，这不仅会让这些低频词无法被学习，还会影响上下文中的高频词的学习。但如果词表过大，就会带来效率问题。&lt;/p&gt;
&lt;p&gt;​    BPE就是一种很好的方法，在不扩大词表的情况下，能够让更多的词语“被模型看到”。&lt;/p&gt;
    
    </summary>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="BERT" scheme="http://yoursite.com/tags/BERT/"/>
    
  </entry>
  
  <entry>
    <title>联合实体关系抽取（Joint Entity and Relation Extraction）论文阅读笔记</title>
    <link href="http://yoursite.com/2021/01/06/natural-language-processing/Relation-Extraction/"/>
    <id>http://yoursite.com/2021/01/06/natural-language-processing/Relation-Extraction/</id>
    <published>2021-01-06T15:09:16.956Z</published>
    <updated>2021-01-06T15:09:16.956Z</updated>
    
    <content type="html"><![CDATA[<p>联合实体关系抽取(Joint Entity and Relation Extraction)，旨在抽取文本中的实体与实体间的关系，一般可以表示为三元组的形式: \&lt; head entity, relation, tial entity>。举个例子，”我在学习自然语言处理的知识”，则可以抽取出：&lt;我，学习，自然语言处理&gt; 等诸如此类的三元组。联合实体关系抽取的一般化做法是将任务分解为两个子任务：</p><ul><li>实体识别</li><li>关系检测</li></ul><p>通过两个子任务之间的信息交互与同步，同时提升两个子任务的表现。</p><p>下面简单地对最近的一些相关论文进行概述。（持续更新中…）</p><h3 id="A-Relation-Specific-Attention-Network-for-Joint-Entity-and-Relation-Extraction"><a href="#A-Relation-Specific-Attention-Network-for-Joint-Entity-and-Relation-Extraction" class="headerlink" title="A Relation-Specific Attention Network for Joint Entity and Relation Extraction"></a>A Relation-Specific Attention Network for Joint Entity and Relation Extraction</h3><p>文中提到，联合实体关系抽取是一个很大的挑战，因为一句话中可能会有多个重叠的实体（ovelapping entity）,如下图所示。</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/20201229230105.png" alt="重叠实体问题" style="zoom:50%;"></p><p>过去的方法大多是先进行实体识别，然后在每个可能的实体对之间进行关系检测，这些方法通常会遭受大量冗余操作的困扰。因此，文章提出了relation-specific attention network (RSAN)，利用关系感知注意机制（relation-aware attention mechanism）为每个关系构建特定的句子表示形式，然后执行序列标记以提取其对应的头和尾实体。在两个公开的数据集New York Times(NYT)和WebNLG上取得了SOTA的效果。</p><p>模型结构图如下：</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/20201229230855.png" alt="模型" style="zoom:50%;"></p><p>实验数据：</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/20201229232832.png" style="zoom:50%;"></p><p>实验结果如下：</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/20201229232751.png" style="zoom:50%;"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;联合实体关系抽取(Joint Entity and Relation Extraction)，旨在抽取文本中的实体与实体间的关系，一般可以表示为三元组的形式: \&amp;lt; head entity, relation, tial entity&gt;。举个例子，”我在学习自然语言处
      
    
    </summary>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="Relation Extraction" scheme="http://yoursite.com/tags/Relation-Extraction/"/>
    
  </entry>
  
  <entry>
    <title>BERT</title>
    <link href="http://yoursite.com/2021/01/06/natural-language-processing/BERT/"/>
    <id>http://yoursite.com/2021/01/06/natural-language-processing/BERT/</id>
    <published>2021-01-06T15:09:16.955Z</published>
    <updated>2021-01-06T15:09:16.955Z</updated>
    
    <content type="html"><![CDATA[<h1 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h1><p>&emsp; <strong>BERT</strong>全称是<strong>Bidirectional Encoder Representations from Transformers</strong>，从字面意思可以知道，这是一个基于Transformer的双向的编码器表征模型.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/BERT1.png?raw=trueF:\blog-fig\BERT1.png" alt="BERT1"></p><blockquote><p>上面是BERT和GPT、ELMo的结构对比.</p></blockquote><a id="more"></a><h2 id="Embedding"><a href="#Embedding" class="headerlink" title="Embedding"></a>Embedding</h2><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/BERT3.png?raw=true" alt></p><p>&emsp; <strong>BERT</strong>的Embedding是3个Embedding的相加，即Token Embeddings、Segment Embeddings、Position Embeddings.</p><ul><li>Token Embeddings：词向量，其中第一个单词是分类任务的标记，可用于后面的分类任务</li><li>Segment Embeddings：用来区别上句和下句</li><li>Position Embeddings：位置信息向量，和Transformer中使用三角函数不一样，这里的是通过学习得到的</li></ul><h2 id="预训练任务"><a href="#预训练任务" class="headerlink" title="预训练任务"></a>预训练任务</h2><p>&emsp; BERT有两个无监督预训练任务，第一步是随机遮盖一些词，然后依赖上下文来预测这些词，得到初步的预训练模型；第二步在第一步的基础上，是随机改变一半的句子对中的第二句，预测一个句子对的第二句是否是第一句的下一句。</p><h3 id="Task-1"><a href="#Task-1" class="headerlink" title="Task 1"></a>Task 1</h3><p>&emsp; 第一个任务称为Mask-LM，常规的LanguageModel是这样的</p><script type="math/tex; mode=display">P(X_i|X_{i-1},...X_1)</script><p>或是这样的</p><script type="math/tex; mode=display">P(X_i|X_{i+1},...,X_{n})</script><p>而Mask-LM充分利用上下文信息，具体地，随机Mask掉15%的词，然后只去预测那些被Mask的词，即</p><script type="math/tex; mode=display">P(masked|X_{else})</script><p>&emsp; 实际上，BERT并不总是Mask那15%的词，而是</p><ul><li>80%的概率Mask</li><li>10%的概率用一个随机的词代替</li><li>10%的概率不变</li></ul><blockquote><p>需注意的是模型本身并不知道它将要被要求预测哪些单词，或哪些单词被替换.</p></blockquote><h3 id="Task-2"><a href="#Task-2" class="headerlink" title="Task 2"></a>Task 2</h3><p>&emsp; 第二个任务则是预测句子的下一句，即给定句子对$<s_1,s_2>$，预测$S_2$是否是$S_1$的下一句。具体地，BERT随机替换50%的句子对中的$S_2$，并自动给定标签，然后去对所有句子对做预测. 例如：</s_1,s_2></p><blockquote><p>Input = [CLS] the man went to [MASK] store [SEP]</p><p>he bought a gallon [MASK] milk [SEP]</p><p>Label = IsNext</p><p>Input = [CLS] the man [MASK] to the store [SEP]</p><p>penguin [MASK] are flight ##less birds [SEP]</p><p>Label = NotNext</p></blockquote><h2 id="Fine-tuning"><a href="#Fine-tuning" class="headerlink" title="Fine-tuning"></a>Fine-tuning</h2><p>&emsp; 预训练好的BERT模型，再去做一些简单调整，再在新的数据集上做训练，可以在许多自然语言处理的任务上有很好的表现. 例如，文本分类、问答、命名实体识别、上下文预测、对话等.</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/BERT2.png?raw=true" alt></p><p>&lt;/br&gt;</p><p>&lt;/br&gt;</p><blockquote><p>参考资料</p><p><a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">https://arxiv.org/abs/1810.04805</a></p><p><a href="https://zhuanlan.zhihu.com/p/46652512---" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/46652512---</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;BERT&quot;&gt;&lt;a href=&quot;#BERT&quot; class=&quot;headerlink&quot; title=&quot;BERT&quot;&gt;&lt;/a&gt;BERT&lt;/h1&gt;&lt;p&gt;&amp;emsp; &lt;strong&gt;BERT&lt;/strong&gt;全称是&lt;strong&gt;Bidirectional Encoder Representations from Transformers&lt;/strong&gt;，从字面意思可以知道，这是一个基于Transformer的双向的编码器表征模型.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://blog-fig.oss-cn-shenzhen.aliyuncs.com/BERT1.png?raw=trueF:\blog-fig\BERT1.png&quot; alt=&quot;BERT1&quot;&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;上面是BERT和GPT、ELMo的结构对比.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="BERT" scheme="http://yoursite.com/tags/BERT/"/>
    
      <category term="Pre-training" scheme="http://yoursite.com/tags/Pre-training/"/>
    
  </entry>
  
  <entry>
    <title>ABSA 综述阅读笔记</title>
    <link href="http://yoursite.com/2021/01/06/natural-language-processing/ABSA-Survey/"/>
    <id>http://yoursite.com/2021/01/06/natural-language-processing/ABSA-Survey/</id>
    <published>2021-01-06T15:09:16.955Z</published>
    <updated>2021-01-06T15:09:16.955Z</updated>
    
    <content type="html"><![CDATA[<h1 id="ABSA-综述阅读笔记"><a href="#ABSA-综述阅读笔记" class="headerlink" title="ABSA 综述阅读笔记"></a>ABSA 综述阅读笔记</h1><p>本文是 <a href="https://ieeexplore.ieee.org/document/8976252" target="_blank" rel="noopener">Issues and Challenges of Aspect-based Sentiment Analysis: A Comprehensive Survey</a> 一文的阅读笔记，目的是快速入门基于方面的情感分析（ABSA或AbSA）的领域。此外，还加入了我的一些个人理解和相关知识的汇总。<br>ABSA（AbSA），Aspect-based Sentiment Analysis，直译为基于方面的情感分析，是近几年NLP领域研究的比较火热的一个方向。它的核心思想是，人们的情感往往是围绕着某一个方面产生和变化的，这在评论文本中尤为明显，人们往往会就着某个事物的某个方面去发表正/负/中性的评论。就像我们买手机的时候，去评价手机的好和坏，往往是会针对手机的某些方面去谈。<br>举个例子，“这部手机的摄像头很好”，“好”的这样一个正面评价不是凭空出现，而是基于“摄像头”这一个方面（aspect）所产生的，因此，当商家分析这句话的时候，就需要知道：用户的评价是关于哪一个方面的（“摄像头”）；用户的情感倾向是什么（正面），这样商家就能从用户的评价中挖掘出情感倾向/用户喜好/需要改进的方面等等信息，对于商家今后改进产品和改变营销手段有很重要的参考价值。这也就引申出了AbSA的两大核心问题：如何去抽取aspect，以及如何利用aspect来改进情感分析是。此外，文中还提出了第三大核心问题：如何去衡量和测量用户的情感动态。</p><a id="more"></a><h2 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h2><p>文中提到了一些名词，下面是其中比较重要的一些名词解释（加入了一些个人的理解）：</p><ul><li><strong>aspect：</strong>人们的情感往往是围绕这某一个方面（aspect），就像前文提到的例子“这部手机的摄像头很好”中，“好”这个评价就是围绕着“摄像头”这个aspect的。</li><li><strong>entity</strong>：aspect往往是跟一个实体（entity）相关的，比如上面这句“这部手机的摄像头很好”，“手机”就是一个实体，也即：用户对“手机”这个实体的“摄像头”方面的情感倾向是“好”。当然，广义地理解，aspect本身也可以是实体（不少研究中并没有明确地区分二者）。</li><li><strong>“explicit”:</strong> “explicit”的意思是，这个aspect显式包含在文本中。还是刚刚的例子，“手机”和“摄像头”都显式地出现在了文本中。</li><li><strong>“implicit”</strong>：跟“explicit”相反，aspect是隐含在文本中。比如：“这部手机很容易没电”，其中的aspect“电池”并没有显式地体现在文本中，而是需要一定语义理解。</li><li><strong>Opinion Target Expression（OTE）</strong>：字面意思是观点目标表达式，还是“这个手机的摄像头很好”的例子，用户的评价目标是“手机”的一个属性，所以OTE是“手机”。这里跟aspect和entity的解释有点重叠，所以其实<strong>aspect是一个很宽泛的概念</strong>。</li></ul><p>另外，还有一些概念文中没有提到，但在不少论文中有所体现的：</p><ul><li><strong>Aspect Term, Target：</strong>往往是”explicit”的，即显式地在文本中出现，上面例子中的“手机”，“摄像头”都属于Aspect Term，也称Target。</li><li><strong>Aspect Category：</strong>一般是预先定义的类别集合，不必显式地出现在文本中。比如上面例子中的“电池”，假如我们事先根据数据集定义的类别集合里包含“电池”这一类别，则它是文本的Aspect Category<strong>。</strong></li></ul><h2 id="AbSA-分类"><a href="#AbSA-分类" class="headerlink" title="AbSA 分类"></a>AbSA 分类</h2><p>文中，作者根据AbSA的处理过程，将AbSA归为3类：</p><ul><li><p><strong>Aspect Extraction (AE)</strong>:</p><p>顾名思义，就是抽取aspect的任务，一般包括 explicit aspects, implicit aspects, aspect terms, entities 和Opinion Target Expressions (OTE)</p></li><li><p><strong>Aspect Sentiment Analysis (ASA):</strong></p><p>在给定aspect，target或者是entity的条件下，对情感的极性进行分类</p></li><li><p><strong>Sentiment Evolution (SE):</strong><br>关注的是人们在一段时间内对某些aspect或事件的情绪变化</p><p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/20201214141332.png" alt></p></li></ul><h2 id="AbSA-核心问题和挑战"><a href="#AbSA-核心问题和挑战" class="headerlink" title="AbSA 核心问题和挑战"></a>AbSA 核心问题和挑战</h2><p>文中提到，目前AbSA任务主要有如下核心问题：</p><ol><li>如何有效地提高AE的表现；为什么至今这仍是一个大的挑战？<ol><li>如何抽取explicit aspect，OTE，implicit aspect，中性情感的aspect？</li><li>怎么进行跨领域和跨语言的AE？</li><li>如何映射不同数据对象之间的关系以改善AE？</li></ol></li><li>如何进行ASA； 如何建立一个情感计算模型，对所有情感方面进行深入分析； 为什么这仍然是一个巨大的挑战和研究热点？<ol><li>如何基于aspect(target)，entity和多词target（multi-word-target level）进行SA？</li><li>如何利用多任务学习（muti-task learning）来提高情感预测的准确率</li><li>数据对象之间的交互，依存关系和上下文语义关系如何有助于改善情感分类？</li></ol></li><li>如何衡量情感值随着时间的变化；为什么没有出色的工作，使得SE成为一个开放的领域？<ol><li>如何识别不断变化的情感特征中的因素并追踪明显的缺陷？</li><li>如何通过社交数据来预测SE</li></ol></li></ol><h2 id="Aspect-Extraction"><a href="#Aspect-Extraction" class="headerlink" title="Aspect Extraction"></a><strong>Aspect Extraction</strong></h2><p>AE(Aspect Extraction)，AbSA的第一个阶段，旨在将文本中的Aspect抽取出来。此外，AE还处理不同aspect之间的关系分布，来识别出一致的、关联的、相似的aspect，以提高抽取出来aspect的表示，可以用于改进下游的任务，也可以用以增强AE本身。<br><strong>Extraction of Aspects：</strong><br>文中将AE任务中需要抽取的aspects分为了如下几个类别：</p><ul><li>Explicit Aspects and OTE：也即显式在文本中出现的aspect，OTE往往也是”explicit”的，因此将它们归类在一起；</li><li>Implicit Aspects：即没有显式地在文本中出现的aspect；</li><li>Aspect with Neural Sentiment：这里单独将中性的aspect归为一类，原因是中性的aspect往往是容易被忽视的。我认为，在对中性的一句话抽取aspect时，中性情感和aspect的对应关系往往不像正/负情感那么强烈，因此就很容易退化成普通文本的关键词抽取或者NER的任务；</li><li>Cross Domain and Cross Lingual Aspects：跨领域和跨语言的任务往往是非常困难的，AE也是如此，基本上NLP中普适的方法如迁移学习等都可以套用在AE上。此外，few-shot和zero-shot也是一种改进方向。</li></ul><p><strong>Map Relationships：</strong><br>对抽取出的实体对象进行关系映射，也是AE中的一个重要任务，而关键是如何去挖掘和表示数据对象之间的关系，因此作者将映射的关系分为两类：</p><ul><li>Co-occurrence Relation：共现关系，这往往是最基础的统计特征；</li><li>Semantic and Dependency Relation (Hierarchical and Ontology Structure)：语义关系和依存关系，往往是较深层的语义信息。一般是通过层次结构或者是Ontology Tree的形式来表现</li></ul><h2 id="Aspect-Sentiment-Analysis"><a href="#Aspect-Sentiment-Analysis" class="headerlink" title="Aspect Sentiment Analysis"></a><strong>Aspect Sentiment Analysis</strong></h2><p>ASA(Aspect Sentimet Analysis)，AbSA的第二阶段，将情感分数分配给每个提取的aspect，target或entity。其中也包括了通过不同数据对象之间的交互（interactions），依存关系（dependencies），以及上下文语义关系（contextual-semantic relationships）来提升情感分类的准确度。<br><strong>Sentiment Analysis：</strong><br>ASA相较于普通的SA任务，能够提供更多信息。文中将ASA分为四类（有重叠的部分）：</p><ul><li>Aspect(Target)-level SA：这是最常见的ASA任务，给定aspect，去计算情感分数；</li><li>Entity-level SA：跟上面的任务类似，不同点在于需要基于实体（entity）进行SA，而entity往往是无固定边界的（类比NER任务）；</li><li>Multi-word Target SA：社交数据上的aspect往往不止一个词（我认为，这在英语上比较明显，对于中文而言，本质上就是分词的问题）；</li><li>Multi-Task Learning SA：多任务学习是近几年的热点，同样地，也可以在ASA任务上应用多任务学习；</li></ul><p><strong>Interactions and Contextual Sentiment Information for Improved ASA：</strong><br><strong>利用交互和上下文情感信息用以改进ASA，</strong>这在ASA里也是流行的做法，实际上就是引入更多信息，使机器能够学习到更多东西，自然也就会有更好的表现。</p><ul><li>不同数据对象之间的交互：数据对象之间的交互和依存关系可以增强aspect的表示，这里的“数据对象”，可以是aspect，也可以是普通的词，一般的做法是用注意力机制和深层的记忆网络来捕捉信息。</li><li>上下文语义情感信息：一般指的是aspect和它上下文之间的关系，比较常见的做法是使用注意力机制，为他们之间分配注意力分数，来增强aspect的表示。</li></ul><h2 id="Sentiment-Evolution"><a href="#Sentiment-Evolution" class="headerlink" title="Sentiment Evolution"></a><strong>Sentiment Evolution</strong></h2><p>除了AE和ASA，作者还提出了一个很重要但相关研究很少的挑战，Sentiment Evolution，关注的是人们在一段时间内对某些aspect或事件的情绪变化。作者认为SE有两个子问题：<br><strong>Recognition of Factors in SE：</strong><br>识别出使得用户改变情感的因素，是分析用户情感变化的关键。作者归纳了以下几种问题：</p><ul><li><strong>Cognitive (Informational) Attitude</strong>：认知的态度，人们对于某个aspect的情感，取决于他们对于这个aspect（对象，事件，实体，目标或人）的认知；</li><li><strong>Adopting the Majority Sentiment (Minority Avoidance)</strong>：采纳多数情绪（避免少数）。另一个问题是多数人的偏好（从众心理）。 人们不愿表现自己的情感，而是采用与之关系紧密的人的情感，避免与持有不同情感态度的少数人建立联系；</li><li><strong>Social Behavior：</strong>社会性行为，当个人与社区及其周围环境互动时，就会出现社会行为。 在社交互动中，情感的变化取决于诸如和谐和分歧（harmonization and divergence）之类的社会影响；</li><li><strong>Heterogeneity of Confined Confidence and Influence (HCCI)：</strong>局限性和影响力的异质性，由于社会上不同的地理位置或地位，人们无法相互交流，但仍具有相同的情感值。（也就是说，可能两个完全不相干的人，对某件事情的看法是一致的）</li></ul><p><strong>Predicting Sentiment Evolution over Social Data：</strong><br>第二个子问题是，预测社交数据的情感的动态变化。在实时应用程序上预测个人不断变化的社会行为是一项复杂的任务，因此去预测情感的动态变化是一个相当困难的问题，但也有学者提出了一些方法，具体的内容可以去看原文5.2，这里不再详述。</p><h2 id="关于未来研究方向的讨论"><a href="#关于未来研究方向的讨论" class="headerlink" title="关于未来研究方向的讨论"></a>关于未来研究方向的讨论</h2><p>文中提到，Aspect的抽取和情感分数的检测是AbSA任务中的核心问题和挑战，并不能通过一个单一而又全面的方法解决。研究者们应当更多地关注那些子问题，进而去解决主要问题。作者还提出了一些有价值的子挑战。</p><ul><li>数据裁剪和清洁（Data Pruning and Cleansing）</li><li>跨领域迁移学习（Cross-Domain-Transfer Learning）</li><li>上下文语义关系（Contextual-Semantic Relationships）</li><li>Aspect 摘要（Aspect Summarization）</li><li>预测情感动态（Predicting Dynamicity of Sentiment）</li><li>多模态情感分析（Multimodal SA）</li></ul><p><strong>个人见解：</strong>上述挑战都是相当有前景的研究方向，然而，对于普通研究者而言，新数据集的获取永远是一个大的问题。此外，在较常规AE和ASA任务中：ASA赛道已经挤满了“选手”，并且技术也日趋成熟，在没有新模型新方法的推出前，往往很难做出有效的成果；AE赛道虽然也有些拥挤，但相较于ASA，空间还是相对大一些。<br>此外，目前也已经有一些比较容易上手研究的新任务（有相应的数据集）：</p><ul><li>Targeted Aspect Based Sentiment Analysis，结合了ATSA和ACSA，旨在识别句子中针对一个指定Aspect term的一个指定Aspect category的情感；</li><li>Target-oriented Opinion Words Extraction任务，旨在抽取句子中与一个指定实体相关的情感词（可以理解为前面提到的OTE）。</li></ul><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>本文并没有较清晰地描述AbSA任务中常见的数据集，但对于研究者来说，数据集是很重要的，因此这里简单地对AbSA研究任务中常见的数据集做一个概括。</p><ul><li><strong>SemEval 14 Restaurant Review</strong>：共有四千多条数据，分为训练集和测试集，包含Aspect Term Sentiment Analysis(ATSA)和Aspect Category Sentimet Analysis(ACSA)两种版本。ATSA的数据集也可以用来做Aspect Term Extraction，ACSA的数据集也可以用来做Aspect Category Extraction。</li><li><strong>SemEval 14 Laptop Review</strong>：共三千多条数据，同样分为训练集和测试集，但只有ATSA的版本。</li><li><strong>Twitter</strong>（ACL 2014）：共六千多条数据，分为训练集和测试集，只有ATSA。</li><li><strong>MAMS</strong>（EMNLP-2019）：共有一万多条数据，包含MAMS数据集中包含ATSA和ACSA两种版本。MAMS的特点是，一个句子中一定包含至少两个Aspect，并且同一个句子中至少有两个Aspect情感极性是不同的，难度比上面几个都要大。</li><li><strong>SentiHood</strong>（COLING 2016）：适用于Targeted Aspect Based Sentiment Analysis任务</li><li><strong>TOWE</strong>（NACCL 2019）：适用于Target-oriented Opinion Words Extraction任务，实际上使用的是SemEval 2014 Resturant, SemEval 14 Laptop, SemEval 15 Resturant, SemEval 16 Resturant, 作者们对它们进行了Target-Opinion pair的标注。</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;ABSA-综述阅读笔记&quot;&gt;&lt;a href=&quot;#ABSA-综述阅读笔记&quot; class=&quot;headerlink&quot; title=&quot;ABSA 综述阅读笔记&quot;&gt;&lt;/a&gt;ABSA 综述阅读笔记&lt;/h1&gt;&lt;p&gt;本文是 &lt;a href=&quot;https://ieeexplore.ieee.org/document/8976252&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Issues and Challenges of Aspect-based Sentiment Analysis: A Comprehensive Survey&lt;/a&gt; 一文的阅读笔记，目的是快速入门基于方面的情感分析（ABSA或AbSA）的领域。此外，还加入了我的一些个人理解和相关知识的汇总。&lt;br&gt;ABSA（AbSA），Aspect-based Sentiment Analysis，直译为基于方面的情感分析，是近几年NLP领域研究的比较火热的一个方向。它的核心思想是，人们的情感往往是围绕着某一个方面产生和变化的，这在评论文本中尤为明显，人们往往会就着某个事物的某个方面去发表正/负/中性的评论。就像我们买手机的时候，去评价手机的好和坏，往往是会针对手机的某些方面去谈。&lt;br&gt;举个例子，“这部手机的摄像头很好”，“好”的这样一个正面评价不是凭空出现，而是基于“摄像头”这一个方面（aspect）所产生的，因此，当商家分析这句话的时候，就需要知道：用户的评价是关于哪一个方面的（“摄像头”）；用户的情感倾向是什么（正面），这样商家就能从用户的评价中挖掘出情感倾向/用户喜好/需要改进的方面等等信息，对于商家今后改进产品和改变营销手段有很重要的参考价值。这也就引申出了AbSA的两大核心问题：如何去抽取aspect，以及如何利用aspect来改进情感分析是。此外，文中还提出了第三大核心问题：如何去衡量和测量用户的情感动态。&lt;/p&gt;
    
    </summary>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="深度学习" scheme="http://yoursite.com/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="Aspect-based Sentiment Analysis" scheme="http://yoursite.com/tags/Aspect-based-Sentiment-Analysis/"/>
    
      <category term="ABSA" scheme="http://yoursite.com/tags/ABSA/"/>
    
  </entry>
  
  <entry>
    <title>Softmax回归</title>
    <link href="http://yoursite.com/2021/01/06/linear-model/softmax-regression/"/>
    <id>http://yoursite.com/2021/01/06/linear-model/softmax-regression/</id>
    <published>2021-01-06T15:09:16.954Z</published>
    <updated>2021-01-06T15:09:16.954Z</updated>
    
    <content type="html"><![CDATA[<p>  在逻辑回归(<em>logistic regression</em>)中，我们处理的是<strong>二分类</strong>任务，即一次只能将样本划分为两类，那么如果遇到多分类问题呢？Softmax 回归便是一个解决方案.</p><h1 id="Softmax-回归（Softmax-Regression）"><a href="#Softmax-回归（Softmax-Regression）" class="headerlink" title="Softmax 回归（Softmax Regression）"></a>Softmax 回归（Softmax Regression）</h1><p><em>(本文省略了很多与逻辑回归类似的但又复杂很多的推导，类似推导：<a href="https://uniblog.cn/2018/06/07/logistic-regression/" target="_blank" rel="noopener">逻辑回归</a>)</em></p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>  在 <strong>Softmax</strong> 回归中，类别标签 $y$ 可以取 $K$ 个不同的值，因此，训练集样本${(x^{(1)},y^{(1)}),…,(x^{(n)},y^{n})}$ 的类别标签有 $y^{(i)} \in {1,2,…,k}$ .<a id="more"></a> </p><p> 于是我们给定输入 $x$，希望可以对同一样本在不同$k=1,2,…k$ 值下估计<strong>条件概率</strong> $P(y = k|x)$ 的值，即估计标签取 $K$ 个不同值时的概率。因此，我们假设输出 $K$ 维向量，即 $h_\theta(x)$ :</p><script type="math/tex; mode=display">\begin{align}h_\theta(x) &=\left[\begin{array}    \ P(y=1 |x;\theta)    \\P(y=2 |x;\theta)    \\\qquad \  \ \vdots    \\P(y = K | x;\theta)\end{array}\right]    \\\\&= \frac{1}{\sum^K_{j=1}exp(\theta^{(j)T}x)} \left[\begin{array}    \ exp(\theta^{(1)T}x)    \\exp(\theta^{(2)T}x)        \\\qquad \ \ \vdots    \\exp(\theta^{(K)T}x)\end{array}    \right]\end{align}</script><p>于是，参数 $\theta$ 实际上由 $K$ 列的向量组成：</p><script type="math/tex; mode=display">\theta = \theta^{(1)} , \theta^{(2)},...,\theta^{(k)}</script><h2 id="代价函数-Cost-Function"><a href="#代价函数-Cost-Function" class="headerlink" title="代价函数 (Cost Function)"></a>代价函数 (Cost Function)</h2><p>下面直接给出代价函数：</p><script type="math/tex; mode=display">J(\theta) = - \sum^m_{i=1}\sum^K_{k=1} I\{ y^{(i)} = k\} log \frac{exp(\theta^{(k)T}x^{(i)})}{\sum^K_{j=1}exp(\theta^{(j)T}x^{(i)})}</script><p>其中 $1{·}$ 称为<strong>指示器函数</strong> <em>(indicator funciton)</em> 即：$I{true} =1$，$I{false}=0$.</p><p>  与逻辑回归的代价函数不同的是，Softmax回归是将 $K$ 个不同的类标签的概率值相加.</p><p>且Softmax回归中的<strong>输出概率函数</strong>为：</p><script type="math/tex; mode=display">P(y^{(i)}=k|x^{(i)};\theta) = \frac{exp(\theta^{(k)T}x^{(i)})}{\sum^K_{j=1}exp(\theta^{(j)T}x^{(i)})}</script><p>  遗憾的是，对于 $ J(\theta)$ 的最优化问题，目前还没有<strong>闭式解法</strong>(<em>closed-formway</em>，即无需通过迭代计算而得到结果的解法)，因此，这里我们使用<strong>梯度下降</strong>的方式求解.</p><h2 id="梯度下降-gradient-descent"><a href="#梯度下降-gradient-descent" class="headerlink" title="梯度下降(gradient descent)"></a>梯度下降(gradient descent)</h2><p>​    对 $J(\theta)$ 求偏导得：</p><script type="math/tex; mode=display">\frac{\partial J(\theta)}{\partial \theta} =- \sum^m_{i=1}[x^{(i)}(I\{y^{(i)}=k\}-P(y^{(i)}=k|x^{(i)};\theta))]</script><p>于是代入</p><script type="math/tex; mode=display">\theta := \theta - \alpha\frac{1}{m}\frac{\partial J(\theta)}{\partial \theta}</script><p>就能得到参数 $\theta$ 的更新公式. </p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;  在逻辑回归(&lt;em&gt;logistic regression&lt;/em&gt;)中，我们处理的是&lt;strong&gt;二分类&lt;/strong&gt;任务，即一次只能将样本划分为两类，那么如果遇到多分类问题呢？Softmax 回归便是一个解决方案.&lt;/p&gt;
&lt;h1 id=&quot;Softmax-回归（Softmax-Regression）&quot;&gt;&lt;a href=&quot;#Softmax-回归（Softmax-Regression）&quot; class=&quot;headerlink&quot; title=&quot;Softmax 回归（Softmax Regression）&quot;&gt;&lt;/a&gt;Softmax 回归（Softmax Regression）&lt;/h1&gt;&lt;p&gt;&lt;em&gt;(本文省略了很多与逻辑回归类似的但又复杂很多的推导，类似推导：&lt;a href=&quot;https://uniblog.cn/2018/06/07/logistic-regression/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;逻辑回归&lt;/a&gt;)&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&quot;介绍&quot;&gt;&lt;a href=&quot;#介绍&quot; class=&quot;headerlink&quot; title=&quot;介绍&quot;&gt;&lt;/a&gt;介绍&lt;/h2&gt;&lt;p&gt;  在 &lt;strong&gt;Softmax&lt;/strong&gt; 回归中，类别标签 $y$ 可以取 $K$ 个不同的值，因此，训练集样本${(x^{(1)},y^{(1)}),…,(x^{(n)},y^{n})}$ 的类别标签有 $y^{(i)} \in {1,2,…,k}$ .
    
    </summary>
    
    
      <category term="线性模型" scheme="http://yoursite.com/categories/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="Softmax回归" scheme="http://yoursite.com/tags/Softmax%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记之逻辑回归</title>
    <link href="http://yoursite.com/2021/01/06/linear-model/logistic-regression/"/>
    <id>http://yoursite.com/2021/01/06/linear-model/logistic-regression/</id>
    <published>2021-01-06T15:09:16.954Z</published>
    <updated>2021-01-06T15:09:16.954Z</updated>
    
    <content type="html"><![CDATA[<p>  在线性回归中，我们希望通过一条直线尽可能地拟合数据，从而实现对数据的预测，但是假如我们需要对数据进行分类，那么我们就需要找到一条直线（或一个超平面）尽可能地将属于不同的标签数据分开，于是我们可以使用逻辑回归算法实现该功能。</p><h1 id="逻辑回归-logistic-regression"><a href="#逻辑回归-logistic-regression" class="headerlink" title="逻辑回归(logistic regression)"></a>逻辑回归(logistic regression)</h1><h2 id="函数模型"><a href="#函数模型" class="headerlink" title="函数模型"></a>函数模型</h2><p>  在线性回归中，我们使用 $h(x) = \theta^Tx$ 作为预测函数，显然 $h(x) \in (-\infty, +\infty)$ 。但在进行分类任务时，显然我们希望函数值落在 $(0,1)$ 的区间内，且函数关于原点对称，于是<strong>Sigmoid函数：</strong></p><script type="math/tex; mode=display">g(z) = \frac{1}{1+e^{-z}}    \tag{1.1}</script><p>便是一个很好的选择。 </p><a id="more"></a><p>  其图像为</p><p><img src="http://pbug3xg5x.bkt.clouddn.com/v2-cff9ab66142c0eba9aca8949ab68ecbe_r.jpg" alt="sigmoid函数"></p><h2 id="损失函数-loss-function"><a href="#损失函数-loss-function" class="headerlink" title="损失函数(loss function)"></a>损失函数(loss function)</h2><p>  我们对Sigmoid函数求导，得到：</p><script type="math/tex; mode=display">g'(z) = \frac{e^{-z}}{(1+e^{-z})^2} = g(z)(1-g(z))    \tag{2.1}</script><p>我们假设输出概率函数如下，这实际上是一个<strong>伯努利分布</strong>(<em>Bernoulli distribution</em>)也称为<strong>两点分布</strong>、<strong>0-1分布</strong> ：</p><script type="math/tex; mode=display">\begin{align*}P(y=1|x;\theta) &= g(\theta^Tx)&= \frac{1}{1+e^{-\theta^T x}} &= h_\theta(x)    \\    \tag{2.2}P(y=0|x;\theta) &= 1- g(\theta^Tx)&= \frac{e^{-\theta^Tx}}{1+e^{-\theta^T x}} &= 1- h_\theta(x)     \end{align*}</script><p>于是，我们可以得到一般的输出概率函数为：</p><script type="math/tex; mode=display">P(y |x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y}    \tag{2.3}</script><p>我们采用<strong>对数似然估计</strong>(相应内容：<a href="https://uniblog.cn/2018/07/16/MLE/" target="_blank" rel="noopener">极大似然估计</a>)，可以得到</p><script type="math/tex; mode=display">\begin{align*}l(\theta) &= \sum^m_{i=1} log P(y^{(i)}|x^{(i)};\theta) \\&= \sum^m_{i=1} log  (h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}    \\&= \sum^m_{i=1}y^{(i)} log  (h_\theta(x^{(i)})) + \sum^m_{i=1}(1-y^{(i)})log(1-h_\theta(x^{(i)}))    \\\end{align*}    \tag{2.4}</script><p>我们希望当损失函数 $J(\theta)$ 达到最小时 $l(\theta) $ 达到最大值，因此我们希望 $J(\theta)$ 和 $l(\theta)$ 是负相关的关系，于是可取：</p><script type="math/tex; mode=display">\begin{align*}J(\theta) &= -\frac{1}{m} l(\theta) \\ &=\frac{1}{m} \sum^m_{i=1}-y^{(i)} log  (h_\theta(x^{(i)})) - \sum^m_{i=1}(1-y^{(i)})log(1-h_\theta(x^{(i)}))    \\\end{align*}    \tag{2.5}</script><h2 id="参数更新方法："><a href="#参数更新方法：" class="headerlink" title="参数更新方法："></a>参数更新方法：</h2><h3 id="梯度下降法-gradient-descent-："><a href="#梯度下降法-gradient-descent-：" class="headerlink" title="梯度下降法(gradient descent)："></a>梯度下降法(gradient descent)：</h3><p>  我们对损失函数 $J(\theta)$ 进行求导，得到：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial J(\theta)}{\partial \theta_j}  = \frac{1}{m}[\sum^m_{i=1}-y^{(i)}\frac{1}{h_\theta(x^{(i)})} +\sum^m_{i=1}(1-y^{(i)})\frac{1}{1-h_\theta(x^{(i)})}] \frac{\partial h_\theta(x^{(i)})}{\partial\theta_j}\end{aligned}    \tag{3.1}</script><p>化简为：</p><script type="math/tex; mode=display">\frac{\partial J(\theta)}{\partial\theta_j} = \frac{1}{m} \sum^m_{i=1} (h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}    \tag{3.2}</script><p>于是参数的更新方程为：</p><script type="math/tex; mode=display">\theta_{i} := \theta_{i} - \alpha \frac{1}{m} \sum^m_{i=1}  (h_{\theta}(x^{(i)})-y^{(i)}) x^{(i)}    \tag{3.3}</script><p><em>(此处与线性回归的方程是一样的)</em></p><h3 id="正规方程-normal-equation-："><a href="#正规方程-normal-equation-：" class="headerlink" title="正规方程(normal equation)："></a>正规方程(normal equation)：</h3><script type="math/tex; mode=display">\theta = (X^T X)^{-1} X^T y    \tag{4.1}</script><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(dir)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    导入训练数据</span></span><br><span class="line"><span class="string">    :param dir:训练数据的路径</span></span><br><span class="line"><span class="string">    :return: 放入学习器的数据</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    tmp = np.loadtxt(dir, dtype=np.str, delimiter=<span class="string">","</span>)</span><br><span class="line">    data = tmp[<span class="number">1</span>:, <span class="number">1</span>:<span class="number">-2</span>].astype(np.float)</span><br><span class="line">    label = tmp[<span class="number">1</span>:, <span class="number">-1</span>].astype(np.float)</span><br><span class="line">    <span class="keyword">return</span> data, label</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(in_x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    sigmoid函数</span></span><br><span class="line"><span class="string">    :param in_x:特征值x矩阵</span></span><br><span class="line"><span class="string">    :return: 映射到sigmoid函数上的函数值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span>/(<span class="number">1</span>+exp(-in_x))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grad_ascent</span><span class="params">(data_array, label_array, alpha, max_cycles)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    梯度下降法更新参数</span></span><br><span class="line"><span class="string">    :param data_array: 特征集</span></span><br><span class="line"><span class="string">    :param label_array: 标签集</span></span><br><span class="line"><span class="string">    :param alpha: 学习率</span></span><br><span class="line"><span class="string">    :param max_cycles: 最大迭代次数</span></span><br><span class="line"><span class="string">    :return: 参数 weigh</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data_mat = mat(data_array)   <span class="comment"># size:m*n</span></span><br><span class="line">    label_mat = mat(label_array).transpose()     <span class="comment"># size:m*1</span></span><br><span class="line">    m, n = shape(data_mat)</span><br><span class="line">    weigh = ones((n, <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_cycles):</span><br><span class="line">        h = sigmoid(data_mat * weigh)</span><br><span class="line">        error = label_mat-h  <span class="comment"># size:m*1</span></span><br><span class="line">        weigh = weigh+alpha*(data_mat.transpose())*error</span><br><span class="line">    <span class="keyword">return</span> weigh</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">classfy</span><span class="params">(dir, weigh)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对验证集进行分类预测，并评估模型</span></span><br><span class="line"><span class="string">    :param dir: 验证集的路径</span></span><br><span class="line"><span class="string">    :param weigh: 训练好的参数</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data_array, label_array = load_data(dir)</span><br><span class="line">    data_mat = mat(data_array)</span><br><span class="line">    label_mat = mat(label_array).transpose()</span><br><span class="line">    h = sigmoid(data_mat * weigh)  <span class="comment"># size:m*1</span></span><br><span class="line">    m = len(h)</span><br><span class="line">    cnt_acc = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        <span class="keyword">if</span> int(h[i]) &gt; <span class="number">0.5</span> <span class="keyword">and</span> label_mat[i] == <span class="number">1</span>:</span><br><span class="line">            cnt_acc += <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> int(h[i]) &lt;= <span class="number">0.5</span> <span class="keyword">and</span> label_mat[i] == <span class="number">0</span>:</span><br><span class="line">            cnt_acc += <span class="number">0</span></span><br><span class="line">    acc_rate = cnt_acc/m</span><br><span class="line">    print(acc_rate)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    print(<span class="string">"----------1.load data------------"</span>)</span><br><span class="line">    <span class="comment"># 导入数据按实际数据情况进行修改</span></span><br><span class="line">    data, label = load_data(<span class="string">'train_data.csv'</span>)</span><br><span class="line">    print(<span class="string">"----------2. training------------"</span>)</span><br><span class="line">    <span class="comment"># 学习率alpha和最大迭代次数max_cycles根据实际情况制定</span></span><br><span class="line">    weigh = grad_ascent(data, label, alpha=<span class="number">0.05</span>, max_cycles=<span class="number">10</span>)</span><br><span class="line">    print(<span class="string">"----------3. testing------------"</span>)</span><br><span class="line">    classfy(<span class="string">'verification_data.csv'</span>, weigh)</span><br></pre></td></tr></table></figure><blockquote><p>参考资料:</p><p>[1] 周志华. 机器学习[M], 北京: 清华大学出版社, 2016: 53-60.</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;  在线性回归中，我们希望通过一条直线尽可能地拟合数据，从而实现对数据的预测，但是假如我们需要对数据进行分类，那么我们就需要找到一条直线（或一个超平面）尽可能地将属于不同的标签数据分开，于是我们可以使用逻辑回归算法实现该功能。&lt;/p&gt;
&lt;h1 id=&quot;逻辑回归-logistic-regression&quot;&gt;&lt;a href=&quot;#逻辑回归-logistic-regression&quot; class=&quot;headerlink&quot; title=&quot;逻辑回归(logistic regression)&quot;&gt;&lt;/a&gt;逻辑回归(logistic regression)&lt;/h1&gt;&lt;h2 id=&quot;函数模型&quot;&gt;&lt;a href=&quot;#函数模型&quot; class=&quot;headerlink&quot; title=&quot;函数模型&quot;&gt;&lt;/a&gt;函数模型&lt;/h2&gt;&lt;p&gt;  在线性回归中，我们使用 $h(x) = \theta^Tx$ 作为预测函数，显然 $h(x) \in (-\infty, +\infty)$ 。但在进行分类任务时，显然我们希望函数值落在 $(0,1)$ 的区间内，且函数关于原点对称，于是&lt;strong&gt;Sigmoid函数：&lt;/strong&gt;&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
g(z) = \frac{1}{1+e^{-z}}    \tag{1.1}&lt;/script&gt;&lt;p&gt;便是一个很好的选择。 &lt;/p&gt;
    
    </summary>
    
    
      <category term="线性模型" scheme="http://yoursite.com/categories/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="逻辑回归" scheme="http://yoursite.com/tags/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/"/>
    
      <category term="判别模型" scheme="http://yoursite.com/tags/%E5%88%A4%E5%88%AB%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>机器学习笔记之局部加权线性回归</title>
    <link href="http://yoursite.com/2021/01/06/linear-model/lr_local_weighted/"/>
    <id>http://yoursite.com/2021/01/06/linear-model/lr_local_weighted/</id>
    <published>2021-01-06T15:09:16.954Z</published>
    <updated>2021-01-06T15:09:16.954Z</updated>
    
    <content type="html"><![CDATA[<p>  局部加权线性回归是线性回归的升级版，目的是为了解决普通线性回归中无法解决的非线性的问题。其实质是每次只使用预测点附近的部分数据进行回归预测，因而可以拟合出一条折线，相较于普通的线性回归，能够较好地拟合非线性数据。</p><h1 id="局部加权线性回归"><a href="#局部加权线性回归" class="headerlink" title="局部加权线性回归"></a>局部加权线性回归</h1><h3 id="普通线性回归"><a href="#普通线性回归" class="headerlink" title="普通线性回归"></a>普通线性回归</h3><p>  在线性回归的算法中，我们的最终目的是求出使得<strong>代价函数</strong></p><script type="math/tex; mode=display">J(\theta) = \frac{1}{2m}  (h_{\theta}(x)-y)^2</script><p>达到<strong>最小值</strong>的线性拟合函数：</p><script type="math/tex; mode=display">h_\theta(x) = \theta_0x_0 + \theta_1 x_1 + ... + \theta_n x_n</script><p>  在普通的线性回归算法中，对于每一组 $x :x<em>0, x_1,…x_n$ ，我们运用梯度下降或正规方程，求得所对应的一组 $\theta : \theta_0 , \theta_1, …, \theta_n$，因此我们求得的 $h</em>\theta$ 是一条直线，显然对于非线性的数据拟合程度很差。</p><a id="more"></a> <p>如图所示：</p><p><img src="http://pbug3xg5x.bkt.clouddn.com/lr.png" alt="普通线性回归"></p><p>   有关线性回归的详细笔记：<a href="https://uniblog.cn/2018/06/07/linear-regression/" target="_blank" rel="noopener">线性回归</a></p><h3 id="考虑局部"><a href="#考虑局部" class="headerlink" title="考虑局部"></a>考虑局部</h3><p>  于是我们提出一种想法：能否不考虑<strong>全部的样本值</strong>，而是在局部进行线性回归，考虑<strong>局部的样本值</strong>得局部到整体的多条直线，所构成的折线（近似于曲线）便可以较好地拟合数据。</p><p>  因此在局部加权线性回归算法中，我们赋予预测点附近的每一个点一定的权值，离它较近的赋予较大的权值，较远的赋予较小的权值，即与较近的点进行线性拟合，而忽略较远的点的贡献。</p><h3 id="权值函数"><a href="#权值函数" class="headerlink" title="权值函数"></a>权值函数</h3><p>  因此，权值应当考虑充分地考虑预测点和其他点之间的距离，这里我们使用如下函数作为权值函数</p><script type="math/tex; mode=display">W^{(i)}_{x_j} = exp(-\frac{(x^{(i)}-x_j)^2}{2\sigma^2})</script><p>  其中，$x_j$ 为预测点，$x^{(i)}$ 为该点附近的所有点，而 $\sigma$ 决定了对附近的点应该赋予多大的权值，$ \sigma $ 值越大，距离的增大对权值的减小影响越小，一次训练所用到的数据就越多，即最终拟合的函数越“线性”。</p><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>  相对于普通的线性回归，局部加权函数的损失函数应当包含有权值参数，我们这里给出局部加权线性回归的损失函数，如下</p><script type="math/tex; mode=display">J(\theta) = \frac{1}{2m}  W_{x_j}^{(i)}(h_{\theta}(x)-y)^2</script><h3 id="更新方程"><a href="#更新方程" class="headerlink" title="更新方程"></a>更新方程</h3><p>  得到损失函数后，使用与普通的线性回归相类似的方法，我们可以得到梯度下降法和最小二乘法下，参数 $\theta$ 的更新方程。</p><h4 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h4><script type="math/tex; mode=display">\theta_k : = \theta_k - \frac{\alpha}{m} \sum^m_{i=1} W^{(i)} (h_{\theta}(x^{(i)})-y^{(i)}) x^{(i)}_k</script><h4 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h4><script type="math/tex; mode=display">\theta = (X^TWX)^{-1}X^TWY</script><h3 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h3><h4 id="梯度下降实现"><a href="#梯度下降实现" class="headerlink" title="梯度下降实现"></a>梯度下降实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(dir)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    导入训练数据</span></span><br><span class="line"><span class="string">    :param dir:训练数据的路径</span></span><br><span class="line"><span class="string">    :return: 放入学习器的数据</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    tmp = np.loadtxt(dir, dtype=np.float)</span><br><span class="line">    x_array = tmp[:, :<span class="number">-1</span>]</span><br><span class="line">    y_array = tmp[:, <span class="number">-1</span>]</span><br><span class="line">    <span class="keyword">return</span> x_array, y_array</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regress</span><span class="params">(test_point, x_array, label_array ,alpha, k)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对一个点进行线性回归</span></span><br><span class="line"><span class="string">    :param test_point: 输入一个点的x值</span></span><br><span class="line"><span class="string">    :param x_array: 整体的数据集</span></span><br><span class="line"><span class="string">    :param label_array: 标签集</span></span><br><span class="line"><span class="string">    :param alpha: 学习率</span></span><br><span class="line"><span class="string">    :param k: 波长参数</span></span><br><span class="line"><span class="string">    :return: 预测的一个点</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x_mat = np.mat(x_array)</span><br><span class="line">    y_mat = np.mat(label_array)</span><br><span class="line">    m = shape(x_mat)[<span class="number">0</span>]</span><br><span class="line">    weigh = mat(np.eye(m))</span><br><span class="line">    theta = eye(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        weigh[i, i] = np.exp(np.square(test_point - x_mat[i])/(<span class="number">-2</span>*k*k))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">30</span>):</span><br><span class="line">        error = theta * x_mat.T - y_mat  </span><br><span class="line">        theta = theta - (alpha) * error * weigh * x_mat</span><br><span class="line">    <span class="keyword">return</span> test_point * theta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regress_all</span><span class="params">(x_array, label_array, alpha, k)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对全部点进行回归预测</span></span><br><span class="line"><span class="string">    :param x_array: 数据集</span></span><br><span class="line"><span class="string">    :param label_array: 标签集</span></span><br><span class="line"><span class="string">    :param alpha: 学习率</span></span><br><span class="line"><span class="string">    :param k: 波长参数</span></span><br><span class="line"><span class="string">    :return: 预测的标签集</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = shape(x_array)[<span class="number">0</span>]</span><br><span class="line">    y_label = ones(m)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        y_label[i] = regress(x_array[i], x_array, label_array, alpha, k)</span><br><span class="line">    <span class="keyword">return</span> y_label</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    主函数</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x_array, y_array = load_data(<span class="string">"data.txt"</span>)</span><br><span class="line">    y = regress_all(x_array, y_array, <span class="number">0.25</span>, <span class="number">0.018</span>)</span><br><span class="line">    <span class="comment"># 对 x 排序后画出回归图</span></span><br><span class="line">    x_mat = mat(x_array).transpose()</span><br><span class="line">    y_mat = mat(y)</span><br><span class="line">    ind = x_mat.argsort(<span class="number">1</span>)</span><br><span class="line">    x_sort = x_mat.T[ind][<span class="number">0</span>]</span><br><span class="line">    y_sort = y_mat.T[ind][<span class="number">0</span>]</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    ax = fig.add_subplot(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    fig_name = <span class="string">"linear regression"</span></span><br><span class="line">    plt.title(fig_name)</span><br><span class="line">    ax.plot(x_sort, y_sort)</span><br><span class="line">    ax.scatter(x_array, y_array, <span class="number">8</span>, <span class="string">"red"</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure><h4 id="最小二乘法实现"><a href="#最小二乘法实现" class="headerlink" title="最小二乘法实现"></a>最小二乘法实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 上文与梯度下降的内容一致</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regress</span><span class="params">(test_point, x_array, label_array, k)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对一个点进行线性回归</span></span><br><span class="line"><span class="string">    :param test_point: 输入一个点的x值</span></span><br><span class="line"><span class="string">    :param x_array: 整体的数据集</span></span><br><span class="line"><span class="string">    :param label_array: 标签集</span></span><br><span class="line"><span class="string">    :param k: 波长参数</span></span><br><span class="line"><span class="string">    :return: 预测的一个点</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x_mat = np.mat(x_array)</span><br><span class="line">    y_mat = np.mat(label_array)</span><br><span class="line">    m = shape(x_mat)[<span class="number">0</span>]</span><br><span class="line">    weigh = mat(np.eye(m))</span><br><span class="line">    theta = eye(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        weigh[i, i] = np.exp(np.square(test_point - x_mat[i])/(<span class="number">-2</span>*k*k))</span><br><span class="line">theta = (x_mat.T * weigh * x_mat).I * x_mat.T * weigh * y_mat.T</span><br><span class="line">    <span class="keyword">return</span> test_point * theta</span><br><span class="line"><span class="comment"># 下文与梯度下降的内容一致（去除函数内的学习率alpha）</span></span><br></pre></td></tr></table></figure><h3 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h3><p>现在，我们再对非线性数据进行拟合，如图：</p><p><img src="http://pbug3xg5x.bkt.clouddn.com/lr_weighted.png" alt="局部加权线性回归"></p><p>显然，拟合效果比普通的线性模型要好得多。</p><h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p><a href="http://pbug3xg5x.bkt.clouddn.com/sine.txt" target="_blank" rel="noopener">数据集</a></p><h3 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h3><p>  虽然局部加权线性回归在处理非线性的问题上有着不错的效果，但同时也存在着问题，比如：</p><p>(1) 一次只对一个数据点进行拟合，对大量数据的处理十分缓慢</p><p>(2) 容易产生过拟合问题</p><p>  因此，我们还要去寻找更优秀的模型。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;  局部加权线性回归是线性回归的升级版，目的是为了解决普通线性回归中无法解决的非线性的问题。其实质是每次只使用预测点附近的部分数据进行回归预测，因而可以拟合出一条折线，相较于普通的线性回归，能够较好地拟合非线性数据。&lt;/p&gt;
&lt;h1 id=&quot;局部加权线性回归&quot;&gt;&lt;a href=&quot;#局部加权线性回归&quot; class=&quot;headerlink&quot; title=&quot;局部加权线性回归&quot;&gt;&lt;/a&gt;局部加权线性回归&lt;/h1&gt;&lt;h3 id=&quot;普通线性回归&quot;&gt;&lt;a href=&quot;#普通线性回归&quot; class=&quot;headerlink&quot; title=&quot;普通线性回归&quot;&gt;&lt;/a&gt;普通线性回归&lt;/h3&gt;&lt;p&gt;  在线性回归的算法中，我们的最终目的是求出使得&lt;strong&gt;代价函数&lt;/strong&gt;&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
J(\theta) = \frac{1}{2m}  (h_{\theta}(x)-y)^2&lt;/script&gt;&lt;p&gt;达到&lt;strong&gt;最小值&lt;/strong&gt;的线性拟合函数：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
h_\theta(x) = \theta_0x_0 + \theta_1 x_1 + ... + \theta_n x_n&lt;/script&gt;&lt;p&gt;  在普通的线性回归算法中，对于每一组 $x :x&lt;em&gt;0, x_1,…x_n$ ，我们运用梯度下降或正规方程，求得所对应的一组 $\theta : \theta_0 , \theta_1, …, \theta_n$，因此我们求得的 $h&lt;/em&gt;\theta$ 是一条直线，显然对于非线性的数据拟合程度很差。&lt;/p&gt;
    
    </summary>
    
    
      <category term="线性模型" scheme="http://yoursite.com/categories/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="线性回归" scheme="http://yoursite.com/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    
  </entry>
  
  <entry>
    <title>广义线性模型</title>
    <link href="http://yoursite.com/2021/01/06/linear-model/GLM/"/>
    <id>http://yoursite.com/2021/01/06/linear-model/GLM/</id>
    <published>2021-01-06T15:09:16.953Z</published>
    <updated>2021-01-06T15:09:16.953Z</updated>
    
    <content type="html"><![CDATA[<h1 id="广义线性模型-Generalized-Linear-Models）"><a href="#广义线性模型-Generalized-Linear-Models）" class="headerlink" title="广义线性模型(Generalized Linear Models）"></a>广义线性模型(Generalized Linear Models）</h1><h2 id="构造广义线性模型"><a href="#构造广义线性模型" class="headerlink" title="构造广义线性模型"></a>构造广义线性模型</h2><p>  广义线性模型有3个假设：</p><ul><li>$y|x;\theta \sim ExponentialFamily(\eta)：$固定参数 $\theta$，在给定 $x$ 的情况下，$y$ 服从<strong>指数分布族</strong> <em>(the exponential family)</em>中以 $\eta$ 为参数的某个分布.</li><li>给定一个 $x$，我们需要的目标函数为 $h_\theta(x) = E[T(y) | x;\theta] $，后者为该分布的期望.</li><li>令 $\eta = \theta^Tx$.</li></ul><p>其中，所有可以表示成如下形式的概率分布，都属于指数分布族：</p><script type="math/tex; mode=display">p(y;n) = b(y)exp(\eta^TT(y)-a(\eta))    \tag{1.1}</script><p>在式$(1.1)$ 中，$\eta$ 为该分布的<strong>自然参数</strong> <em>(nature parameter)</em>；$T(y)$ 是<strong>充分统计量</strong> <em>(sufficient statistic)</em> ，通常情况下 $T(y) =y$.</p><a id="more"></a><h2 id="推导线性回归"><a href="#推导线性回归" class="headerlink" title="推导线性回归"></a>推导线性回归</h2><p><a href="http://uniblog.cn/2018/06/07/linear-regression/" target="_blank" rel="noopener">线性回归</a></p><p>  在线性回归中，我们使用了<strong>高斯分布</strong>作为误差 $e$ 的概率分布，现在我们将高斯分布表示为$(1,1)$的形式：</p><script type="math/tex; mode=display">\begin{align}p(y;\mu) &= \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(y-\mu)^2}{2\sigma^2})    \\&= \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{1}{2}y^2)exp(\mu y-\frac{1}{2}\mu^2)\end{align}    \tag{2.1}</script><p>其中</p><script type="math/tex; mode=display">\begin{align}\eta &= \eta^T =\mu \\T(y) &= y \\a(\eta)& = \frac{1}{2}\mu^2 = \frac{1}{2}\eta^2    \\b(y) &= \frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{1}{2}y^2)\end{align}    \tag{2.2}</script><p>  即满足第1点要求，接下来：</p><script type="math/tex; mode=display">\begin{align}h_\theta(x) &= E[T(y)|x;\theta]    \\&= E[y|x;\theta]    \\&=\mu\end{align}    \tag{2.3}</script><p>即第2点满足，最后我们令 $\eta = \theta^Tx$ ，由 $(2.2) $中 $\eta=\mu$ ，有：</p><script type="math/tex; mode=display">h_\theta(x) = \mu = \eta = \theta^Tx    \tag{2.4}</script><h2 id="推导逻辑回归"><a href="#推导逻辑回归" class="headerlink" title="推导逻辑回归"></a>推导逻辑回归</h2><p> <a href="http://uniblog.cn/2018/06/07/logistic-regression/" target="_blank" rel="noopener">逻辑回归</a></p><p> 在逻辑回归的推导中，我们假定条件概率 $P(y=c|x;\theta)$ 服从<strong>伯努利分布</strong>，现在我们同样将伯努利分布放在广义线性模型下：</p><script type="math/tex; mode=display">\begin{align}p(y;\phi) &= \phi^y(1-\phi)^{1-y}    \\&= (e^{log\phi})^y(e^{log(1-\phi)})^{(1-y)}    \\&= exp(ylog\phi+(1-y)log(1-\phi))    \\&= exp((log\frac{\phi}{1-\phi})y+log(1-\phi))\end{align}    \tag{3.1}</script><p>相应的参数为：</p><script type="math/tex; mode=display">\begin{align}\eta &= log\frac{\phi}{1-\phi} \rightarrow \frac{1}{1+e^{-\eta}}    \\T(y) &=y    \\a(\eta) &= -log(1-\phi) = log((1-\phi)^{-1}) = log(1+e^\phi)    \\b(y) &= 1\end{align}     \tag{3.2}</script><p>接下来执行第2点和第3点，令 $h_\theta(x)$ 等于伯努利函数的期望，并且令$\eta = \theta^Tx$：</p><script type="math/tex; mode=display">\begin{align}h_\theta(x)& = E[T(y)|x;\theta]    \\&= E[y|x;\theta]    \\&=\phi    \\&= \frac{1}{1+e^{-\eta}}    \\&= \frac{1}{1+e^{-\theta^Tx}}\end{align}    \tag{3.3}</script><h2 id="推导-Softmax-回归"><a href="#推导-Softmax-回归" class="headerlink" title="推导 Softmax 回归"></a>推导 Softmax 回归</h2><p><a href="http://uniblog.cn/2018/07/16/softmax-regression/" target="_blank" rel="noopener">Softmax回归</a></p><h3 id="多项式分布"><a href="#多项式分布" class="headerlink" title="多项式分布"></a>多项式分布</h3><p>我们利用多项式分布进行建模，得到解决多分类问题的模型，实际上我们可以把它理解为是逻辑回归中 <strong>1重二项分布(伯努利分布)</strong> 的拓展，在多项式分布中，有：</p><script type="math/tex; mode=display">\begin{align}p(y=1) = \phi_i    \\\sum^k_{i=1}\phi_i = 1        \end{align}    \tag{4.1}</script><p>其中$y\in{1,2,3…,k}$，又因为 $\phi_i$ 累加为1，所有我们可以只保留 $k-1$ 个参数：</p><script type="math/tex; mode=display">\phi_k = 1-\sum^{k-1}_{i=1}\phi_i    \tag{4.2}</script><p>(这跟我们在二分类时只保留1个参数是一样的)</p><h3 id="概率分布"><a href="#概率分布" class="headerlink" title="概率分布"></a>概率分布</h3><p>在多分类问题中，有如下的<strong>概率分布</strong>：</p><script type="math/tex; mode=display">p(y;\theta) = \phi_1^{I(y=1)} \phi_2^{I(y=2)}... \phi_k^{I(y=k)}    \tag{4.2}</script><p>其中$I(y=i)$ 为<strong>指示器函数</strong> <em>(indicator function)</em></p><script type="math/tex; mode=display">\begin{align}I\{true\} = 1 \\I\{false\} = 0\end{align}    \tag{4.3}</script><p>为了实现指示器函数的功能，我们给出 $T(y)$ ：</p><script type="math/tex; mode=display">\begin{align}T(1) &= [1\ 0\  ...\ 0]^T ,\\T(2) & = [0\ 1\  ...\ 0]^T, \\T(k-1) &=  [0\ 0\  ...\ 1]^T, \\T(k) &=  [0\ 0\  ...\ 0]^T, \\\end{align}\tag{4.4}</script><p>于是 $T(y) $ 中的某个元素：</p><script type="math/tex; mode=display">T(y)_i = I\{y=i\}    \tag{4.5}</script><p>于是由 $(4.2)$ 有</p><script type="math/tex; mode=display">\begin{align}p(y;\theta) &= \phi^{I(y=1)} \phi_2^{I(y=2)}...\phi_k^{1-\sum^{k-1}_{i=1}I\{y=i\}} \\&=\phi_1^{T(y)_1}\phi_2^{T(y)_2}...\phi_k^{1-\sum^{k-1}_{i=1}T(y)_i}\end{align}\tag{4.6}</script><h3 id="在广义线性模型下的推导"><a href="#在广义线性模型下的推导" class="headerlink" title="在广义线性模型下的推导"></a>在广义线性模型下的推导</h3><p>  首先从广义线性模型的第1点开始，我们将$(4.6)$表示为<strong>指数分布族</strong>的形式：</p><script type="math/tex; mode=display">\begin{align}p(y;\theta) &=\phi_1^{T(y)_1}\phi_2^{T(y)_2}...\phi_k^{1-\sum^{k-1}_{i=1}T(y)_i}    \\&= exp(T(y)_1log\phi_1+T(y)_2log\phi_2+⋯+((1−\sum _{i=1}^{k−1}T(y)_i)log\phi_k)    \\&= exp(T(y)_1log\frac{\phi_1}{\phi_k}+T(y)_2log\frac{\phi_2}{\phi_k}+⋯+T(y)_{k-1}log\frac{\phi_{k-1}}{\phi_k} +log_{\phi_k}    )\\&= b(y)exp(\eta^TT(y)-a(\eta))\end{align}\tag{5.1}</script><p>这里有：</p><script type="math/tex; mode=display">\begin{align}\eta &= [log\frac{\phi_1}{\phi_k}, ... log\frac{\phi_k-1}{\phi_k}]^T    \\T(y) &= 式(4.4) \\a(\eta) &= -log\phi_k    \\b(y)&= 1\end{align}\tag{5.2}</script><p>观察发现：</p><script type="math/tex; mode=display">\eta_i = log\frac{\phi_i}{\phi_k} \rightarrow e^{\eta_i} = \frac{\phi_i}{\phi_k}    \tag{5.3}</script><p>令其累加，有：</p><script type="math/tex; mode=display">\sum^k_{i=1}e^{\eta_i} = \frac{\sum^k_{i=1}\phi_i}{\phi_k} = \frac{1}{\phi_k} \\\rightarrow \phi_k = \frac{1}{\sum^k_{i=1}e^{\eta_i}}    \tag{5.4}</script><p>代入$(5.3)$得：</p><script type="math/tex; mode=display">\phi_i = \frac{e^{\eta_i}}{\sum^k_{i=1}e^{\eta_i}}    \tag{5.5}</script><p>下面推导第2点和第3点，假设$h_\theta(x)$ 等于多项式分布的期望，并令 $\eta =\theta^Tx$ :</p><script type="math/tex; mode=display">\begin{align}h_\theta(x) &= ET(y)|x;\theta]    \\& = E[p(y=1|x;\theta),p(y=2|x;\theta),...,p(y=k-1|x;\theta)^T    \\& = [\phi_1,\phi_2,...,\phi_{k-1}]^T    \\& =[\frac{e^{\eta_1}}{\sum^k_{i=1}e^{\eta_i}},\frac{e^{\eta_2}}{\sum^k_{i=1}e^{\eta_i}},...,\frac{e^{\eta_{k-1}}}{\sum^k_{i=1}e^{\eta_i}}]^T    \\& =[\frac{e^{\theta_1^Tx}}{\sum^k_{i=1}e^{\theta_i^Tx}},\frac{e^{\theta_2^Tx}}{\sum^k_{i=1}e^{\theta_i^Tx}},...,\frac{e^{\theta_{k-1}^Tx}}{\sum^k_{i=1}e^{\theta_i^Tx}}]^T\end{align}\tag{5.6}</script>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;广义线性模型-Generalized-Linear-Models）&quot;&gt;&lt;a href=&quot;#广义线性模型-Generalized-Linear-Models）&quot; class=&quot;headerlink&quot; title=&quot;广义线性模型(Generalized Linear Models）&quot;&gt;&lt;/a&gt;广义线性模型(Generalized Linear Models）&lt;/h1&gt;&lt;h2 id=&quot;构造广义线性模型&quot;&gt;&lt;a href=&quot;#构造广义线性模型&quot; class=&quot;headerlink&quot; title=&quot;构造广义线性模型&quot;&gt;&lt;/a&gt;构造广义线性模型&lt;/h2&gt;&lt;p&gt;  广义线性模型有3个假设：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$y|x;\theta \sim ExponentialFamily(\eta)：$固定参数 $\theta$，在给定 $x$ 的情况下，$y$ 服从&lt;strong&gt;指数分布族&lt;/strong&gt; &lt;em&gt;(the exponential family)&lt;/em&gt;中以 $\eta$ 为参数的某个分布.&lt;/li&gt;
&lt;li&gt;给定一个 $x$，我们需要的目标函数为 $h_\theta(x) = E[T(y) | x;\theta] $，后者为该分布的期望.&lt;/li&gt;
&lt;li&gt;令 $\eta = \theta^Tx$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;其中，所有可以表示成如下形式的概率分布，都属于指数分布族：&lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
p(y;n) = b(y)exp(\eta^TT(y)-a(\eta))    \tag{1.1}&lt;/script&gt;&lt;p&gt;在式$(1.1)$ 中，$\eta$ 为该分布的&lt;strong&gt;自然参数&lt;/strong&gt; &lt;em&gt;(nature parameter)&lt;/em&gt;；$T(y)$ 是&lt;strong&gt;充分统计量&lt;/strong&gt; &lt;em&gt;(sufficient statistic)&lt;/em&gt; ，通常情况下 $T(y) =y$.&lt;/p&gt;
    
    </summary>
    
    
      <category term="线性模型" scheme="http://yoursite.com/categories/%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
    
      <category term="机器学习" scheme="http://yoursite.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="广义线性模型" scheme="http://yoursite.com/tags/%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
</feed>
