<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">



  
  
    
    
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>
  <link href="/lib/pace/pace-theme-flash.min.css?v=1.0.2" rel="stylesheet">







<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/idea.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/idea.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">


  <link rel="manifest" href="/images/manifest.json">




  <meta name="keywords" content="机器学习,判别模型,支持向量机,">





  <link rel="alternate" href="/atom.xml" title="One Blog" type="application/atom+xml">






<meta name="description" content="支持向量机(Support Vector Machine)介绍 在 感知机(Perceptron) 算法中，我们寻求一个超平面将两类数据分开，但显然这样的超平面可以有很多个，那么哪个超平面才是最好的呢？很显然，那个距离两边的点最远的超平面，能够更好地将样本分类. 这便是 支持向量机(Support Vector Machine) 的基本型.  如图，$H_2$ 和 $H_3$ 都可以将两类数据分离">
<meta name="keywords" content="机器学习,判别模型,支持向量机">
<meta property="og:type" content="article">
<meta property="og:title" content="支持向量机">
<meta property="og:url" content="http://yoursite.com/2018/07/24/discrimination-model/SVM/SVM/index.html">
<meta property="og:site_name" content="One Blog">
<meta property="og:description" content="支持向量机(Support Vector Machine)介绍 在 感知机(Perceptron) 算法中，我们寻求一个超平面将两类数据分开，但显然这样的超平面可以有很多个，那么哪个超平面才是最好的呢？很显然，那个距离两边的点最远的超平面，能够更好地将样本分类. 这便是 支持向量机(Support Vector Machine) 的基本型.  如图，$H_2$ 和 $H_3$ 都可以将两类数据分离">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://blog-fig.oss-cn-shenzhen.aliyuncs.com//svm1.png">
<meta property="og:image" content="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/svm.jpg">
<meta property="og:image" content="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/Svm_max_sep_hyperplane_with_margin.png">
<meta property="og:image" content="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/kernel.jpg">
<meta property="og:updated_time" content="2020-04-25T11:47:04.858Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="支持向量机">
<meta name="twitter:description" content="支持向量机(Support Vector Machine)介绍 在 感知机(Perceptron) 算法中，我们寻求一个超平面将两类数据分开，但显然这样的超平面可以有很多个，那么哪个超平面才是最好的呢？很显然，那个距离两边的点最远的超平面，能够更好地将样本分类. 这便是 支持向量机(Support Vector Machine) 的基本型.  如图，$H_2$ 和 $H_3$ 都可以将两类数据分离">
<meta name="twitter:image" content="https://blog-fig.oss-cn-shenzhen.aliyuncs.com//svm1.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":14,"onmobile":false,"dimmer":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/07/24/discrimination-model/SVM/SVM/">





  <title>支持向量机 | One Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>
    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">One Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">好好学习</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/07/24/discrimination-model/SVM/SVM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yao-zz">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/header.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="One Blog">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">支持向量机</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-24T09:31:59+08:00">
                2018-07-24
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/判别模型/" itemprop="url" rel="index">
                    <span itemprop="name">判别模型</span>
                  </a>
                </span>

                
                
              
            </span>
          

          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-eye"></i>
            <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  4,034 
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  18 mins
                </span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="支持向量机-Support-Vector-Machine"><a href="#支持向量机-Support-Vector-Machine" class="headerlink" title="支持向量机(Support Vector Machine)"></a>支持向量机(<em>Support Vector Machine</em>)</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p> 在 <strong>感知机</strong>(<em>Perceptron</em>) 算法中，我们寻求一个超平面将两类数据分开，但显然这样的超平面可以有很多个，那么哪个超平面才是最好的呢？很显然，那个距离两边的点最远的超平面，能够更好地将样本分类. 这便是 <strong>支持向量机</strong>(<em>Support Vector Machine</em>) 的基本型.</p>
<p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com//svm1.png" alt="来自维基百科" style="zoom:80%;"></p>
<p>如图，$H_2$ 和 $H_3$ 都可以将两类数据分离，但显然 $H_3$ 更好.</p>
<a id="more"></a>
<h2 id="线性可分支持向量机"><a href="#线性可分支持向量机" class="headerlink" title="线性可分支持向量机"></a>线性可分支持向量机</h2><h3 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h3><p>​    我们给定线性可分训练数据集，希望得到一个分离超平面为</p>
<script type="math/tex; mode=display">
w^* \cdot x + b^ * = 0 \tag{1}</script><p>以及相应的分类决策函数</p>
<script type="math/tex; mode=display">
f(x) = sign(w^*\cdot x + b^*)    \tag{2}</script><p>把它称为<strong>线性可分支持向量机</strong>. 其中</p>
<script type="math/tex; mode=display">
sign(t) =
\left \{
\begin{align}
&+1, \quad t \ge0  \\
&-1, \quad t<0
\end{align}
\right.</script><p>支持向量机的基本型如图所示：</p>
<p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/svm.jpg" alt="图片来源谷歌图片" style="zoom:80%;"></p>
<h3 id="函数间隔和几何间隔"><a href="#函数间隔和几何间隔" class="headerlink" title="函数间隔和几何间隔"></a>函数间隔和几何间隔</h3><h4 id="函数间隔"><a href="#函数间隔" class="headerlink" title="函数间隔"></a>函数间隔</h4><p>​    对于给定的训练数据集 $T$ 和超平面 $(w,b)$，我们定义超平面 $(w,b)$ 关于样本点 $(x_i,y_i)$ 的<strong>函数间隔</strong>为：</p>
<script type="math/tex; mode=display">
\hat{y_i} = y_i (w\cdot x_i +b)    \tag{3}</script><p>定义超平面 $(w,b)$ 关于训练数据集 $T$ 的函数间隔为所有样本点 $(x_i, y_i)$ 的函数间隔的最小值，即</p>
<script type="math/tex; mode=display">
\hat{y} = \min_{i=1,...,N}\hat{y_i}    \tag{4}</script><p><strong>函数间隔可以表示分类预测的正确性和确信度.</strong></p>
<h4 id="几何间隔"><a href="#几何间隔" class="headerlink" title="几何间隔"></a>几何间隔</h4><p>​    我们给定训练数据集 $T$ 和超平面 $(w,b)$，则它关于样本点 $(x_i,y_i)$ 的<strong>几何间隔</strong>为：</p>
<script type="math/tex; mode=display">
\gamma_i = y_i(\frac{w}{||w||} \cdot x_i + \frac{b}{||w||})    \tag{5}</script><p>定义超平面 $(w,b)$ 关于训练数据集 $T$ 的几何间隔为所有样本点 $(x_i,y_i)$ 的几何间隔的最小值，即</p>
<script type="math/tex; mode=display">
\gamma = \min_{i=1,...,N} \gamma _i    \tag{6}</script><p>​    可以看到，超平面 $(w,b)$ 关于样本点 $(x_i,y_i)$ 的几何间隔一般是实例点到超平面的<strong>带符号的距离</strong>(<em>sign distance</em>).</p>
<p> 从函数间隔和几何间隔的定义，我们可以得到：</p>
<script type="math/tex; mode=display">
\gamma_i = \frac{\hat{y_i}}{||w||}    \tag{7}</script><script type="math/tex; mode=display">
\gamma = \frac{\hat{y}}{||w||}     \tag{8}</script><p><strong>如果超平面参数 $w$ 和 $b$ 成比例地改变，函数间隔也按此比例改变，而几何间隔不变.</strong></p>
<h3 id="间隔最大化"><a href="#间隔最大化" class="headerlink" title="间隔最大化"></a>间隔最大化</h3><p> 在SVM算法中，我们的目标的找到一个超平面，离两类数据点都尽可能地远，即希望求得一个几何间隔最大的分离超平面. 即：</p>
<script type="math/tex; mode=display">
\max_{w,b} \gamma    \tag{9}</script><script type="math/tex; mode=display">
s.t. \quad y_i(\frac{w}{||w||}\cdot x_i+ \frac{b}{||w||}) \ge \gamma, \qquad i=1,2,...N \tag{10}</script><p>考虑到几何间隔和函数间隔的关系式(8)，所以有：</p>
<script type="math/tex; mode=display">
\max_{w,b} \frac{\hat{y}}{||w||}    \tag{11}</script><script type="math/tex; mode=display">
s.t. \quad y_i(w\cdot x_i +b) \ge \hat{y}, \qquad i=1,2,...N \tag{12}</script><p><strong>函数间隔 $\hat{y}$ 的取值并不影响最优化问题的解.</strong> 于是我们可以令 $\hat{y}=1$ ，且我们意识到最大化 $\frac{1}{||w||}$ 和最小化 $\frac{1}{2}w^2$ 是等价的，于是得到如下的线性可分支持向量机的最优化问题：</p>
<script type="math/tex; mode=display">
\min_{w,b} \frac{1}{2} ||w||^2    \tag{13}</script><script type="math/tex; mode=display">
s.t. \quad y_i (w\cdot x_i + b) \ge 1    \tag{14}</script><blockquote>
<p><strong>注：</strong>为什么函数间隔 $\hat{y}$ 的取值并不影响最优化问题的解？</p>
<p>在SVM中，我们的求得的超平面为：</p>
<script type="math/tex; mode=display">
w\cdot x + b =0</script><p>显然，对 $w$ 和 $b$ 同时乘一个常数，所得的超平面还是同一个. 于是，回到式 (12)，对 $\hat{y}$ 进行缩放即对 $w$ 和 $b$ 同时进行缩放，并不会影响最终的分类超平面.</p>
</blockquote>
<h3 id="支持向量"><a href="#支持向量" class="headerlink" title="支持向量"></a>支持向量</h3><p>​    在线性可分的情况下，训练数据的样本点中与分离超平面最近的样本点的实例称为<strong>支持向量</strong>(<em>support vector</em>). </p>
<p> 我们让式 (14) 取等号，即得到</p>
<script type="math/tex; mode=display">
y_i(w\cdot x_i +b) = 1</script><p>则支持向量即是满足上式的点.</p>
<p>对正例点，支持向量在超平面:</p>
<script type="math/tex; mode=display">
H_1 : w\cdot x + b =1</script><p>对负例点，支持向量在超平面:</p>
<script type="math/tex; mode=display">
H_2 : w\cdot x + b = -1</script><p>如图所示，在 $H_1$ 和 $H_2$ 上的点就是支持向量.</p>
<p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/Svm_max_sep_hyperplane_with_margin.png" alt="支持向量" style="zoom:45%;"></p>
<p> 我们看到，这两个超平面是平行的，并且中间形成了一条没有实例点的长带，分离超平面恰好在它们的中间. 长带的宽度叫做<strong>间隔</strong>(<em>margin</em>)，等于 $\frac{2}{||w||}$ .</p>
<p> 显然我们可以知道，<strong>决定分离超平面时只有支持向量起作用.</strong></p>
<h3 id="对偶算法"><a href="#对偶算法" class="headerlink" title="对偶算法"></a>对偶算法</h3><blockquote>
<p>​    为了求解最优化问题(13)~(14)，将它作为原始最优化问题，应用<strong>拉格朗日对偶性</strong>，通过求解<strong>对偶问题</strong>(<em>dual problem</em>)得到<strong>原始问题</strong>(<em>primal problem</em>)的最优解，这就是线性可分支持向量机的<strong>对偶算法</strong>(<em>dual algorithm</em>). 这样做的优点，一是对偶问题往往更容易求解；二是自然引入核函数，进而推广到非线性分类问题.</p>
<p>—— 李航 《统计学习方法》</p>
</blockquote>
<p> 我们定义<strong>拉格朗日函数</strong>(<em>Lagrange function</em>):</p>
<script type="math/tex; mode=display">
L(w,b,\alpha) = \frac{1}{2}||w||^2 - \sum^N_{i=1}\alpha_iy_i(w\cdot x_i + b) + \sum^N_{i=1}\alpha_i \tag{15}</script><p>其中 $\alpha_i \ge 0$ 为<strong>拉格朗日乘子</strong>(<em>Lagrange multiplier</em>).</p>
<p> 根据拉格朗日对偶性，原始问题的对偶问题即是极大极小问题：</p>
<script type="math/tex; mode=display">
\max_\alpha \min_{w,b} L(w,b,\alpha)</script><p>于是，为了得到对偶问题的解，需要先求 $L(w,b,\alpha)$ 对 $w,b$ 的极小，再求对 $\alpha$ 的极大.</p>
<ul>
<li><p>求 $\min_{w,b}L(w,b,\alpha)$：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial L}{\partial w} &= w- \sum^N_{i=1}\alpha_iy_ix_i = 0    \\
\frac{\partial L}{\partial b}  &= - \sum^N_{i=1} \alpha_iy_i = 0
\end{align}</script><p>得：</p>
<script type="math/tex; mode=display">
\begin{align}
w  = \sum^N_{i=1}\alpha_iy_ix_i    \\    
\sum^N_{i=1} \alpha_iy_i = 0    
\end{align}    \tag{16}</script><p>将式(16)代入(15)得:</p>
<script type="math/tex; mode=display">
\min_{w,b} L(w,b,\alpha) = - \frac{1}{2} \sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) + \sum ^N_{i=1}\alpha_i</script></li>
<li><p>求 $\min_{w,b}L(w,b,\alpha)$ 对 $\alpha$ 的极大，即是对偶问题</p>
<script type="math/tex; mode=display">
\begin{align}
\max_\alpha \quad &-\frac{1}{2} \sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) + \sum ^N_{i=1}\alpha_i    \\
s.t. \quad &\sum^N_{i=1}\alpha_iy_i = 0    \\
& \alpha_i \ge0, \qquad i=1,2,...N
\end{align}    \tag{17}</script><p>将式 (17) 的目标函数由极大转化为极小，即有：</p>
<script type="math/tex; mode=display">
\begin{align}
\min_\alpha \quad &\frac{1}{2} \sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) - \sum ^N_{i=1}\alpha_i    \\
s.t. \quad &\sum^N_{i=1}\alpha_iy_i = 0    \\
& \alpha_i \ge0, \qquad i=1,2,...N
\end{align}    \tag{18}</script></li>
</ul>
<p>于是，我们设 $\alpha^{\ast}= (\alpha_1^{\ast}, \alpha_2^{\ast},…,\alpha_N^{\ast})^T$为最优化问题 (18) 的解，选择一个分量 $\alpha_j^{\ast} \ge 0$，那么有：</p>
<script type="math/tex; mode=display">
\begin{align}
w^* &= \sum^N_{i=1}\alpha_i^*y_ix_i    \\
b^* &= y_j - \sum^N_{i=1} \alpha_i^*y_i(x_i\cdot x_j)
\end{align}
\tag{19}</script><h2 id="硬间隔和软间隔"><a href="#硬间隔和软间隔" class="headerlink" title="硬间隔和软间隔"></a>硬间隔和软间隔</h2><h3 id="原始问题"><a href="#原始问题" class="headerlink" title="原始问题"></a>原始问题</h3><p> 我们希望数据都是线性可分的，但往往训练数据集中会有一些<strong>奇异点</strong>(<em>outlier</em>)，而导致了原本线性可分的数据变得不可分.  即一些样本点无法满足函数间隔大于等于1的约束条件(14). </p>
<p> 于是我们修改约束条件和目标函数：</p>
<script type="math/tex; mode=display">
\begin{align}
\min_{w,b\in \xi_i} \quad &\frac{1}{2} ||w||^2+ C\sum^N_{i=1}\xi_i    \\
s.t. \quad &y_i(w\cdot x_i +b) \ge 1- \xi_i, \quad i=1,2,...,N    \\
&\xi_i \ge 0 , \quad i=1,2,...,N
\end{align}
\tag{20}</script><p>其中 $\xi_i$ 为松弛变量，即函数间隔不严格地大于等于1. 而 $C$ 称为惩罚参数，即运行一定的误分类，$C$值大的的时候对误分类的惩罚增大，反之减小. </p>
<p> 相对的原来的理想化条件下我们追求的是<strong>硬间隔最大化</strong>，我们的此时目标即<strong>软间隔最大化</strong>.</p>
<h3 id="对偶算法-1"><a href="#对偶算法-1" class="headerlink" title="对偶算法"></a>对偶算法</h3><p> 对原始问题构造拉格朗日函数：</p>
<script type="math/tex; mode=display">
L(w,b,\xi,\alpha,\mu) = \frac{1}{2}||w||^2+C\sum^N_{i=1}\xi_i - \sum^N_{i=1}\alpha_i(y_i(w\cdot x_i + b)-1 + \xi_i)- \sum^N_{i=1}\mu_i\xi_i    \tag{21}</script><p>其中，$\alpha_i \ge0 , \mu_i \ge 0$ .</p>
<p>所以，对偶问题为：</p>
<script type="math/tex; mode=display">
\max_\alpha \min_{w,b,\xi} L(w,b,\xi,\alpha,\mu)     \tag{22}</script><ul>
<li><p>与线性可分的对偶问题同理，求$L(w,b,\xi,\alpha,\mu)$对 $w,b,\xi$ 的极小：</p>
<script type="math/tex; mode=display">
\begin{align}
\frac{\partial L}{\partial w} &= w- \sum^N_{i=1}\alpha_iy_ix_i = 0    \\
\frac{\partial L}{\partial b}  &= - \sum^N_{i=1} \alpha_iy_i = 0    \\
\frac{\partial L}{\partial \xi_i}  &= C- \alpha_i - \mu_i = 0
\end{align}</script><p>于是得：</p>
<script type="math/tex; mode=display">
\begin{align}
w  = \sum^N_{i=1}\alpha_iy_ix_i    \\    
\sum^N_{i=1} \alpha_iy_i = 0    \\
C- \alpha_i - \mu_i = 0
\end{align}    \tag{23}</script><p>将式(24)代入式(22)得：</p>
<script type="math/tex; mode=display">
\min_{w,b,\xi} L(w,b,\xi,\alpha,\mu) = - \frac{1}{2} \sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) + \sum ^N_{i=1}\alpha_i</script></li>
<li><p>再求对 $\alpha$ 的极大，即得：</p>
<script type="math/tex; mode=display">
\begin{align}
\max_\alpha \quad &-\frac{1}{2} \sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) + \sum ^N_{i=1}\alpha_i    \\
s.t. \quad &\sum^N_{i=1}\alpha_iy_i = 0    \\
& C- \alpha_i -\mu_i =0    \\
& \alpha_i \ge0    \\
& \mu \ge 0 , \qquad i=1,2,...N
\end{align} \tag{24}</script><p>消去 $\mu$ 后可得</p>
<script type="math/tex; mode=display">
0 \le \alpha_i \le C</script><p>于是，再将极大转化为极小，即得对偶问题：</p>
<script type="math/tex; mode=display">
\begin{align}
\min_\alpha\quad & \frac{1}{2} \sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(x_i\cdot x_j) - \sum ^N_{i=1}\alpha_i    \\
s.t. \quad &\sum^N_{i=1}\alpha_iy_i = 0    \\
& 0 \le \alpha_i \le C, \qquad i=1,2,...N
\end{align}    \tag{25}</script></li>
<li><p>于是设 $\alpha^{\ast}= (\alpha_1^{\ast}, \alpha_2^{\ast},…,\alpha_N^{\ast})^T$ 为对偶问题的解，选择一个分量 $0\le \alpha_j \le C$，那么原始问题的解 </p>
<p>$w^{\ast}, b^{\ast}$ 可按下式求得：</p>
<script type="math/tex; mode=display">
w^* = \sum_{i=1}^N \alpha_i^* y_ix_i     \\
b^* = y_j - \sum_{i=1}^Ny_i\alpha_i^*(x_i\cdot x_j)
\tag{26}</script></li>
</ul>
<p> 我们观察发现：软间隔最大化下的对偶算法，仅仅在求解 $\alpha^{\ast}$ 时比硬间隔最大化多一个 $\alpha_i \le C$ 的条件.</p>
<h2 id="非线性可分支持向量机"><a href="#非线性可分支持向量机" class="headerlink" title="非线性可分支持向量机"></a>非线性可分支持向量机</h2><h3 id="高维映射"><a href="#高维映射" class="headerlink" title="高维映射"></a>高维映射</h3><p>  前面，我们假设样本线性可分，或在允许一些误分类的情况下线性可分，但如果遇到样本需要曲线来拟合的情况呢？一个有效的方法是将样本映射到高维空间，使样本在该空间内线性可分.</p>
<p><img src="https://blog-fig.oss-cn-shenzhen.aliyuncs.com/kernel.jpg" alt="图片来源CSDN" style="zoom:80%;"></p>
<p> 令 $\phi(x)$ 表示将 $x$ 映射后的特征向量，于是在特征空间中划分的超平面为：</p>
<script type="math/tex; mode=display">
f(x) = w^T\phi(x) + b \tag{27}</script><p>于是有：</p>
<script type="math/tex; mode=display">
\begin{align}
\min_\alpha\quad & \frac{1}{2} \sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_j(\phi(x_i)\cdot \phi(x_j)) - \sum ^N_{i=1}\alpha_i    \\
s.t. \quad &\sum^N_{i=1}\alpha_iy_i = 0    \\
& 0 \le \alpha_i \le C, \qquad i=1,2,...N
\end{align}    \tag{25}</script><h3 id="核方法-核技巧-kernel-trick"><a href="#核方法-核技巧-kernel-trick" class="headerlink" title="核方法(核技巧, kernel trick)"></a>核方法(核技巧, kernel trick)</h3><p>  然而在式(25)中计算 $\phi(x_i)\cdot \phi(x_j)$ 往往比较困难，于是我们设想这样一个函数：</p>
<script type="math/tex; mode=display">
k(x_i,x_j) = \phi(x_i)\cdot \phi(x_j)    \tag{26}</script><p>这样我们就可以在原始空间内计算出 $\phi(x_i) \cdot \phi(x_j)$ 的结果，于是式(25)可重写为：</p>
<script type="math/tex; mode=display">
\begin{align}
\min_\alpha\quad & \frac{1}{2} \sum^N_{i=1}\sum^N_{j=1}\alpha_i\alpha_jy_iy_jk(x_i,x_j) - \sum ^N_{i=1}\alpha_i    \\
s.t. \quad &\sum^N_{i=1}\alpha_iy_i = 0    \\
& 0 \le \alpha_i \le C, \qquad i=1,2,...N
\end{align}    \tag{27}</script><p>解得 $\alpha^{\ast}= (\alpha_1^{\ast}, \alpha_2^{\ast},…,\alpha_N^{\ast})^T$，选择 $\alpha^{\ast}$ 一个正分量 $0&lt;\alpha_j^{\ast}&lt;C$，则：</p>
<script type="math/tex; mode=display">
b^* = y_j - \sum^N_{i=1}\alpha_i^*y_ik(x_i,y_j)    \tag{28}</script><p>分离超平面为：</p>
<script type="math/tex; mode=display">
\sum^N_{i=1} \alpha^*_iy_i k(x,x_i)+b^* = 0    \tag{29}</script><p>于是，分类决策函数：</p>
<script type="math/tex; mode=display">
f(x) = sign(\sum^N_{i=1} \alpha^*_iy_i k(x,x_i)+b^*)    \tag{30}</script><h3 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h3><p>常见的核函数有：</p>
<ul>
<li><p><strong>线性核</strong>：</p>
<script type="math/tex; mode=display">
k(x_i,x_j) = x_i^Tx_j</script></li>
<li><p><strong>多项式核</strong>：</p>
<script type="math/tex; mode=display">
k(x_i,x_j) = (x_i^Tx_j)^d</script><p>$d\ge1$为多项式次数.</p>
</li>
<li><p><strong>高斯核</strong>：</p>
<script type="math/tex; mode=display">
k(x_i,x_j) = exp(-\frac{||x_i-x_j||^2}{2\sigma^2})</script><p>$\sigma&gt;0$ 为高斯核的带宽(width).</p>
</li>
<li><p><strong>拉普拉斯核</strong>：</p>
<script type="math/tex; mode=display">
k(x_i,x_j) =exp(-\frac{||x_i-x_j||}{\sigma})</script></li>
<li><p><strong>Sigmoid核</strong>：</p>
<script type="math/tex; mode=display">
k(x_i,x_j) =tanh(\beta x_i^Tx_j+\theta)</script></li>
</ul>
<p>   $tanh$ 为双曲正切函数， $\beta&gt;0,\theta&gt;0$.</p>
<h2 id="序列最小最优化算法"><a href="#序列最小最优化算法" class="headerlink" title="序列最小最优化算法"></a>序列最小最优化算法</h2><p>  <strong>序列最小最优化算法</strong>(<em>Sequential Minimal Optimization, SMO</em>)，是一种能高效实现SVM的算法. 其基本思路是：<strong>如果所有的变量的解都满足此最优化问题的KKT条件，那么这个最优化问题就解决了.</strong> 其方法是每次选取两个两变量，固定其它变量，针对这两个变量构建二次规划问题.</p>
<h3 id="变量选择"><a href="#变量选择" class="headerlink" title="变量选择"></a>变量选择</h3><ul>
<li><p><strong>第1个变量的选择</strong></p>
<p>SMO 称选择第1个变量的过程为外层循环. 我们在训练样本中选取违反KKT条件最严重的样本点，其对应的变量作为第1个变量.</p>
</li>
<li><p><strong>第2个变量的选择</strong></p>
<p>SMO称选择第2个变量的过程为内层循环. 我们在外层循环找到第1个变量后，我们希望第2个变量能使 $\alpha_2$ 有足够的变化. 具体的：</p>
<script type="math/tex; mode=display">
\begin{align}
E_1 > 0  \rightarrow E_2 = min(E_i)    \\
E_1 < 0 \rightarrow E_2 = max(E_i)    \\
\alpha_2 = arg\max_{\alpha_i \in \alpha}(|E_1 - E_2|)
\end{align}</script></li>
</ul>
<h3 id="变量求解"><a href="#变量求解" class="headerlink" title="变量求解"></a>变量求解</h3><p> 在SMO算法中，我们有：</p>
<script type="math/tex; mode=display">
\alpha_2^{new,unc} = \alpha_2^{old} + \frac{y_2(E_1-E_2)}{\eta}    \\
\eta = K_{11} + K_{22} - K_{12}</script><p>接着，我们计算上下界 $L$ 和 $H$：</p>
<p>如果 $y_1 \neq y_2$:</p>
<script type="math/tex; mode=display">
L=max(0,\alpha_2^{old}-\alpha_1^{old})    \\
H = min(C, C+\alpha_2^{old}-\alpha_1^{old})    \\</script><p>如果 $y_1 = y_2 $:</p>
<script type="math/tex; mode=display">
L=max(0,\alpha_2^{old}+\alpha_1{old}-C)    \\
H= min(C, \alpha_2^{old}+\alpha_1^{old})</script><p>那么有：</p>
<script type="math/tex; mode=display">
\alpha_2^{new} = 
\left \{
\begin{matrix}
H, \qquad \alpha_2^{new,unc} > H\\
\alpha_2^{new,unc},    \qquad L\le\alpha_2^{new,unc} \le H\\
L, \qquad \alpha_2^{new,unc} <L
\end{matrix}
\right .</script><p>由 $\alpha_2^{new}$ 求 $\alpha_1^{new}$，有</p>
<script type="math/tex; mode=display">
\alpha_1^{new} = \alpha_1^{old} + y_1y_2(\alpha_2^{old}-\alpha_2^{new})</script><p>另外，偏置 $b$ 的更新公式为</p>
<script type="math/tex; mode=display">
b_1^{new} = -E_1-y_1K_{11}(\alpha_1^{new}-\alpha_1^{old})-y_2K_{21}(\alpha_2^{new}-\alpha_2^{old})+b^{old}    \\
b_2^{new} = -E_2 - y_1K_{12}(\alpha_1^{new}-\alpha_1^{old})-y_2K_{22}(\alpha_2^{new}-\alpha_2^{old})+b^{old}</script><p> 如果 $\alpha_1^{new} $ 和 $\alpha_2^{new} $ 同时满足 $0 \le\alpha_2^{new} \le C$，如果$\alpha_1^{new} 、\alpha_2^{new} $ 是0或者C，那么$b_1^{new} $和 $b_2^{new} $以及它们之间的数都是符合KKT条件的，这是取它们的中点作为 $b^{new}$.</p>
<p>最后，是误差值的更新：</p>
<script type="math/tex; mode=display">
E_1^{new} = \sum_Sy_j\alpha_jK(x_i,x_j)+b^{new}-y_i</script><h3 id="算法实现"><a href="#算法实现" class="headerlink" title="算法实现"></a>算法实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#coding:UTF-8</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVM</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, data_set,labels, C, toler, kernel_option)</span>:</span></span><br><span class="line">        self.train_x = data_set  <span class="comment"># 训练特征</span></span><br><span class="line">        self.train_y = labels   <span class="comment"># 训练标签</span></span><br><span class="line">        self.C = C  <span class="comment"># 惩罚参数</span></span><br><span class="line">        self.toler = toler     <span class="comment"># 结束迭代的条件之一</span></span><br><span class="line">        self.n_samples = np.shape(data_set)[<span class="number">0</span>]  <span class="comment"># 样本数量</span></span><br><span class="line">        self.alphas = np.mat(np.zeros((self.n_samples, <span class="number">1</span>)))   <span class="comment"># 拉格朗日乘子</span></span><br><span class="line">        self.b = <span class="number">0</span>          <span class="comment"># 偏置量</span></span><br><span class="line">        self.error_tmp = np.mat(np.zeros((self.n_samples, <span class="number">2</span>)))    <span class="comment"># 缓存的误差</span></span><br><span class="line">        self.kernel_opt = kernel_option     <span class="comment"># 选择的核函数</span></span><br><span class="line">        self.kernel_mat = calc_kernel(self.train_x, self.kernel_opt)    <span class="comment"># 核函数矩阵</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(dir)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    载入数据</span></span><br><span class="line"><span class="string">    :param dir: 数据集的路径 (str)</span></span><br><span class="line"><span class="string">    :return: samples_x: 特征集 (mat)</span></span><br><span class="line"><span class="string">             samples_y: 标签集 (mat)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    samples = np.loadtxt(dir, dtype=np.str, delimiter=<span class="string">"\t"</span>)</span><br><span class="line">    samples_x = samples[:, <span class="number">0</span>:<span class="number">-1</span>].astype(np.float)</span><br><span class="line">    samples_y = samples[:, <span class="number">-1</span>:].astype(np.float)</span><br><span class="line">    print(np.shape(samples_x))</span><br><span class="line">    print(np.shape(samples_y))</span><br><span class="line">    <span class="keyword">return</span> np.mat(samples_x), np.mat(samples_y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_kernel_value</span><span class="params">(train_x, train_x_i, kernel_option)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算各个样本的核函数</span></span><br><span class="line"><span class="string">    :param train_x: 训练集 (mat)</span></span><br><span class="line"><span class="string">    :param train_x_i: 训练样本 (mat)</span></span><br><span class="line"><span class="string">    :param kernel_option: 核函数参数，包括类型和参数 (list)</span></span><br><span class="line"><span class="string">    :return: kernel_value: 核函数向量 (mat)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    kernel_type = kernel_option[<span class="number">0</span>]  <span class="comment"># 核函数的类型，分为rbf和其他</span></span><br><span class="line">    m = np.shape(train_x)[<span class="number">0</span>]  <span class="comment"># 样本的个数</span></span><br><span class="line"></span><br><span class="line">    kernel_value = np.mat(np.zeros((m, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> kernel_type == <span class="string">'rbf'</span>:  <span class="comment"># 高斯核函数</span></span><br><span class="line">        sigma = kernel_option[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> sigma == <span class="number">0</span>:</span><br><span class="line">            sigma = <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            diff = train_x[i, :] - train_x_i</span><br><span class="line">            kernel_value[i] = np.exp(diff * diff.T / (<span class="number">-2.0</span> * sigma ** <span class="number">2</span>))</span><br><span class="line">    <span class="keyword">elif</span> kernel_type == <span class="string">'laplace'</span>:  <span class="comment"># 拉普拉斯核函数</span></span><br><span class="line">        sigma = kernel_option[<span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> sigma == <span class="number">0</span>:</span><br><span class="line">            sigma = <span class="number">1.0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">            diff = train_x[i, :] - train_x_i</span><br><span class="line">            kernel_value[i] = np.exp(-np.sqrt(diff * diff.T ) / (sigma))</span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment"># 不使用核函数</span></span><br><span class="line">        kernel_value = train_x * train_x_i.T</span><br><span class="line">    <span class="keyword">return</span> kernel_value</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calc_kernel</span><span class="params">(train_x, kernel_option)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算核函数矩阵</span></span><br><span class="line"><span class="string">    :param train_x: 输入的训练特征 (mat)</span></span><br><span class="line"><span class="string">    :param kernel_option: 核函数的参数 (list)</span></span><br><span class="line"><span class="string">    :return: kernel_matrix: 核函数矩阵 (mat)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    m = np.shape(train_x)[<span class="number">0</span>]  <span class="comment"># 样本的个数</span></span><br><span class="line">    kernel_matrix = np.mat(np.zeros((m, m)))  <span class="comment"># 初始化样本之间的核函数值</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        kernel_matrix[:, i] = cal_kernel_value(train_x, train_x[i, :], kernel_option)</span><br><span class="line">    <span class="keyword">return</span> kernel_matrix</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_error</span><span class="params">(svm, alpha_idx)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算机误差</span></span><br><span class="line"><span class="string">    :param svm: SVM模型 (class)</span></span><br><span class="line"><span class="string">    :param alpha_idx: 参数alpha的下标 (int)</span></span><br><span class="line"><span class="string">    :return: error: 误差 (float)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    output = float(np.multiply(svm.alphas, svm.train_y).T * svm.kernel_mat[:, alpha_idx] + svm.b)</span><br><span class="line">    error = output - float(svm.train_y[alpha_idx])</span><br><span class="line">    <span class="keyword">return</span> error</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_error_tmp</span><span class="params">(svm, alpha_idx)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    更新SVM中缓存的误差</span></span><br><span class="line"><span class="string">    :param svm: SVM模型 (class)</span></span><br><span class="line"><span class="string">    :param alpha_idx: 参数alpha的下标 (int)</span></span><br><span class="line"><span class="string">    :return: 0</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    error = cal_error(svm, alpha_idx)</span><br><span class="line">    svm.error_tmp[alpha_idx] = [<span class="number">1</span>, error]</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_second_var</span><span class="params">(svm, alpha1_idx, error_1)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    选择第二个变量</span></span><br><span class="line"><span class="string">    :param svm: SVM模型 (class)</span></span><br><span class="line"><span class="string">    :param alpha1_idx: 第1个变量的alpha的下标 (int)</span></span><br><span class="line"><span class="string">    :param error_1: 第1个变量的误差 (float)</span></span><br><span class="line"><span class="string">    :return: alpha2_idx: 第2个变量的alpha的下标 (int)</span></span><br><span class="line"><span class="string">             error_2: 第2个变量的误差 (float)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    alpha2_idx = <span class="number">0</span></span><br><span class="line">    error_2 = <span class="number">0</span></span><br><span class="line">    max_delta = <span class="number">0.0</span></span><br><span class="line"></span><br><span class="line">    svm.error_tmp[alpha1_idx] = [<span class="number">1</span>, error_1]</span><br><span class="line">    valid_e_list = np.nonzero(svm.error_tmp[:, <span class="number">0</span>].A)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> len(valid_e_list) &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">for</span> alpha_k <span class="keyword">in</span> valid_e_list:</span><br><span class="line">            <span class="keyword">if</span> alpha_k == alpha1_idx:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            error_k = cal_error(svm, alpha_k)</span><br><span class="line">            error_delta = abs(error_1 - error_k)</span><br><span class="line">            <span class="comment"># 选择最大的步长作为alpha2,error2</span></span><br><span class="line">            <span class="keyword">if</span> error_delta &gt; max_delta:</span><br><span class="line">                alpha2_idx = alpha_k</span><br><span class="line">                max_delta = error_delta</span><br><span class="line">                error_2 = error_k</span><br><span class="line">    <span class="comment"># 随机选择alpha2</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        alpha2_idx = alpha1_idx</span><br><span class="line">        <span class="keyword">while</span> alpha2_idx == alpha1_idx:</span><br><span class="line">            alpha2_idx = int(np.random.uniform(<span class="number">0</span>, svm.n_samples))</span><br><span class="line"></span><br><span class="line">        error_2 = cal_error(svm, alpha2_idx)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> alpha2_idx, error_2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_and_update</span><span class="params">(svm, alpha1_idx)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    更新参数</span></span><br><span class="line"><span class="string">    :param svm: SVM模型(class)</span></span><br><span class="line"><span class="string">    :param alpha1_idx: 第一个参数alpha的下标(int)</span></span><br><span class="line"><span class="string">    :return: 0: alpha1没有违背KKT，重新选择alpha1</span></span><br><span class="line"><span class="string">             1: alpha1违背了KKT，更新参数</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    error_1 = cal_error(svm, alpha1_idx)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (svm.train_y[alpha1_idx]*error_1 &lt; -svm.toler) <span class="keyword">and</span> (svm.alphas[alpha1_idx] &lt; svm.C) <span class="keyword">or</span> \</span><br><span class="line">            (svm.train_y[alpha1_idx]*error_1 &gt; svm.toler) <span class="keyword">and</span> (svm.alphas[alpha1_idx] &gt; <span class="number">0</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 选择第二个变量</span></span><br><span class="line">        alpha2_idx, error_2 = select_second_var(svm, alpha1_idx, error_1)</span><br><span class="line"></span><br><span class="line">        alpha1_old = svm.alphas[alpha1_idx].copy()</span><br><span class="line">        alpha2_old = svm.alphas[alpha2_idx].copy()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算上下界</span></span><br><span class="line">        <span class="keyword">if</span> svm.train_y[alpha1_idx] != svm.train_y[alpha2_idx]:</span><br><span class="line">            L = max(<span class="number">0</span>, alpha2_old - alpha1_old)</span><br><span class="line">            H = min(svm.C, svm.C+alpha2_old-alpha1_old)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            L = max(<span class="number">0</span>,alpha2_old+alpha1_old-svm.C)</span><br><span class="line">            H = min(svm.C, alpha2_old+alpha1_old)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算eta</span></span><br><span class="line">        eta = svm.kernel_mat[alpha1_idx,alpha1_idx] + svm.kernel_mat[alpha2_idx, alpha2_idx]    \</span><br><span class="line">            - <span class="number">2.0</span>*svm.kernel_mat[alpha1_idx,alpha2_idx]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算alpha2_new_unc</span></span><br><span class="line">        alpha2_new_unc = alpha2_old + svm.train_y[alpha2_idx]*(error_1 - error_2)/eta</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算alpha2_new</span></span><br><span class="line">        <span class="keyword">if</span> alpha2_new_unc &gt; H:</span><br><span class="line">            alpha2_new = H</span><br><span class="line">        <span class="keyword">elif</span> alpha2_new_unc &lt; L:</span><br><span class="line">            alpha2_new = L</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            alpha2_new = alpha2_new_unc</span><br><span class="line">        svm.alphas[alpha2_idx] = alpha2_new</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 计算alpha1_new</span></span><br><span class="line">        alpha1_new = alpha1_old + svm.train_y[alpha1_idx]*svm.train_y[alpha2_idx]*(alpha2_old-alpha2_new)</span><br><span class="line">        svm.alphas[alpha1_idx] = alpha1_new</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新b</span></span><br><span class="line">        b1 = -error_1 - svm.train_y[alpha1_idx]*svm.kernel_mat[alpha1_idx,alpha1_idx]*(alpha1_new-alpha1_old)    \</span><br><span class="line">             - svm.train_y[alpha2_idx]*svm.kernel_mat[alpha2_idx,alpha1_idx]*(alpha2_new-alpha2_old) + svm.b</span><br><span class="line"></span><br><span class="line">        b2 = -error_2 - svm.train_y[alpha1_idx]*svm.kernel_mat[alpha1_idx,alpha2_idx]*(alpha1_new-alpha1_old) \</span><br><span class="line">             - svm.train_y[alpha2_idx]*svm.kernel_mat[alpha2_idx,alpha2_idx]*(alpha2_new-alpha2_old) + svm.b</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (<span class="number">0</span> &lt; svm.alphas[alpha1_idx]) <span class="keyword">and</span> (svm.alphas[alpha1_idx] &lt; svm.C):</span><br><span class="line">            svm.b = b1</span><br><span class="line">        <span class="keyword">elif</span> (<span class="number">0</span> &lt; svm.alphas[alpha2_idx]) <span class="keyword">and</span> (svm.alphas[alpha2_idx] &lt; svm.C):</span><br><span class="line">            svm.b = b2</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            svm.b = (b1 + b2) / <span class="number">2.0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 更新error</span></span><br><span class="line">        update_error_tmp(svm, alpha2_idx)</span><br><span class="line">        update_error_tmp(svm, alpha1_idx)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_training</span><span class="params">(train_x, train_y, c, toler, max_iter, kernel_option=<span class="params">(<span class="string">'laplace'</span>, <span class="number">0.5</span>)</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    训练SVM模型</span></span><br><span class="line"><span class="string">    :param train_x: 特征集 (mat)</span></span><br><span class="line"><span class="string">    :param train_y: 标签集 (mat)</span></span><br><span class="line"><span class="string">    :param c: 惩罚参数 (float)</span></span><br><span class="line"><span class="string">    :param toler: KKT条件的参数之一 (float)</span></span><br><span class="line"><span class="string">    :param max_iter: 最大迭代次数 (int)</span></span><br><span class="line"><span class="string">    :param kernel_option: 核函数的参数 (list)</span></span><br><span class="line"><span class="string">    :return: svm: 训练好的SVM模型</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 初始化SVM</span></span><br><span class="line">    svm = SVM(data_set=train_x, labels=train_y, C=c, toler=toler, kernel_option=kernel_option)</span><br><span class="line"></span><br><span class="line">    itr = <span class="number">0</span></span><br><span class="line">    entire_set = <span class="keyword">False</span></span><br><span class="line">    alpha_changed = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> itr &lt; max_iter <span class="keyword">and</span> ((alpha_changed &gt; <span class="number">0</span>) <span class="keyword">or</span> entire_set):</span><br><span class="line"></span><br><span class="line">        alpha_changed = <span class="number">0</span></span><br><span class="line">        <span class="comment"># 遍历边界样本</span></span><br><span class="line">        <span class="keyword">if</span> entire_set <span class="keyword">is</span> <span class="keyword">False</span>:</span><br><span class="line">            bound_samples = []</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(svm.n_samples):</span><br><span class="line">                <span class="keyword">if</span> <span class="number">0</span> &lt; svm.alphas[i, <span class="number">0</span>] &lt; c:</span><br><span class="line">                    bound_samples.append(i)</span><br><span class="line">            <span class="keyword">for</span> idx <span class="keyword">in</span> bound_samples:</span><br><span class="line">                alpha_changed += choose_and_update(svm, idx)</span><br><span class="line">            itr += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 遍历所有的样本</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(svm.n_samples):</span><br><span class="line">                alpha_changed += choose_and_update(svm, i)</span><br><span class="line">            itr += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 在所有边界样本和所有样本之间交替</span></span><br><span class="line">        <span class="keyword">if</span> alpha_changed == <span class="number">0</span>:</span><br><span class="line">            entire_set = <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">elif</span> entire_set:</span><br><span class="line">            entire_set = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> svm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_predict</span><span class="params">(svm, test_sample_x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    更加训练好的SVM预测样本</span></span><br><span class="line"><span class="string">    :param svm: SVM模型 (class)</span></span><br><span class="line"><span class="string">    :param test_sample_x: 测试特征集 (mat)</span></span><br><span class="line"><span class="string">    :return: predict: 预测的值 (float)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    kernel_value = cal_kernel_value(svm.train_x, test_sample_x, svm.kernel_opt)</span><br><span class="line">    predict = kernel_value.T * np.multiply(svm.train_y, svm.alphas) + svm.b</span><br><span class="line">    <span class="keyword">return</span> predict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_acc</span><span class="params">(svm, test_x, test_y)</span>:</span></span><br><span class="line">    n_samples = np.shape(test_x)[<span class="number">0</span>] <span class="comment"># 样本的个数</span></span><br><span class="line">    correct = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_samples):</span><br><span class="line">        <span class="comment"># 对每一个样本得到预测值</span></span><br><span class="line">        predict = svm_predict(svm, test_x[i, :])</span><br><span class="line">        <span class="comment"># 判断每一个样本的预测值与真实值是否一致</span></span><br><span class="line">        <span class="keyword">if</span> np.sign(predict) == np.sign(test_y[i]):</span><br><span class="line">            correct += <span class="number">1</span></span><br><span class="line">    acc = correct / n_samples</span><br><span class="line">    <span class="keyword">return</span> acc</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    print(<span class="string">"----------1.load data------------"</span>)</span><br><span class="line">    train_x, train_y = load_data(<span class="string">"svm_data_train.txt"</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"----------2. training------------"</span>)</span><br><span class="line">    svm = svm_training(train_x=train_x, train_y=train_y, c=<span class="number">0.6</span>, max_iter=<span class="number">35</span>, toler=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"----------3. testing------------"</span>)</span><br><span class="line">    test_x, test_y = load_data(<span class="string">"svm_data_ver.txt"</span>)</span><br><span class="line">    acc = cal_acc(svm, test_x, test_y)</span><br><span class="line">    print(acc)</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">main()</span><br></pre></td></tr></table></figure>
<h3 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h3><p><a href="http://pbug3xg5x.bkt.clouddn.com/svm_data_train.txt" target="_blank" rel="noopener">训练集</a></p>
<p><a href="http://pbug3xg5x.bkt.clouddn.com/svm_data_ver.txt" target="_blank" rel="noopener">验证集</a></p>
<blockquote>
<p>参考资料</p>
<p>[1] 李航. 统计学习方法[M]. 北京: 清华大学出版社, 2012: 95-135.</p>
<p>[2] 维基百科. 支持向量机[DB/OL]. <a href="https://zh.wikipedia.org/wiki/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA" target="_blank" rel="noopener">https://zh.wikipedia.org/wiki/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA</a> , 2018-6-10/2018-7-25.</p>
<p>[3] 周志华. 机器学习[M]. 北京: 清华大学出版社, 2016: 121-140.</p>
<p>[4] Peter Harrington. 机器学习实战[M]. 北京: 人民邮电出版社, 2013: 89-114.</p>
<p>[5] 赵志勇. Python机器学习算法[M]. 北京: 电子工业出版社, 2017: 58-88.</p>
</blockquote>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/机器学习/" rel="tag"># 机器学习</a>
          
            <a href="/tags/判别模型/" rel="tag"># 判别模型</a>
          
            <a href="/tags/支持向量机/" rel="tag"># 支持向量机</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/07/20/discrimination-model/SVM/perceptron/" rel="next" title="感知机">
                <i class="fa fa-chevron-left"></i> 感知机
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/07/24/optimization-theory/CO/" rel="prev" title="约束优化方法">
                约束优化方法 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
    </div>

  








        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/header.jpg" alt="Yao-zz">
            
              <p class="site-author-name" itemprop="name">Yao-zz</p>
              <p class="site-description motion-element" itemprop="description">在机器学习的方向努力中......</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">54</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">18</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">59</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Zzoay" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:uni_coder99@163.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://stackoverflow.com/users/10145221/jiang-gongyao" target="_blank" title="StackOverflow">
                      
                        <i class="fa fa-fw fa-stack-overflow"></i>StackOverflow</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.quora.com/profile/Jiang-Gongyao" target="_blank" title="Quora">
                      
                        <i class="fa fa-fw fa-quora"></i>Quora</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://juejin.im/user/5b6b0ff9f265da0f48613fa3" target="_blank" title="Juejin">
                      
                        <i class="fa fa-fw fa-angle-double-down"></i>Juejin</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.jianshu.com/u/8d6a6834e4fc" target="_blank" title="Jianshu">
                      
                        <i class="fa fa-fw fa-heartbeat"></i>Jianshu</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#支持向量机-Support-Vector-Machine"><span class="nav-number">1.</span> <span class="nav-text">支持向量机(Support Vector Machine)</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#介绍"><span class="nav-number">1.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性可分支持向量机"><span class="nav-number">1.2.</span> <span class="nav-text">线性可分支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#介绍-1"><span class="nav-number">1.2.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#函数间隔和几何间隔"><span class="nav-number">1.2.2.</span> <span class="nav-text">函数间隔和几何间隔</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#函数间隔"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">函数间隔</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#几何间隔"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">几何间隔</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#间隔最大化"><span class="nav-number">1.2.3.</span> <span class="nav-text">间隔最大化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#支持向量"><span class="nav-number">1.2.4.</span> <span class="nav-text">支持向量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对偶算法"><span class="nav-number">1.2.5.</span> <span class="nav-text">对偶算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#硬间隔和软间隔"><span class="nav-number">1.3.</span> <span class="nav-text">硬间隔和软间隔</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#原始问题"><span class="nav-number">1.3.1.</span> <span class="nav-text">原始问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#对偶算法-1"><span class="nav-number">1.3.2.</span> <span class="nav-text">对偶算法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#非线性可分支持向量机"><span class="nav-number">1.4.</span> <span class="nav-text">非线性可分支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#高维映射"><span class="nav-number">1.4.1.</span> <span class="nav-text">高维映射</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#核方法-核技巧-kernel-trick"><span class="nav-number">1.4.2.</span> <span class="nav-text">核方法(核技巧, kernel trick)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#核函数"><span class="nav-number">1.4.3.</span> <span class="nav-text">核函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#序列最小最优化算法"><span class="nav-number">1.5.</span> <span class="nav-text">序列最小最优化算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#变量选择"><span class="nav-number">1.5.1.</span> <span class="nav-text">变量选择</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#变量求解"><span class="nav-number">1.5.2.</span> <span class="nav-text">变量求解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#算法实现"><span class="nav-number">1.5.3.</span> <span class="nav-text">算法实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#数据集"><span class="nav-number">1.5.4.</span> <span class="nav-text">数据集</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        ﻿<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yao-zz</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">总字数: </span>
    
    <span title="Site words total count">64.4k</span>
  
</div>

<div class="powered-by">
  Power by<a href="https://hexo.io"> Next  </a> | 

</div>

<div class="powered-by">
<i class="fa fa-eye"></i><span id="busuanzi_container_site_pv">
  本站总访问量: <span id="busuanzi_value_site_pv"></span> | 
</span>
</div>

<div class="powered-by">
<i class="fa fa-user-md"></i><span id="busuanzi_container_site_uv">
  本站访客数: <span id="busuanzi_value_site_uv"></span>
</span>
</div>






  <span class="post-meta-divider"></span>









        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  

  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  










  <script src="//cdn.jsdelivr.net/npm/valine@1.4.9/dist/Valine.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/valine@1.4.9/dist/Valine.min.js"></script>
  
  <script type="text/javascript">
    var GUEST = ['nick','mail','link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item=>{
      return GUEST.indexOf(item)>-1;
    });
    new Valine({
        el: '#comments' ,
        verify: false,
        notify: true,
        appId: 'PYesa2aFUX1zKBBdFA9M17bq-gzGzoHsz',
        appKey: '81LH73KMJTtNEXA7qfJvT1Pu',
        placeholder: '欢迎畅所欲言',
        avatar:'mm',
        guest_info:guest,
        pageSize:'7' || 10,
    });
  </script>


<script type="text/javascript">
    (function() {
        // 匿名函数，防止污染全局变量
        var utterances = document.createElement('script');
        utterances.type = 'text/javascript';
        utterances.async = true;
        utterances.setAttribute('issue-term','pathname')
        utterances.setAttribute('theme','github-light')
        utterances.setAttribute('repo','Zzoay/comments')
        utterances.crossorigin = 'anonymous';
        utterances.src = 'https://utteranc.es/client.js';
        // content 是要插入评论的地方
        document.getElementById('gitment-container').appendChild(utterances);
    })();
</script>


  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>

<!-- 页面点击小红心 -->
<!-- <script type="text/javascript" src="/js/src/clicklove.js"></script> -->
